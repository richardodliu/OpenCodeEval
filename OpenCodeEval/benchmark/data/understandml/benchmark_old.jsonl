{"id": 1, "difficulty": "easy", "category": "Linear Algebra", "title": "Matrix-Vector Dot Product", "description": "Write a Python function that computes the dot product of a matrix and a vector. The function should return a list representing the resulting vector if the operation is valid, or -1 if the matrix and vector dimensions are incompatible. A matrix (a list of lists) can be dotted with a vector (a list) only if the number of columns in the matrix equals the length of the vector. For example, an n x m matrix requires a vector of length m.", "examples": [{"input": "a = [[1, 2], [2, 4]], b = [1, 2]", "output": "[5, 10]"}], "reasoning": ["Row 1: (1 * 1) + (2 * 2) = 1 + 4 = 5; Row 2: (1 * 2) + (2 * 4) = 2 + 8 = 10"], "import_code": "\n", "output_constrains": "\nYour code should return a list where each element is the dot product of a row of 'a' with 'b'. If the number of columns in 'a' does not match the length of 'b', return -1.\n", "starter_code": "def matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:", "entry_point": "matrix_dot_vector", "reference_code": "\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    if len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        total = 0\n        for i in range(len(row)):\n            total += row[i] * b[i]\n        result.append(total)\n    return result\n", "test_cases": ["\nassert matrix_dot_vector([[1, 2, 3], [2, 4, 5], [6, 8, 9]], [1, 2, 3]) == [14, 25, 49]\n", "\nassert matrix_dot_vector([[1, 2], [2, 4], [6, 8], [12, 4]], [1, 2, 3]) == -1\n", "\nassert matrix_dot_vector([[1.5, 2.5], [3.0, 4.0]], [2, 1]) == [5.5, 10.0]\n"]}
{"id": 2, "difficulty": "easy", "category": "Linear Algebra", "title": "Transpose of a Matrix", "description": "\nWrite a Python function that computes the transpose of a given matrix.\n", "examples": [{"input": "a = [[1,2,3],[4,5,6]]", "output": "[[1,4],[2,5],[3,6]]"}], "reasoning": ["Row 1: (1 * 1) + (2 * 2) = 1 + 4 = 5; Row 2: (1 * 2) + (2 * 4) = 2 + 8 = 10"], "import_code": "\n", "output_constrains": "\n", "entry_point": "transpose_matrix", "starter_code": "def transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:", "reference_code": "\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    return [list(i) for i in zip(*a)]\n", "test_cases": ["\nassert transpose_matrix([[1,2],[3,4],[5,6]]) == [[1, 3, 5], [2, 4, 6]]\n", "\nassert transpose_matrix([[1,2,3],[4,5,6]]) == [[1, 4], [2, 5], [3, 6]]\n", "\nassert transpose_matrix([[1,2],[3,4]]) == [[1, 3], [2, 4]]\n"]}
{"id": 3, "difficulty": "easy", "category": "Linear Algebra", "title": "Reshape Matrix", "description": "\nWrite a Python function that reshapes a given matrix into a specified shape. if it can not be reshaped return back an empty list \"[]\".\n", "examples": [{"input": "a = [[1,2,3,4],[5,6,7,8]], new_shape = (4, 2)", "output": "[[1, 2], [3, 4], [5, 6], [7, 8]]"}], "reasoning": ["The given matrix is reshaped from 2x4 to 4x2."], "import_code": "\nimport numpy as np\n", "output_constrains": "\nReturn python list after reshaping by using numpy's tolist() method.\n", "entry_point": "reshape_matrix", "starter_code": "def reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:", "reference_code": "\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Not compatible case\n    if len(a)*len(a[0]) != new_shape[0]*new_shape[1]:\n        return []\n    return np.array(a).reshape(new_shape).tolist()    \n", "test_cases": ["\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (4, 2)) == [[1, 2], [3, 4], [5, 6], [7, 8]]\n", "\nassert reshape_matrix([[1, 2, 3, 4], [5, 6, 7, 8]], (1, 4)) == []\n", "\nassert reshape_matrix([[1,2,3],[4,5,6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]]\n", "\nassert reshape_matrix([[1,2,3,4],[5,6,7,8]], (2, 4)) == [[1, 2, 3, 4], [5, 6, 7, 8]]\n"]}
{"id": 4, "difficulty": "easy", "category": "Linear Algebra", "title": "Calculate Mean by Row or Column", "description": "\nWrite a Python function that calculates the mean of a matrix either by row or by column, based on a given mode. The function should take a matrix (list of lists) and a mode ('row' or 'column') as input and return a list of means according to the specified mode.\n", "examples": [{"input": "\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]], mode = 'column'\n", "output": "\n[4.0, 5.0, 6.0]\n"}], "reasoning": ["\nCalculating the mean of each column results in [(1+4+7)/3, (2+5+8)/3, (3+6+9)/3].        \n"], "import_code": "\n", "output_constrains": "\n", "entry_point": "calculate_matrix_mean", "starter_code": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:", "reference_code": "\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'column':\n        return [sum(col) / len(matrix) for col in zip(*matrix)]\n    elif mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n", "test_cases": ["\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'column') == [4.0, 5.0, 6.0]\n", "\nassert calculate_matrix_mean([[1, 2, 3], [4, 5, 6], [7, 8, 9]], 'row') == [2.0, 5.0, 8.0]\n"]}
{"id": 5, "difficulty": "easy", "category": "Linear Algebra", "title": "Scalar Multiplication of a Matrix", "description": "\nWrite a Python function that multiplies a matrix by a scalar and returns the result.\n", "examples": [{"input": "\nmatrix = [[1, 2], [3, 4]], scalar = 2\n", "output": "\n[[2, 4], [6, 8]]\n"}], "reasoning": ["\nEach element of the matrix is multiplied by the scalar.\n"], "import_code": "\n", "output_constrains": "\n", "entry_point": "scalar_multiply", "starter_code": "def scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:", "reference_code": "\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    return [[element * scalar for element in row] for row in matrix]\n", "test_cases": ["\nassert scalar_multiply([[1,2],[3,4]], 2) == [[2, 4], [6, 8]]\n", "\nassert scalar_multiply([[0,-1],[1,0]], -1) == [[0, 1], [-1, 0]]\n"]}
{"id": 6, "difficulty": "easy", "category": "Linear Algebra", "title": "Calculate Eigenvalues of a Matrix", "description": "\nWrite a Python function that calculates the eigenvalues of a 2x2 matrix. The function should return a list containing the eigenvalues, sort values from highest to lowest.\n", "examples": [{"input": "\nmatrix = [[2, 1], [1, 2]]\n", "output": "\n[3.0, 1.0]\n"}], "reasoning": ["\nThe eigenvalues of the matrix are calculated using the characteristic equation of the matrix, which for a 2\u00d72 matrix is $\\lambda^2 - \text{trace}(A)\\lambda + \text{det}(A) = 0$ where $\\lambda$ are the eigenvalues.\n"], "import_code": "\n", "output_constrains": "\n", "entry_point": "calculate_eigenvalues", "starter_code": "def calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:", "reference_code": "\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    a, b, c, d = matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1]\n    trace = a + d\n    determinant = a * d - b * c\n    # Calculate the discriminant of the quadratic equation\n    discriminant = trace**2 - 4 * determinant\n    # Solve for eigenvalues\n    lambda_1 = (trace + discriminant**0.5) / 2\n    lambda_2 = (trace - discriminant**0.5) / 2\n    return [lambda_1, lambda_2]\n", "test_cases": ["\nassert calculate_eigenvalues([[2, 1], [1, 2]]) == [3.0, 1.0]\n", "\nassert calculate_eigenvalues([[4, -2], [1, 1]]) == [3.0, 2.0]\n"]}
{"id": 7, "difficulty": "easy", "category": "Linear Algebra", "title": "Matrix Transformation", "description": "\nWrite a Python function that transforms a given matrix $A$ using the operation $T^{-1}AS$, where $T$ and $S$ are invertible matrices. The function should first validate if the matrices $T$ and $S$ are invertible, and then perform the transformation. In cases where there is no solution return -1.\n", "examples": [{"input": "\nA = [[1, 2], [3, 4]], T = [[2, 0], [0, 2]], S = [[1, 1], [0, 1]]\n", "output": "\n[[0.5,1.5],[1.5,3.5]]\n"}], "reasoning": ["\nThe matrices $T$ and $S$ are used to transform matrix $A$ by computing $T^{-1}AS$.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "entry_point": "transform_matrix", "starter_code": "def transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:", "reference_code": "\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert to numpy arrays for easier manipulation\n    A = np.array(A, dtype=float)\n    T = np.array(T, dtype=float)\n    S = np.array(S, dtype=float)\n    \n    # Check if the matrices T and S are invertible\n    if np.linalg.det(T) == 0 or np.linalg.det(S) == 0:\n        # raise ValueError(\"The matrices T and/or S are not invertible.\")\n        return -1\n    \n    # Compute the inverse of T\n    T_inv = np.linalg.inv(T)\n\n    # Perform the matrix transformation; use @ for better readability\n    transformed_matrix = np.round(T_inv @ A @ S, 4)\n    \n    return transformed_matrix.tolist()\n", "test_cases": ["\nassert transform_matrix([[1, 2], [3, 4]], [[2, 0], [0, 2]], [[1, 1], [0, 1]]) == [[0.5,1.5],[1.5,3.5]]\n", "\nassert transform_matrix([[1, 0], [0, 1]], [[1, 2], [3, 4]], [[2, 0], [0, 2]]) == [[-4.0, 2.0], [3.0, -1.0]]\n", "\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [0, 1]]) == [[0.6667, 1.6667], [0.3333, 1.6667]]\n", "\nassert transform_matrix([[2, 3], [1, 4]], [[3, 0], [0, 3]], [[1, 1], [1, 1]]) == -1\n"]}
{"id": 8, "difficulty": "easy", "category": "Linear Algebra", "title": "Calculate 2x2 Matrix Inverse", "description": "\nWrite a Python function that calculates the inverse of a 2x2 matrix. Return 'None' if the matrix is not invertible.\n", "examples": [{"input": "\nmatrix = [[4, 7], [2, 6]]\n", "output": "\n[[0.6, -0.7], [-0.2, 0.4]]\n"}], "reasoning": ["\nThe inverse of a 2x2 matrix [a, b], [c, d] is given by (1/(ad-bc)) * [d, -b], [-c, a], provided ad-bc is not zero.\n"], "import_code": "\n", "output_constrains": "\n", "entry_point": "inverse_2x2", "starter_code": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:", "reference_code": "\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    a, b, c, d = matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d/determinant, -b/determinant], [-c/determinant, a/determinant]]\n    return inverse\n", "test_cases": ["\nassert inverse_2x2([[4, 7], [2, 6]]) == [[0.6, -0.7], [-0.2, 0.4]]\n", "\nassert inverse_2x2([[2, 1], [6, 2]]) == [[-1.0, 0.5], [3.0, -1.0]]\n"]}
{"id": 9, "difficulty": "easy", "category": "Linear Algebra", "title": "Matrix times Matrix", "description": "\nWrite a Python function that multiplies two matrices. If the matrices cannot be multiplied, return -1.\n", "examples": [{"input": "\nA = [[1,2],[2,4]], B = [[2,1],[3,4]]\n", "output": "\n[[ 8,  9],[16, 18]]\n"}], "reasoning": ["\n1*2 + 2*3 = 8; 2*2 + 3*4 = 16; 1*1 + 2*4 = 9; 2*1 + 4*4 = 18 Example 2: input: A = [[1,2], [2,4]], B = [[2,1], [3,4], [4,5]] output: -1 reasoning: the length of the rows of A does not equal the column length of B\n"], "import_code": "\n", "output_constrains": "\n", "entry_point": "matrixmul", "starter_code": "def matrixmul(a:list[list[int|float]],\n              b:list[list[int|float]])-> list[list[int|float]]:", "reference_code": "\ndef matrixmul(a:list[list[int|float]],\n              b:list[list[int|float]])-> list[list[int|float]]:\n    if len(a[0]) != len(b):\n        return -1\n    \n    vals = []\n    for i in range(len(a)):\n        hold = []\n        for j in range(len(b[0])):\n            val = 0\n            for k in range(len(b)):\n                val += a[i][k] * b[k][j]\n                           \n            hold.append(val)\n        vals.append(hold)\n\n    return vals\n", "test_cases": ["\nassert matrixmul([[1,2,3],[2,3,4],[5,6,7]],[[3,2,1],[4,3,2],[5,4,3]]) == [[26, 20, 14], [38, 29, 20], [74, 56, 38]]\n", "\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0],[2,4]]) == [[0, 0], [8, 16], [4, 8]]\n", "\nassert matrixmul([[0,0],[2,4],[1,2]],[[0,0,1],[2,4,1],[1,2,3]]) == -1\n"]}
{"id": 10, "difficulty": "easy", "category": "Linear Algebra", "title": "Calculate Covariance Matrix", "description": "\nWrite a Python function to calculate the covariance matrix for a given set of vectors. The function should take a list of lists, where each inner list represents a feature with its observations, and return a covariance matrix as a list of lists. Additionally, provide test cases to verify the correctness of your implementation.\n", "examples": [{"input": "\n[[1, 2, 3], [4, 5, 6]]\n", "output": "\n[[1.0, 1.0], [1.0, 1.0]]\n"}], "reasoning": ["\n\n"], "import_code": "\n", "output_constrains": "\n", "entry_point": "calculate_covariance_matrix", "starter_code": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:", "reference_code": "\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    n_features = len(vectors)\n    n_observations = len(vectors[0])\n    covariance_matrix = [[0 for _ in range(n_features)] for _ in range(n_features)]\n\n    means = [sum(feature) / n_observations for feature in vectors]\n\n    for i in range(n_features):\n        for j in range(i, n_features):\n            covariance = sum((vectors[i][k] - means[i]) * (vectors[j][k] - means[j]) for k in range(n_observations)) / (n_observations - 1)\n            covariance_matrix[i][j] = covariance_matrix[j][i] = covariance\n\n    return covariance_matrix\n", "test_cases": ["\nassert calculate_covariance_matrix([[1, 2, 3], [4, 5, 6]]) == [[1.0, 1.0], [1.0, 1.0]]\n", "\nassert calculate_covariance_matrix([[1, 5, 6], [2, 3, 4], [7, 8, 9]]) == [[7.0, 2.5, 2.5], [2.5, 1.0, 1.0], [2.5, 1.0, 1.0]]\n"]}
{"id": 11, "difficulty": "medium", "category": "Linear Algebra", "title": "Solve Linear Equations using Jacobi Method", "description": "\nWrite a Python function that uses the Jacobi method to solve a system of linear equations given by Ax = b. The function should iterate n times, rounding each intermediate solution to four decimal places, and return the approximate solution x.\n", "examples": [{"input": "\nA = [[5, -2, 3], [-3, 9, 1], [2, -1, -7]], b = [-1, 2, 3], n=2\n", "output": "\n[0.146, 0.2032, -0.5175]\n"}], "reasoning": ["\nThe Jacobi method iteratively solves each equation for x[i] using the formula x[i] = (1/a_ii) * (b[i] - sum(a_ij * x[j] for j != i)), where a_ii is the diagonal element of A and a_ij are the off-diagonal elements.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "entry_point": "solve_jacobi", "starter_code": "def solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:", "reference_code": "\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    d_a = np.diag(A)\n    nda = A - np.diag(d_a)\n    x = np.zeros(len(b))\n    x_hold = np.zeros(len(b))\n    for _ in range(n):\n        for i in range(len(A)):\n            x_hold[i] = (1/d_a[i]) * (b[i] - sum(nda[i]*x))\n        x = x_hold.copy()\n    return np.round(x,4).tolist()\n", "test_cases": ["\nassert solve_jacobi(np.array([[5, -2, 3], [-3, 9, 1], [2, -1, -7]]), np.array([-1, 2, 3]),2) == [0.146, 0.2032, -0.5175]\n", "\nassert solve_jacobi(np.array([[4, 1, 2], [1, 5, 1], [2, 1, 3]]), np.array([4, 6, 7]),5) == [-0.0806, 0.9324, 2.4422]\n", "\nassert solve_jacobi(np.array([[4,2,-2],[1,-3,-1],[3,-1,4]]), np.array([0,7,5]),3) == [1.7083, -1.9583, -0.7812]\n"]}
{"id": 12, "difficulty": "hard", "category": "Linear Algebra", "title": "Singular Value Decomposition (SVD)", "description": "\nWrite a Python function that approximates the Singular Value Decomposition on a 2x2 matrix by using the jacobian method and without using numpy svd function, i mean you could but you wouldn't learn anything. return the result in this format.\n", "examples": [{"input": "\na = [[2, 1], [1, 2]]\n", "output": "\narray([[-0.70710678, -0.70710678], [-0.70710678,  0.70710678]]), array([3., 1.]), array([[-0.70710678, -0.70710678], [-0.70710678,  0.70710678]])\n"}], "reasoning": ["\nU is the first matrix sigma is the second vector and V is the third matrix\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\n", "entry_point": "svd_2x2_singular_values", "starter_code": "def svd_2x2_singular_values(A: np.ndarray) -> tuple:", "reference_code": "\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n   # stick to lowercase\n   a = A\n\n   a_t = np.transpose(a)\n   a_2 = a_t @ a\n\n   v = np.eye(2)\n\n   for _ in range(1):\n       # Compute rotation angle for a 2x2 matrix\n       if a_2[0,0] == a_2[1,1]:\n           theta = np.pi/4\n       else:\n           theta = 0.5 * np.arctan2(2 * a_2[0,1], a_2[0,0] - a_2[1,1])\n       \n       # Create rotation matrix\n       r = np.array(\n           [\n               [np.cos(theta), -np.sin(theta)],\n               [np.sin(theta), np.cos(theta)]\n               ]\n           )\n       \n       # apply rotation\n       d = np.transpose(r) @ a_2 @ r\n\n       # update a_2\n       a_2 = d\n\n       # accumulate v\n       v = v @ r\n\n   # sigma is the diagonal elements squared\n   s = np.sqrt([d[0,0], d[1,1]])\n   s_inv = np.array([[1/s[0], 0], [0, 1/s[1]]])\n   \n   u = a @ v @ s_inv\n   \n   return (np.round(u, 4).tolist(), np.round(s, 4).tolist(), np.round(v.T, 4).tolist())\n    \n", "test_cases": ["\nassert svd_2x2_singular_values(np.array([[2, 1], [1, 2]])) == ([[0.7071, -0.7071], [0.7071, 0.7071]], [3.0, 1.0], [[0.7071, 0.7071], [-0.7071, 0.7071]])\n", "\nassert svd_2x2_singular_values(np.array([[1, 2], [3, 4]])) == ([[0.4046, 0.9145], [0.9145, -0.4046]], [5.465, 0.366], [[0.576, 0.8174], [-0.8174, 0.576]])\n"]}
{"id": 13, "difficulty": "hard", "category": "Linear Algebra", "title": "Determinant of a 4x4 Matrix using Laplace's Expansion", "description": "\nWrite a Python function that calculates the determinant of a 4x4 matrix using Laplace's Expansion method. The function should take a single argument, a 4x4 matrix represented as a list of lists, and return the determinant of the matrix. The elements of the matrix can be integers or floating-point numbers. Implement the function recursively to handle the computation of determinants for the 3x3 minor matrices.\n", "examples": [{"input": "\na = [[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]\n", "output": "\n0\n"}], "reasoning": ["\nUsing Laplace's Expansion, the determinant of a 4x4 matrix is calculated by expanding it into minors and cofactors along any row or column. Given the symmetrical and linear nature of this specific matrix, its determinant is 0. The calculation for a generic 4x4 matrix involves more complex steps, breaking it down into the determinants of 3x3 matrices.\n"], "import_code": "\n", "output_constrains": "\n", "entry_point": "determinant_4x4", "starter_code": "def determinant_4x4(matrix: list[list[int|float]]) -> float:", "reference_code": "\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    # Base case: If the matrix is 1x1, return its single element\n    if len(matrix) == 1:\n        return matrix[0][0]\n    # Recursive case: Calculate determinant using Laplace's Expansion\n    det = 0\n    for c in range(len(matrix)):\n        minor = [row[:c] + row[c+1:] for row in matrix[1:]]  # Remove column c\n        cofactor = ((-1)**c) * determinant_4x4(minor)  # Compute cofactor\n        det += matrix[0][c] * cofactor  # Add to running total\n    return det\n", "test_cases": ["\nassert determinant_4x4([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]) == 0\n", "\nassert determinant_4x4([[4, 3, 2, 1], [3, 2, 1, 4], [2, 1, 4, 3], [1, 4, 3, 2]]) == -160\n", "\nassert determinant_4x4([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) == 0\n"]}
{"id": 14, "difficulty": "easy", "category": "Machine Learning", "title": "Linear Regression Using Normal Equation", "description": "\nWrite a Python function that performs linear regression using the normal equation. The function should take a matrix X (features) and a vector y (target) as input, and return the coefficients of the linear regression model. Round your answer to four decimal places, -0.0 is a valid result for rounding a very small number.\n", "examples": [{"input": "\nX = [[1, 1], [1, 2], [1, 3]], y = [1, 2, 3]\n", "output": "\n[0.0, 1.0]\n"}], "reasoning": ["\nThe linear model is y = 0.0 + 1.0*x, perfectly fitting the input data.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\n", "entry_point": "linear_regression_normal_equation", "starter_code": "def linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:", "reference_code": "\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X = np.array(X)\n    y = np.array(y).reshape(-1, 1)\n    X_transpose = X.T\n    theta = np.linalg.inv(X_transpose.dot(X)).dot(X_transpose).dot(y)\n    theta = np.round(theta, 4).flatten().tolist()\n    return theta\n", "test_cases": ["\nassert linear_regression_normal_equation([[1, 1], [1, 2], [1, 3]], [1, 2, 3]) == [0.0, 1.0]\n", "\nassert linear_regression_normal_equation([[1, 3, 4], [1, 2, 5], [1, 3, 2]], [1, 2, 1]) == [4.0, -1.0, -0.0]\n"]}
{"id": 15, "difficulty": "easy", "category": "Machine Learning", "title": "Linear Regression Using Gradient Descent", "description": "\nWrite a Python function that performs linear regression using gradient descent. The function should take NumPy arrays X (features with a column of ones for the intercept) and y (target) as input, along with learning rate alpha and the number of iterations, and return the coefficients of the linear regression model as a list. Round your answer to four decimal places. -0.0 is a valid result for rounding a very small number.\n", "examples": [{"input": "\nX = np.array([[1, 1], [1, 2], [1, 3]]), y = np.array([1, 2, 3]), alpha = 0.01, iterations = 1000\n", "output": "\n[0.1107, 0.9513]\n"}], "reasoning": ["\nThe linear model is y = 0.0 + 1.0*x, which fits the input data after gradient descent optimization.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "entry_point": "linear_regression_gradient_descent", "starter_code": "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:", "reference_code": "\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m, n = X.shape\n    theta = np.zeros((n, 1))\n    for _ in range(iterations):\n        predictions = X @ theta\n        errors = predictions - y.reshape(-1, 1)\n        updates = X.T @ errors / m\n        theta -= alpha * updates\n    return np.round(theta.flatten(), 4).tolist()\n", "test_cases": ["\nassert linear_regression_gradient_descent(np.array([[1, 1], [1, 2], [1, 3]]), np.array([1, 2, 3]), 0.01, 1000) == [0.1107, 0.9513]\n"]}
{"id": 16, "difficulty": "easy", "category": "Machine Learning", "title": "Feature Scaling Implementation", "description": "\nWrite a Python function that performs feature scaling on a dataset using both standardization and min-max normalization. The function should take a 2D NumPy array as input, where each row represents a data sample and each column represents a feature. It should return two 2D lists: one scaled by standardization and one by min-max normalization.\n", "examples": [{"input": "\nnp.array([[1, 2], [3, 4], [5, 6]])\n", "output": "\n([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n"}], "reasoning": ["\nStandardization rescales the feature to have a mean of 0 and a standard deviation of 1. Min-max normalization rescales the feature to a range of [0, 1], where the minimum feature value maps to 0 and the maximum to 1.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "entry_point": "feature_scaling", "starter_code": "def feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):", "reference_code": "\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    \n    # Min-Max Normalization\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized_data = (data - min_val) / (max_val - min_val)\n    \n    return np.round(standardized_data,4).tolist(), np.round(normalized_data,4).tolist()\n", "test_cases": ["\nassert feature_scaling(np.array([[1, 2], [3, 4], [5, 6]])) == ([[-1.2247, -1.2247], [0.0, 0.0], [1.2247, 1.2247]], [[0.0, 0.0], [0.5, 0.5], [1.0, 1.0]])\n"]}
{"id": 17, "difficulty": "medium", "category": "Machine Learning", "title": "Polynomial Regression using Normal Equation", "description": "\nWrite a Python function that implements the k-Means clustering algorithm. This function should take specific inputs and produce a list of final centroids. k-Means clustering is a method used to partition n points into k clusters. The goal is to group similar points together and represent each group by its center (called the centroid).\nFunction Inputs:\n- points: A list of points, where each point is a tuple of coordinates (e.g., (x, y) for 2D points)\n- k: An integer representing the number of clusters to form\n- initial_centroids: A list of initial centroid points, each a tuple of coordinates\n- max_iterations: An integer representing the maximum number of iterations to perform\nFunction Output:\n- A list of the final centroids of the clusters, where each centroid is rounded to the nearest fourth decimal.\n", "examples": [{"input": "\npoints = [(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], k = 2, initial_centroids = [(1, 1), (10, 1)], max_iterations = 10\n", "output": "\n[(1, 2), (10, 2)]\n"}], "reasoning": ["\nGiven the initial centroids and a maximum of 10 iterations, the points are clustered around these points, and the centroids are updated to the mean of the assigned points, resulting in the final centroids which approximate the means of the two clusters. The exact number of iterations needed may vary, but the process will stop after 10 iterations at most.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\n", "entry_point": "k_means_clustering", "starter_code": "def k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:", "reference_code": "\ndef euclidean_distance(a, b):\n    return np.sqrt(((a - b) ** 2).sum(axis=1))\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for iteration in range(max_iterations):\n        # Assign points to the nearest centroid\n        distances = np.array([euclidean_distance(points, centroid) for centroid in centroids])\n        assignments = np.argmin(distances, axis=0)\n\n        new_centroids = np.array([points[assignments == i].mean(axis=0) if len(points[assignments == i]) > 0 else centroids[i] for i in range(k)])\n        \n        # Check for convergence\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n        centroids = np.round(centroids,4)\n    return [tuple(centroid) for centroid in centroids]\n", "test_cases": ["\nassert k_means_clustering([(1, 2), (1, 4), (1, 0), (10, 2), (10, 4), (10, 0)], 2, [(1, 1), (10, 1)], 10) == [(1.0, 2.0), (10.0, 2.0)]\n", "\nassert k_means_clustering([(0, 0, 0), (2, 2, 2), (1, 1, 1), (9, 10, 9), (10, 11, 10), (12, 11, 12)], 2, [(1, 1, 1), (10, 10, 10)], 10) == [(1.0, 1.0, 1.0), (10.3333, 10.6667, 10.3333)]\n", "\nassert k_means_clustering([(1, 1), (2, 2), (3, 3), (4, 4)], 1, [(0,0)], 10) == [(2.5, 2.5)]\n", "\nassert k_means_clustering([(0, 0), (1, 0), (0, 1), (1, 1), (5, 5), (6, 5), (5, 6), (6, 6),(0, 5), (1, 5), (0, 6), (1, 6), (5, 0), (6, 0), (5, 1), (6, 1)], 4, [(0, 0), (0, 5), (5, 0), (5, 5)], 10) == [(0.5, 0.5), (0.5, 5.5), (5.5, 0.5), (5.5, 5.5)]\n"]}
{"id": 18, "difficulty": "easy", "category": "Machine Learning", "title": "Implement K-Fold Cross-Validation", "description": "\nWrite a Python function to generate train and test splits for K-Fold Cross-Validation. Your task is to divide the dataset into k folds and return a list of train-test indices for each fold.\n", "examples": [{"input": "\nk_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False)\n", "output": "\n[([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\n"}], "reasoning": ["\nThe function splits the dataset into 5 folds without shuffling and returns train-test splits for each iteration.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\n", "entry_point": "k_fold_cross_validation", "starter_code": "def k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):", "reference_code": "\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Return train and test indices for k-fold cross-validation.\n    \"\"\"\n    n_samples = len(X)\n    indices = np.arange(n_samples)\n    \n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    \n    fold_sizes = np.full(k, n_samples // k, dtype=int)\n    fold_sizes[:n_samples % k] += 1\n\n    current = 0\n    folds = []\n    for fold_size in fold_sizes:\n        folds.append(indices[current:current + fold_size])\n        current += fold_size\n\n    result = []\n    for i in range(k):\n        test_idx = folds[i]\n        train_idx = np.concatenate(folds[:i] + folds[i+1:])\n        result.append((train_idx.tolist(), test_idx.tolist()))\n    \n    return result\n", "test_cases": ["\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=5, shuffle=False) == [([2, 3, 4, 5, 6, 7, 8, 9], [0, 1]), ([0, 1, 4, 5, 6, 7, 8, 9], [2, 3]), ([0, 1, 2, 3, 6, 7, 8, 9], [4, 5]), ([0, 1, 2, 3, 4, 5, 8, 9], [6, 7]), ([0, 1, 2, 3, 4, 5, 6, 7], [8, 9])]\n", "\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=True, random_seed=42) == [([2, 9, 4, 3, 6], [8, 1, 5, 0, 7]), ([8, 1, 5, 0, 7], [2, 9, 4, 3, 6])]\n", "\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]), k=3, shuffle=False) == [([5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4, 10, 11, 12, 13, 14], [5, 6, 7, 8, 9]), ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14])]\n", "\nassert k_fold_cross_validation(np.array([0,1,2,3,4,5,6,7,8,9]), np.array([0,1,2,3,4,5,6,7,8,9]), k=2, shuffle=False) == [([5, 6, 7, 8, 9], [0, 1, 2, 3, 4]), ([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])]\n"]}
{"id": 19, "difficulty": "medium", "category": "Machine Learning", "title": "Principal Component Analysis (PCA) Implementation", "description": "\nWrite a Python function that performs Principal Component Analysis (PCA) from scratch. The function should take a 2D NumPy array as input, where each row represents a data sample and each column represents a feature. The function should standardize the dataset, compute the covariance matrix, find the eigenvalues and eigenvectors, and return the principal components (the eigenvectors corresponding to the largest eigenvalues). The function should also take an integer k as input, representing the number of principal components to return.\n", "examples": [{"input": "\ndata = np.array([[1, 2], [3, 4], [5, 6]]), k = 1\n", "output": "\n[[0.7071], [0.7071]]\n"}], "reasoning": ["\nAfter standardizing the data and computing the covariance matrix, the eigenvalues and eigenvectors are calculated. The largest eigenvalue's corresponding eigenvector is returned as the principal component, rounded to four decimal places.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\n", "entry_point": "pca", "starter_code": "def pca(data: np.ndarray, k: int) -> list[list[float]]:", "reference_code": "\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Standardize the data\n    data_standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Compute the covariance matrix\n    covariance_matrix = np.cov(data_standardized, rowvar=False)\n    \n    # Eigen decomposition\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n    \n    # Sort the eigenvectors by decreasing eigenvalues\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues_sorted = eigenvalues[idx]\n    eigenvectors_sorted = eigenvectors[:,idx]\n    \n    # Select the top k eigenvectors (principal components)\n    principal_components = eigenvectors_sorted[:, :k]\n    \n    return np.round(principal_components, 4).tolist()\n", "test_cases": ["\nassert pca(np.array([[4,2,1],[5,6,7],[9,12,1],[4,6,7]]),2) == [[0.6855, 0.0776], [0.6202, 0.4586], [-0.3814, 0.8853]]\n", "\nassert pca(np.array([[1, 2], [3, 4], [5, 6]]), 1) == [[0.7071], [0.7071]]\n"]}
{"id": 20, "difficulty": "hard", "category": "Machine Learning", "title": "Decision Tree Learning", "description": "\nWrite a Python function that implements the decision tree learning algorithm for classification. The function should use recursive binary splitting based on entropy and information gain to build a decision tree. It should take a list of examples (each example is a dict of attribute-value pairs) and a list of attribute names as input, and return a nested dictionary representing the decision tree.\n", "examples": [{"input": "\nexamples = [\n    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'No'},\n    {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Strong', 'PlayTennis': 'No'},\n    {'Outlook': 'Overcast', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'},\n    {'Outlook': 'Rain', 'Temperature': 'Mild', 'Humidity': 'High', 'Wind': 'Weak', 'PlayTennis': 'Yes'}\n],\nattributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n", "output": "\n{\n    'Outlook': {\n        'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}},\n        'Overcast': 'Yes',\n        'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}\n    }\n}\n"}], "reasoning": ["\nUsing the given examples, the decision tree algorithm determines that 'Outlook' is the best attribute to split the data initially. When 'Outlook' is 'Overcast', the outcome is always 'Yes', so it becomes a leaf node. In cases of 'Sunny' and 'Rain', it further splits based on 'Humidity' and 'Wind', respectively. The resulting tree structure is able to classify the training examples with the attributes 'Outlook', 'Temperature', 'Humidity', and 'Wind'.\n"], "import_code": "\nimport math\nfrom collections import Counter\n", "output_constrains": "\n", "entry_point": "learn_decision_tree", "starter_code": "def learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:", "reference_code": "\ndef calculate_entropy(labels):\n    label_counts = Counter(labels)\n    total_count = len(labels)\n    entropy = -sum((count / total_count) * math.log2(count / total_count) for count in label_counts.values())\n    return entropy\n\ndef calculate_information_gain(examples, attr, target_attr):\n    total_entropy = calculate_entropy([example[target_attr] for example in examples])\n    values = set(example[attr] for example in examples)\n    attr_entropy = 0\n    for value in values:\n        value_subset = [example[target_attr] for example in examples if example[attr] == value]\n        value_entropy = calculate_entropy(value_subset)\n        attr_entropy += (len(value_subset) / len(examples)) * value_entropy\n    return total_entropy - attr_entropy\n\ndef majority_class(examples, target_attr):\n    return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    if not examples:\n        return 'No examples'\n    if all(example[target_attr] == examples[0][target_attr] for example in examples):\n        return examples[0][target_attr]\n    if not attributes:\n        return majority_class(examples, target_attr)\n    \n    gains = {attr: calculate_information_gain(examples, attr, target_attr) for attr in attributes}\n    best_attr = max(gains, key=gains.get)\n    tree = {best_attr: {}}\n    \n    for value in set(example[best_attr] for example in examples):\n        subset = [example for example in examples if example[best_attr] == value]\n        new_attributes = attributes.copy()\n        new_attributes.remove(best_attr)\n        subtree = learn_decision_tree(subset, new_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    \n    return tree\n", "test_cases": ["\nassert learn_decision_tree([ {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'No'}, {'Outlook': 'Overcast', 'Wind': 'Strong', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Sunny', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Sunny', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Overcast', 'Wind': 'Weak', 'PlayTennis': 'Yes'}, {'Outlook': 'Rain', 'Wind': 'Strong', 'PlayTennis': 'No'}, {'Outlook': 'Rain', 'Wind': 'Weak', 'PlayTennis': 'Yes'} ], ['Outlook', 'Wind'], 'PlayTennis') == {'Outlook': {'Sunny': {'Wind': {'Weak': 'No', 'Strong': 'No'}}, 'Rain': {'Wind': {'Weak': 'Yes', 'Strong': 'No'}}, 'Overcast': 'Yes'}}\n"]}
{"id": 21, "title": "Pegasos Kernel SVM Implementation", "description": "\nWrite a Python function that implements a deterministic version of the Pegasos algorithm to train a kernel SVM classifier from scratch. The function should take a dataset (as a 2D NumPy array where each row represents a data sample and each column represents a feature), a label vector (1D NumPy array where each entry corresponds to the label of the sample), and training parameters such as the choice of kernel (linear or RBF), regularization parameter (lambda), and the number of iterations. Note that while the original Pegasos algorithm is stochastic (it selects a single random sample at each step), this problem requires using all samples in every iteration (i.e., no random sampling). The function should perform binary classification and return the model's alpha coefficients and bias.\n", "difficulty": "hard", "category": "Machine Learning", "examples": [{"input": "\ndata = np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), labels = np.array([1, 1, -1, -1]), kernel = 'rbf', lambda_val = 0.01, iterations = 100\n", "output": "\nalpha = [0.03, 0.02, 0.05, 0.01], b = -0.05\n"}], "reasoning": ["\nUsing the RBF kernel, the Pegasos algorithm iteratively updates the weights based on a sub-gradient descent method, taking into account the non-linear separability of the data induced by the kernel transformation.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100) -> (list, float):", "entry_point": "pegasos_kernel_svm", "reference_code": "\ndef linear_kernel(x, y):\n    return np.dot(x, y)\n\ndef rbf_kernel(x, y, sigma=1.0):\n    return np.exp(-np.linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n\ndef pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = len(data)\n    alphas = np.zeros(n_samples)\n    b = 0\n\n    for t in range(1, iterations + 1):\n        for i in range(n_samples):\n            eta = 1.0 / (lambda_val * t)\n            if kernel == 'linear':\n                kernel_func = linear_kernel\n            elif kernel == 'rbf':\n                kernel_func = lambda x, y: rbf_kernel(x, y, sigma)\n    \n            decision = sum(alphas[j] * labels[j] * kernel_func(data[j], data[i]) for j in range(n_samples)) + b\n            if labels[i] * decision < 1:\n                alphas[i] += eta * (labels[i] - lambda_val * alphas[i])\n                b += eta * labels[i]\n\n    return np.round(alphas,4).tolist(), np.round(b,4)\n", "test_cases": ["\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='linear', lambda_val=0.01, iterations=100) == ([100.0, 0.0, -100.0, -100.0], -937.4755)\n", "\nassert pegasos_kernel_svm(np.array([[1, 2], [2, 3], [3, 1], [4, 1]]), np.array([1, 1, -1, -1]), kernel='rbf', lambda_val=0.01, iterations=100, sigma=0.5) == ([100.0, 99.0, -100.0, -100.0], -115.0)\n"]}
{"id": 22, "title": "Sigmoid Activation Function Understanding", "description": "\nWrite a Python function that computes the output of the sigmoid activation function given an input value z. The function should return the output rounded to four decimal places.   \n", "difficulty": "easy", "category": "Deep Learning", "examples": [{"input": "\nz = 0\n", "output": "\n0.5\n"}], "reasoning": ["\nThe sigmoid function is defined as $\\sigma (z) = 1 / (1 + exp(-z))$. For z = 0, exp(-0) = 1, hence the output is 1 / (1 + 1) = 0.5.\n"], "import_code": "\nimport math\n", "output_constrains": "\n", "starter_code": "def sigmoid(z: float) -> float:", "entry_point": "sigmoid", "reference_code": "\ndef sigmoid(z: float) -> float:\n   result = 1 / (1 + math.exp(-z))\n   return round(result, 4)\n", "test_cases": ["\nassert sigmoid(0) == 0.5\n", "\nassert sigmoid(1) == 0.7311\n", "\nassert sigmoid(-1) == 0.2689\n"]}
{"id": 23, "title": "Softmax Activation Function Implementation", "description": "\nWrite a Python function that computes the softmax activation for a given list of scores. The function should return the softmax values as a list, each rounded to four decimal places.\n", "difficulty": "easy", "category": "Deep Learning", "examples": [{"input": "\nscores = [1, 2, 3]\n", "output": "\n[0.0900, 0.2447, 0.6652]\n"}], "reasoning": ["\nThe softmax function converts a list of values into a probability distribution. The probabilities are proportional to the exponential of each element divided by the sum of the exponentials of all elements in the list.\n"], "import_code": "\nimport math\n", "output_constrains": "\n", "starter_code": "def softmax(scores: list[float]) -> list[float]:", "entry_point": "softmax", "reference_code": "\ndef softmax(scores: list[float]) -> list[float]:\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    probabilities = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return probabilities\n", "test_cases": ["\nassert softmax([1, 2, 3]) == [0.09, 0.2447, 0.6652]\n", "\nassert softmax([1, 1, 1]) == [0.3333, 0.3333, 0.3333]\n", "\nassert softmax([-1, 0, 5]) == [0.0025, 0.0067, 0.9909]\n"]}
{"id": 24, "title": "Single Neuron", "description": "\nWrite a Python function that simulates a single neuron with a sigmoid activation function for binary classification, handling multidimensional input features. The function should take a list of feature vectors (each vector representing multiple features for an example), associated true binary labels, and the neuron's weights (one for each feature) and bias as input. It should return the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places.\n", "difficulty": "easy", "category": "Deep Learning", "examples": [{"input": "\nfeatures = [[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], labels = [0, 1, 0], weights = [0.7, -0.4], bias = -0.1\n", "output": "\n([0.4626, 0.4134, 0.6682], 0.3349)\n"}], "reasoning": ["\nFor each input vector, the weighted sum is calculated by multiplying each feature by its corresponding weight, adding these up along with the bias, then applying the sigmoid function to produce a probability. The MSE is calculated as the average squared difference between each predicted probability and the corresponding true label.\n"], "import_code": "\nimport math\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):", "entry_point": "single_neuron_model", "reference_code": "\ndef single_neuron_model(features, labels, weights, bias):\n    probabilities = []\n    for feature_vector in features:\n        z = sum(weight * feature for weight, feature in zip(weights, feature_vector)) + bias\n        prob = 1 / (1 + math.exp(-z))\n        probabilities.append(round(prob, 4))\n    \n    mse = sum((prob - label) ** 2 for prob, label in zip(probabilities, labels)) / len(labels)\n    mse = round(mse, 4)\n    \n    return probabilities, mse\n", "test_cases": ["\nassert single_neuron_model([[0.5, 1.0], [-1.5, -2.0], [2.0, 1.5]], [0, 1, 0], [0.7, -0.4], -0.1) == ([0.4626, 0.4134, 0.6682], 0.3349)\n", "\nassert single_neuron_model([[1, 2], [2, 3], [3, 1]], [1, 0, 1], [0.5, -0.2], 0) == ([0.525, 0.5987, 0.7858], 0.21)\n"]}
{"id": 25, "title": "Single Neuron with Backpropagation", "description": "\nWrite a Python function that simulates a single neuron with sigmoid activation, and implements backpropagation to update the neuron's weights and bias. The function should take a list of feature vectors, associated true binary labels, initial weights, initial bias, a learning rate, and the number of epochs. The function should update the weights and bias using gradient descent based on the MSE loss, and return the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places.\n", "difficulty": "medium", "category": "Deep Learning", "examples": [{"input": "\nfeatures = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], labels = [1, 0, 0], initial_weights = [0.1, -0.2], initial_bias = 0.0, learning_rate = 0.1, epochs = 2\n", "output": "\nupdated_weights = [0.1036, -0.1425], updated_bias = -0.0167, mse_values = [0.3033, 0.2942]\n"}], "reasoning": ["\nThe neuron receives feature vectors and computes predictions using the sigmoid activation. Based on the predictions and true labels, the gradients of MSE loss with respect to weights and bias are computed and used to update the model parameters across epochs.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):", "entry_point": "train_neuron", "reference_code": "\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs):\n    weights = np.array(initial_weights)\n    bias = initial_bias\n    features = np.array(features)\n    labels = np.array(labels)\n    mse_values = []\n\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        predictions = sigmoid(z)\n        \n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n\n        # Gradient calculation for weights and bias\n        errors = predictions - labels\n        weight_gradients = (2/len(labels)) * np.dot(features.T, errors * predictions * (1 - predictions))\n        bias_gradient = (2/len(labels)) * np.sum(errors * predictions * (1 - predictions))\n        \n        # Update weights and bias\n        weights -= learning_rate * weight_gradients\n        bias -= learning_rate * bias_gradient\n\n        # Round weights and bias for output\n        updated_weights = np.round(weights, 4)\n        updated_bias = round(bias, 4)\n\n    return updated_weights.tolist(), updated_bias, mse_values\n", "test_cases": ["\nassert train_neuron(np.array([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]), np.array([1, 0, 0]), np.array([0.1, -0.2]), 0.0, 0.1, 2) == ([0.1036, -0.1425], -0.0167, [0.3033, 0.2942])\n", "\nassert train_neuron(np.array([[1, 2], [2, 3], [3, 1]]), np.array([1, 0, 1]), np.array([0.5, -0.2]), 0, 0.1, 3) == ([0.4892, -0.2301], 0.0029, [0.21, 0.2087, 0.2076])\n"]}
{"id": 26, "title": "Implementing Basic Autograd Operations", "description": "\nSpecial thanks to Andrej Karpathy for making a video about this, if you haven't already check out his videos on YouTube https://youtu.be/VMj-3S1tku0?si=gjlnFP4o3JRN9dTg. Write a Python class similar to the provided 'Value' class that implements the basic autograd operations: addition, multiplication, and ReLU activation. The class should handle scalar values and should correctly compute gradients for these operations through automatic differentiation.\n", "difficulty": "medium", "category": "Deep Learning", "examples": [{"input": "\na = Value(2)\n        b = Value(-3)\n        c = Value(10)\n        d = a + b * c\n        e = d.relu()\n        e.backward()\n        print(a, b, c, d, e)\n", "output": "\nValue(data=2, grad=0) Value(data=-3, grad=0) Value(data=10, grad=0)\n"}], "reasoning": ["\nThe output reflects the forward computation and gradients after backpropagation. The ReLU on 'd' zeros out its output and gradient due to the negative data value.\n"], "import_code": "\n", "output_constrains": "\n", "starter_code": "class Value:\n\tdef __init__(self, data, _children=(), _op=''):\n\t\tself.data = data\n\t\tself.grad = 0\n\t\tself._backward = lambda: None\n\t\tself._prev = set(_children)\n\t\tself._op = _op\n\tdef __repr__(self):\n\t\treturn f\"Value(data={self.data}, grad={self.grad})\"\n\n\tdef __add__(self, other):\n\t\t # Implement addition here\n\t\tpass\n\n\tdef __mul__(self, other):\n\t\t# Implement multiplication here\n\t\tpass\n\n\tdef relu(self):\n\t\t# Implement ReLU here\n\t\tpass\n\n\tdef backward(self):\n\t\t# Implement backward pass here\n\t\tpass", "entry_point": "Value", "reference_code": "\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for v in reversed(topo):\n            v._backward()\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n", "test_cases": ["\n        a = Value(2)\n        b = Value(3)\n        c = Value(10)\n        d = a + b * c \n        e = Value(7) * Value(2)\n        f = e + d\n        g = f.relu() \n        g.backward() \n        values = [a,b,c,d,e,f,g]\n        expected = [(2, 1), (3, 10), (10, 3), (32, 1), (14, 1), (46, 1), (46, 1)]\n        assert all((v.data, v.grad) == exp for v, exp in zip(values, expected))\n"]}
{"id": 27, "title": "Transformation Matrix from Basis B to C", "description": "\nGiven basis vectors in two different bases B and C for R^3, write a Python function to compute the transformation matrix P from basis B to C.\n", "difficulty": "easy", "category": "Linear Algebra", "examples": [{"input": "\nB = [[1, 0, 0], \n             [0, 1, 0], \n             [0, 0, 1]]\n        C = [[1, 2.3, 3], \n             [4.4, 25, 6], \n             [7.4, 8, 9]]\n", "output": "\n[[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\n"}], "reasoning": ["\nThe transformation matrix P from basis B to C can be found using matrix operations involving the inverse of matrix C.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:", "entry_point": "transform_basis", "reference_code": "\ndef transform_basis(B, C):\n    C = np.array(C)\n    B = np.array(B)\n    C_inv = np.linalg.inv(C)\n    P = np.dot(C_inv, B)\n    return np.round(P, 4).tolist()\n", "test_cases": ["\nassert transform_basis([[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[1, 2.3, 3], [4.4, 25, 6], [7.4, 8, 9]]) == [[-0.6772, -0.0126, 0.2342], [-0.0184, 0.0505, -0.0275], [0.5732, -0.0345, -0.0569]]\n", "\nassert transform_basis([[1,0],[0,1]],[[1,2],[9,2]]) == [[-0.125, 0.125 ],[ 0.5625, -0.0625]]\n", "\nassert transform_basis([[-1, 0], [3, 4]], [[2, -1], [0, 1]]) == [[1, 2], [3, 4]]\n", "\nassert transform_basis([[4, 8], [2, 4]], [[2, 1], [0, 1]]) == [[1, 2], [2, 4]]\n"]}
{"id": 28, "title": "SVD of a 2x2 Matrix using eigen values & vectors", "description": "\nGiven a 2x2 matrix, write a Python function to compute its Singular Value Decomposition (SVD). The function should return the matrices U, S, and V such that A = U * S * V, use the method described in this post https://metamerist.blogspot.com/2006/10/linear-algebra-for-graphics-geeks-svd.html\n", "difficulty": "hard", "category": "Linear Algebra", "examples": [{"input": "\nA = [[-10, 8], \n         [10, -1]]\n", "output": "\n([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\n"}], "reasoning": ["\nThe SVD of the matrix A is calculated using the eigenvalues and eigenvectors of A^T A and A A^T. The singular values are the square roots of the eigenvalues, and the eigenvectors form the columns of matrices U and V.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def svd_2x2(A: np.ndarray) -> tuple:", "entry_point": "svd_2x2", "reference_code": "\ndef svd_2x2(A: np.ndarray) -> tuple:\n    y1, x1 = (A[1, 0] + A[0, 1]), (A[0, 0] - A[1, 1])\n    y2, x2 = (A[1, 0] - A[0, 1]), (A[0, 0] + A[1, 1])\n\n    h1 = np.sqrt(y1**2 + x1**2)\n    h2 = np.sqrt(y2**2 + x2**2)\n\n    t1 = x1 / h1\n    t2 = x2 / h2\n\n    cc = np.sqrt((1.0 + t1) * (1.0 + t2))\n    ss = np.sqrt((1.0 - t1) * (1.0 - t2))\n    cs = np.sqrt((1.0 + t1) * (1.0 - t2))\n    sc = np.sqrt((1.0 - t1) * (1.0 + t2))\n\n    c1, s1 = (cc - ss) / 2.0, (sc + cs) / 2.0\n    U = np.array([[-c1, -s1], [-s1, c1]])\n\n    s = np.array([(h1 + h2) / 2.0, abs(h1 - h2) / 2.0])\n\n    V = np.diag(1.0 / s) @ U.T @ A\n\n    return np.round(U, 4).tolist(), np.round(s, 4).tolist(), np.round(V, 4).tolist()\n    \n", "test_cases": ["\nassert svd_2x2(np.array([[-10, 8], [10, -1]])) == ([[0.8, -0.6], [-0.6, -0.8]], [15.6525, 4.4721], [[-0.8944, 0.4472], [-0.4472, -0.8944]])\n", "\nassert svd_2x2(np.array([[1, 2], [3, 4]])) == ([[-0.4046, -0.9145], [-0.9145, 0.4046]], [5.465, 0.366], [[-0.576, -0.8174], [0.8174, -0.576]])\n"]}
{"id": 29, "title": "Random Shuffle of Dataset", "description": "\nWrite a Python function to perform a random shuffle of the samples in two numpy arrays, X and y, while maintaining the corresponding order between them. The function should have an optional seed parameter for reproducibility.\n", "difficulty": "easy", "category": "Machine Learning", "examples": [{"input": "\nX = np.array([[1, 2], \n                  [3, 4], \n                  [5, 6], \n                  [7, 8]])\n    y = np.array([1, 2, 3, 4])\n", "output": "\n([[5, 6], [1, 2], [7, 8], [3, 4]]), [3, 1, 4, 2])\n"}], "reasoning": ["\nThe samples in X and y are shuffled randomly, maintaining the correspondence between the samples in both arrays.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def shuffle_data(X, y, seed=None):", "entry_point": "shuffle_data", "reference_code": "\ndef shuffle_data(X, y, seed=None):\n    if seed:\n        np.random.seed(seed)\n    idx = np.arange(X.shape[0])\n    np.random.shuffle(idx)\n    return X[idx].tolist(), y[idx].tolist()\n    \n", "test_cases": ["\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([1, 2, 3, 4]), seed=42) == ([[3, 4], [7, 8], [1, 2], [5, 6]], [2, 4, 1, 3])\n", "\nassert shuffle_data(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), seed=24) == ([[4, 4],[2, 2],[1, 1],[3, 3]], [40, 20, 10, 30])\n", "\nassert shuffle_data(np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), np.array([4, 6, 7, 8]), seed=10) == ([[5, 6], [1, 2], [7, 8], [3, 4]], [7, 4, 8, 6])         \n", "\nassert shuffle_data(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), seed=20) == ([[1, 3], [3, 6], [5, 8], [7, 11]], [4, 5, 6, 7])         \n"]}
{"id": 30, "title": "Batch Iterator for Dataset", "description": "\nImplement a batch iterable function that samples in a numpy array X and an optional numpy array y. The function should yield batches of a specified size. If y is provided, the function should yield batches of (X, y) pairs; otherwise, it should yield batches of X only.\n", "difficulty": "easy", "category": "Machine Learning", "examples": [{"input": "\nX = np.array([[1, 2], \n                  [3, 4], \n                  [5, 6], \n                  [7, 8], \n                  [9, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n    batch_size = 2\n    batch_iterator(X, y, batch_size)\n", "output": "\n[[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\n"}], "reasoning": ["\nThe dataset X contains 5 samples, and we are using a batch size of 2. Therefore, the function will divide the dataset into 3 batches. The first two batches will contain 2 samples each, and the last batch will contain the remaining sample. The corresponding values from y are also included in each batch.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def batch_iterator(X, y=None, batch_size=64):", "entry_point": "batch_iterator", "reference_code": "\ndef batch_iterator(X, y=None, batch_size=64):\n    n_samples = X.shape[0]\n    batches = []\n    for i in np.arange(0, n_samples, batch_size):\n        begin, end = i, min(i+batch_size, n_samples)\n        if y is not None:\n            batches.append([X[begin:end].tolist(), y[begin:end].tolist()])\n        else:\n            batches.append( X[begin:end].tolist())\n    return batches\n", "test_cases": ["\nassert batch_iterator(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), batch_size=2) == [[[[1, 2], [3, 4]], [1, 2]], [[[5, 6], [7, 8]], [3, 4]], [[[9, 10]], [5]]]\n", "\nassert batch_iterator(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), batch_size=3) == [[[1, 1], [2, 2], [3, 3]], [[4, 4]]]\n", "\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), batch_size=2) == [[[1, 3], [3, 6]], [[5, 8], [7, 11]]]\n", "\nassert batch_iterator(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([4, 5, 6, 7]), batch_size=2) == [[[[1, 3], [3, 6]], [4, 5]], [[[5, 8], [7, 11]], [6, 7]]]\n"]}
{"id": 31, "title": "Divide Dataset Based on Feature Threshold", "description": "\nWrite a Python function to divide a dataset based on whether the value of a specified feature is greater than or equal to a given threshold. The function should return two subsets of the dataset: one with samples that meet the condition and another with samples that do not.\n", "difficulty": "medium", "category": "Machine Learning", "examples": [{"input": "\nX = np.array([[1, 2], \n                  [3, 4], \n                  [5, 6], \n                  [7, 8], \n                  [9, 10]])\n    feature_i = 0\n    threshold = 5\n", "output": "\n[[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\n"}], "reasoning": ["\nThe dataset X is divided based on whether the value in the 0th feature (first column) is greater than or equal to 5. Samples with the first column value >= 5 are in the first subset, and the rest are in the second subset.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def divide_on_feature(X, feature_i, threshold):", "entry_point": "divide_on_feature", "reference_code": "\ndef divide_on_feature(X, feature_i, threshold):\n    # Define the split function based on the threshold type\n    split_func = None\n    if isinstance(threshold, int) or isinstance(threshold, float):\n        # For numeric threshold, check if feature value is greater than or equal to the threshold\n        split_func = lambda sample: sample[feature_i] >= threshold\n    else:\n        # For non-numeric threshold, check if feature value is equal to the threshold\n        split_func = lambda sample: sample[feature_i] == threshold\n\n    # Create two subsets based on the split function\n    X_1 = np.array([sample for sample in X if split_func(sample)]).tolist()\n    X_2 = np.array([sample for sample in X if not split_func(sample)]).tolist()\n\n    # Return the two subsets\n    return [X_1, X_2]\n", "test_cases": ["\nassert divide_on_feature(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), 0, 5) == [[[5, 6], [7, 8], [9, 10]], [[1, 2], [3, 4]]]\n", "\nassert divide_on_feature(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), 1, 3) == [[[3, 3], [4, 4]], [[1, 1], [2, 2]]]\n", "\nassert divide_on_feature(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), 0, 2) ==  [[[3, 6], [5, 8], [7, 11]], [[1, 3]]]\n", "\nassert divide_on_feature(np.array([[1, 3, 9], [6, 3, 6], [10, 5, 8], [9, 7, 11]]), 1, 5) ==  [[[10, 5, 8], [9, 7, 11]], [[1, 3, 9], [6, 3, 6]]]\n"]}
{"id": 32, "title": "Generate Polynomial Features", "description": "\nWrite a Python function to generate polynomial features for a given dataset. The function should take in a 2D numpy array X and an integer degree, and return a new 2D numpy array with polynomial features up to the specified degree.\n", "difficulty": "medium", "category": "Machine Learning", "examples": [{"input": "\nX = np.array([[2, 3],\n                  [3, 4],\n                  [5, 6]])\n    degree = 2\n    output = polynomial_features(X, degree)\n    print(output)\n", "output": "\n[[  1.   2.   3.   4.   6.   9.]\n     [  1.   3.   4.   9.  12.  16.]\n     [  1.   5.   6.  25.  30.  36.]]\n"}], "reasoning": ["\nFor each sample in X, the function generates all polynomial combinations of the features up to the given degree. For degree=2, it includes combinations like [x1^0, x1^1, x1^2, x2^0, x2^1, x2^2, x1^1*x2^1], where x1 and x2 are the features.\n"], "import_code": "\nimport numpy as np\nfrom itertools import combinations_with_replacement\n", "output_constrains": "\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def polynomial_features(X, degree):", "entry_point": "polynomial_features", "reference_code": "\ndef polynomial_features(X, degree):\n    n_samples, n_features = np.shape(X)\n\n    # Generate all combinations of feature indices for polynomial terms\n    def index_combinations():\n        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n        flat_combs = [item for sublist in combs for item in sublist]\n        return flat_combs\n    \n    combinations = index_combinations()\n    n_output_features = len(combinations)\n    X_new = np.empty((n_samples, n_output_features))\n    \n    # Compute polynomial features\n    for i, index_combs in enumerate(combinations):  \n        X_new[:, i] = np.prod(X[:, index_combs], axis=1)\n\n    return X_new.tolist()\n", "test_cases": ["\nassert polynomial_features(np.array([[2, 3], [3, 4], [5, 6]]), 2) == [[ 1., 2., 3., 4., 6., 9.], [ 1., 3., 4., 9., 12., 16.], [ 1., 5., 6., 25., 30., 36.]]\n", "\nassert polynomial_features(np.array([[1, 2], [3, 4], [5, 6]]), 3) == [[ 1., 1., 2., 1., 2., 4., 1., 2., 4., 8.], [ 1., 3., 4., 9., 12., 16., 27., 36., 48., 64.], [ 1., 5., 6., 25., 30., 36., 125., 150., 180., 216.]]\n", "\nassert polynomial_features(np.array([[1, 2, 3], [3, 4, 5], [5, 6, 9]]), 3) == [[ 1., 1., 2., 3., 1., 2., 3., 4., 6., 9., 1., 2., 3., 4., 6., 9., 8., 12., 18., 27.], [ 1., 3., 4., 5., 9., 12., 15., 16., 20., 25., 27., 36., 45., 48., 60., 75., 64., 80., 100., 125.],[ 1., 5., 6., 9., 25., 30., 45., 36., 54., 81., 125., 150., 225., 180., 270., 405., 216., 324., 486., 729.]]\n"]}
{"id": 33, "title": "Generate Random Subsets of a Dataset", "description": "\nWrite a Python function to generate random subsets of a given dataset. The function should take in a 2D numpy array X, a 1D numpy array y, an integer n_subsets, and a boolean replacements. It should return a list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset). If replacements is True, the subsets should be created with replacements; otherwise, without replacements.\n", "difficulty": "medium", "category": "Machine Learning", "examples": [{"input": "\nX = np.array([[1, 2],\n                  [3, 4],\n                  [5, 6],\n                  [7, 8],\n                  [9, 10]])\n    y = np.array([1, 2, 3, 4, 5])\n    n_subsets = 3\n    replacements = False\n    get_random_subsets(X, y, n_subsets, replacements)\n", "output": "\n[([[7, 8], [1, 2]], [4, 1]), ([[9, 10], [5, 6]], [5, 3]), ([[3, 4], [5, 6]], [2, 3])]\n"}], "reasoning": ["\nThe function generates three random subsets of the dataset without replacements. Each subset includes 50% of the samples (since replacements=False). The samples are randomly selected without duplication.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def get_random_subsets(X, y, n_subsets, replacements=True, seed=42):", "entry_point": "get_random_subsets", "reference_code": "\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n\n    n, m = X.shape\n    \n    subset_size = n if replacements else n // 2\n    idx = np.array([np.random.choice(n, subset_size, replace=replacements) for _ in range(n_subsets)])\n    # convert all ndarrays to lists\n    return [(X[idx][i].tolist(), y[idx][i].tolist()) for i in range(n_subsets)]\n", "test_cases": ["\nassert get_random_subsets(np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]), np.array([1, 2, 3, 4, 5]), 3, False, seed=42) == [([[3, 4], [9, 10]], [2, 5]), ([[7, 8], [3, 4]], [4, 2]), ([[3, 4], [1, 2]], [2, 1])]\n", "\nassert get_random_subsets(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]), np.array([10, 20, 30, 40]), 1, True, seed=42) == [([[3, 3], [4, 4], [1, 1], [3, 3]], [30, 40, 10, 30])]\n"]}
{"id": 34, "title": "One-Hot Encoding of Nominal Values", "description": "\nWrite a Python function to perform one-hot encoding of nominal values. The function should take in a 1D numpy array x of integer values and an optional integer n_col representing the number of columns for the one-hot encoded array. If n_col is not provided, it should be automatically determined from the input array.\n", "difficulty": "easy", "category": "Machine Learning", "examples": [{"input": "\nx = np.array([0, 1, 2, 1, 0])\n    output = to_categorical(x)\n    print(output)\n", "output": "\n[[1. 0. 0.], [0. 1. 0.], [0. 0. 1.], [0. 1. 0.], [1. 0. 0.]]\n"}], "reasoning": ["\nEach element in the input array is transformed into a one-hot encoded vector, where the index corresponding to the value in the input array is set to 1, and all other indices are set to 0.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def to_categorical(x, n_col=None):", "entry_point": "to_categorical", "reference_code": "\ndef to_categorical(x, n_col=None):\n    # One-hot encoding of nominal values\n    # If n_col is not provided, determine the number of columns from the input array\n    if not n_col:\n        n_col = np.amax(x) + 1\n    # Initialize a matrix of zeros with shape (number of samples, n_col)\n    one_hot = np.zeros((x.shape[0], n_col))\n    # Set the appropriate elements to 1\n    one_hot[np.arange(x.shape[0]), x] = 1\n    return one_hot.tolist()\n", "test_cases": ["\nassert to_categorical(np.array([0, 1, 2, 1, 0])) == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.]]\n", "\nassert to_categorical(np.array([3, 1, 2, 1, 3]), 4) == [[0., 0., 0., 1.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 1., 0., 0.], [0., 0., 0., 1.]]\n", "\nassert to_categorical(np.array([2, 3, 4, 1, 1]), 5) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\n", "\nassert to_categorical(np.array([2, 4, 1, 1])) == [[0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]]\n"]}
{"id": 35, "title": "Convert Vector to Diagonal Matrix", "description": "\nWrite a Python function to convert a 1D numpy array into a diagonal matrix. The function should take in a 1D numpy array x and return a 2D numpy array representing the diagonal matrix.\n", "difficulty": "easy", "category": "Linear Algebra", "examples": [{"input": "\nx = np.array([1, 2, 3])\n    output = make_diagonal(x)\n    print(output)\n", "output": "\n[[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\n"}], "reasoning": ["\nThe input vector [1, 2, 3] is converted into a diagonal matrix where the elements of the vector form the diagonal of the matrix.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def make_diagonal(x):", "entry_point": "make_diagonal", "reference_code": "\ndef make_diagonal(x):\n    identity_matrix = np.identity(np.size(x))\n    return (identity_matrix*x).tolist()\n", "test_cases": ["\nassert make_diagonal(np.array([1, 2, 3])) == [[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]\n", "\nassert make_diagonal(np.array([4, 5, 6, 7])) == [[4., 0., 0., 0.], [0., 5., 0., 0.], [0., 0., 6., 0.], [0., 0., 0., 7.]]\n", "\nassert make_diagonal(np.array([2, 4, 1, 1])) == [[2.0, 0.0, 0.0, 0.0], [0.0, 4.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]\n", "\nassert make_diagonal(np.array([1, 3, 5, 0])) == [[1.0, 0.0, 0.0, 0.0], [0.0, 3.0, 0.0, 0.0], [0.0, 0.0, 5.0, 0.0], [0.0, 0.0, 0.0, 0.0]]\n"]}
{"id": 36, "title": "Calculate Accuracy Score", "description": "\nWrite a Python function to calculate the accuracy score of a model's predictions. The function should take in two 1D numpy arrays: y_true, which contains the true labels, and y_pred, which contains the predicted labels. It should return the accuracy score as a float.\n", "difficulty": "easy", "category": "Machine Learning", "examples": [{"input": "\ny_true = np.array([1, 0, 1, 1, 0, 1])\n    y_pred = np.array([1, 0, 0, 1, 0, 1])\n    output = accuracy_score(y_true, y_pred)\n    print(output)\n", "output": "\n# 0.8333333333333334\n"}], "reasoning": ["\nThe function compares the true labels with the predicted labels and calculates the ratio of correct predictions to the total number of predictions. In this example, there are 5 correct predictions out of 6, resulting in an accuracy score of 0.8333333333333334.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\n", "starter_code": "def accuracy_score(y_true, y_pred):", "entry_point": "accuracy_score", "reference_code": "\ndef accuracy_score(y_true, y_pred):\n    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n    return accuracy\n", "test_cases": ["\nassert accuracy_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 0, 1, 0, 1])) == 0.8333333333333334\n", "\nassert accuracy_score(np.array([1, 1, 1, 1]), np.array([1, 0, 1, 0])) == 0.5\n", "\nassert accuracy_score(np.array([1, 0, 1, 0, 1]), np.array([1, 0, 0, 1, 1])) == 0.6\n", "\nassert accuracy_score(np.array([0, 1, 0, 1]), np.array([1, 0, 1, 1])) == 0.25\n"]}
{"id": 37, "title": "Calculate Correlation Matrix", "description": "\nWrite a Python function to calculate the correlation matrix for a given dataset. The function should take in a 2D numpy array X and an optional 2D numpy array Y. If Y is not provided, the function should calculate the correlation matrix of X with itself. It should return the correlation matrix as a 2D numpy array.\n", "difficulty": "medium", "category": "Linear Algebra", "examples": [{"input": "\nX = np.array([[1, 2],\n                  [3, 4],\n                  [5, 6]])\n    output = calculate_correlation_matrix(X)\n    print(output)\n", "output": "\n[[1., 1.], [1., 1.]]\n"}], "reasoning": ["\nThe function calculates the correlation matrix for the dataset X. In this example, the correlation between the two features is 1, indicating a perfect linear relationship.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def calculate_correlation_matrix(X, Y=None):", "entry_point": "calculate_correlation_matrix", "reference_code": "\ndef calculate_correlation_matrix(X, Y=None):\n    # Helper function to calculate standard deviation\n    def calculate_std_dev(A):\n        return np.sqrt(np.mean((A - A.mean(0))**2, axis=0))\n    \n    if Y is None:\n        Y = X\n    n_samples = np.shape(X)[0]\n    # Calculate the covariance matrix\n    covariance = (1 / n_samples) * (X - X.mean(0)).T.dot(Y - Y.mean(0))\n    # Calculate the standard deviations\n    std_dev_X = np.expand_dims(calculate_std_dev(X), 1)\n    std_dev_y = np.expand_dims(calculate_std_dev(Y), 1)\n    # Calculate the correlation matrix\n    correlation_matrix = np.divide(covariance, std_dev_X.dot(std_dev_y.T))\n\n    return np.round(np.array(correlation_matrix, dtype=float), 4).tolist()\n", "test_cases": ["\nassert calculate_correlation_matrix(np.array([[1, 2], [3, 4], [5, 6]])) == [[1., 1.], [1., 1.]]\n", "\nassert calculate_correlation_matrix(np.array([[1, 2, 3], [7, 15, 6], [7, 8, 9]])) == [[1.0, 0.843, 0.866], [0.843, 1.0, 0.4611], [0.866, 0.4611, 1.0]]\n", "\nassert calculate_correlation_matrix(np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]])) == [[ -1., -1.], [ 1., 1.]]\n", "\nassert calculate_correlation_matrix(np.array([[1, 3], [3, 6], [5, 8], [7, 11]])) == [[1.0, 0.9971], [0.9971, 1.0]]\n", "\nassert calculate_correlation_matrix(np.array([[1, 4], [3, 6]]), np.array([[8, 9], [7, 11]])) == [[-1.0, 1.0], [-1.0, 1.0]]\n"]}
{"id": 38, "title": "Implement AdaBoost Fit Method", "description": "\nWrite a Python function `adaboost_fit` that implements the fit method for an AdaBoost classifier. The function should take in a 2D numpy array `X` of shape `(n_samples, n_features)` representing the dataset, a 1D numpy array `y` of shape `(n_samples,)` representing the labels, and an integer `n_clf` representing the number of classifiers. The function should initialize sample weights, find the best thresholds for each feature, calculate the error, update weights, and return a list of classifiers with their parameters.\n", "difficulty": "hard", "category": "Machine Learning", "examples": [{"input": "\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([1, 1, -1, -1])\n    n_clf = 3\n\n    clfs = adaboost_fit(X, y, n_clf)\n    print(clfs)\n", "output": "\n(example format, actual values may vary):\n    # [{'polarity': 1, 'threshold': 2, 'feature_index': 0, 'alpha': 0.5},\n    #  {'polarity': -1, 'threshold': 3, 'feature_index': 1, 'alpha': 0.3},\n    #  {'polarity': 1, 'threshold': 4, 'feature_index': 0, 'alpha': 0.2}]\n"}], "reasoning": ["\nThe function fits an AdaBoost classifier on the dataset X with the given labels y and number of classifiers n_clf. It returns a list of classifiers with their parameters, including the polarity, threshold, feature index, and alpha values\n"], "import_code": "\nimport numpy as np\nimport math\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list.\n", "starter_code": "def adaboost_fit(X, y, n_clf):", "entry_point": "adaboost_fit", "reference_code": "\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = np.shape(X)\n    w = np.full(n_samples, (1 / n_samples))\n    clfs = []\n    \n    for _ in range(n_clf):\n        clf = {}\n        min_error = float('inf')\n        \n        for feature_i in range(n_features):\n            feature_values = np.expand_dims(X[:, feature_i], axis=1)\n            unique_values = np.unique(feature_values)\n            \n            for threshold in unique_values:\n                p = 1\n                prediction = np.ones(np.shape(y))\n                prediction[X[:, feature_i] < threshold] = -1\n                error = sum(w[y != prediction])\n                \n                if error > 0.5:\n                    error = 1 - error\n                    p = -1\n                \n                if error < min_error:\n                    clf['polarity'] = p\n                    clf['threshold'] = threshold\n                    clf['feature_index'] = feature_i\n                    min_error = error\n        \n        clf['alpha'] = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n        predictions = np.ones(np.shape(y))\n        negative_idx = (X[:, clf['feature_index']] < clf['threshold'])\n        if clf['polarity'] == -1:\n            negative_idx = np.logical_not(negative_idx)\n        predictions[negative_idx] = -1\n        w *= np.exp(-clf['alpha'] * y * predictions)\n        w /= np.sum(w)\n        clf['alpha'] = np.round(clf['alpha'], 4)\n        clfs.append(clf)\n\n    return clfs\n", "test_cases": ["\nassert adaboost_fit(np.array([[1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 1, -1, -1]), 3)  == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 11.5129}]\n", "\nassert adaboost_fit(np.array([[8, 7], [3, 4], [5, 9], [4, 0], [1, 0], [0, 7], [3, 8], [4, 2], [6, 8], [0, 2]]), np.array([1, -1, 1, -1, 1, -1, -1, -1, 1, 1]), 2) == [{'polarity': 1, 'threshold': 5, 'feature_index': 0, 'alpha': 0.6931}, {'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}]\n", "\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, 1, -1, -1]), 3) == [{'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}, {'polarity': -1, 'threshold': 5, 'feature_index': 0, 'alpha': 11.5129}]\n", "\nassert adaboost_fit(np.array([[1, 3], [3, 6], [5, 8], [7, 11]]), np.array([1, -1, 1, -1]), 2) == [{'polarity': -1, 'threshold': 3, 'feature_index': 0, 'alpha': 0.5493}, {'polarity': -1, 'threshold': 7, 'feature_index': 0, 'alpha': 0.8047}]\n"]}
{"id": 39, "title": "Implementation of Log Softmax Function", "description": "\nIn machine learning and statistics, the softmax function is a generalization of the logistic function that converts a vector of scores into probabilities. The log-softmax function is the logarithm of the softmax function, and it is often used for numerical stability when computing the softmax of large numbers.\n\nGiven a 1D numpy array of scores, implement a Python function to compute the log-softmax of the array.\n", "difficulty": "easy", "category": "Deep Learning", "examples": [{"input": "\nA = np.array([1, 2, 3])\nprint(log_softmax(A))\n", "output": "\n[-2.4076, -1.4076, -0.4076]\n"}], "reasoning": ["\nThe log-softmax function is applied to the input array [1, 2, 3]. The output array contains the log-softmax values for each element.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def log_softmax(scores: list) -> np.ndarray:", "entry_point": "log_softmax", "reference_code": "\ndef log_softmax(scores: list) -> np.ndarray:\n    # Subtract the maximum value for numerical stability\n    scores = scores - np.max(scores)\n    return (np.round(scores - np.log(np.sum(np.exp(scores))), 4)).tolist()\n", "test_cases": ["\nassert log_softmax([1, 2, 3]) == [-2.4076, -1.4076, -0.4076]\n", "\nassert log_softmax([1, 1, 1]) == [-1.0986, -1.0986, -1.0986]\n", "\nassert log_softmax([1, 1, .0000001]) == [-0.862, -0.862, -1.862]\n"]}
{"id": 40, "title": "Implementing a Custom Dense Layer in Python", "description": "\nImplementing a Custom Dense Layer in Python\nYou are provided with a base `Layer` class that defines the structure of a neural network layer. Your task is to implement a subclass called `Dense`, which represents a fully connected neural network layer. The `Dense` class should extend the `Layer` class and implement the following methods:\n1. Initialization (`__init__`):\n- Define the layer with a specified number of neurons (`n_units`) and an optional input shape (`input_shape`).\n- Set up placeholders for the layer's weights (`W`), biases (`w0`), and optimizers.\n2. Weight Initialization (`initialize`):\n- Initialize the weights `W` using a uniform distribution with a limit of `1 / sqrt(input_shape[0])`, and bias `w0` should be set to zero.\n- Initialize optimizers for `W` and `w0`.\n3. Parameter Count (`parameters`):\n- Return the total number of trainable parameters in the layer, which includes the parameters in `W` and `w0`.\n4. Forward Pass (`forward_pass`):\n- Compute the output of the layer by performing a dot product between the input `X` and the weight matrix `W`, and then adding the bias `w0`.\n5. Backward Pass (`backward_pass`):\n- Calculate and return the gradient with respect to the input.\n- If the layer is trainable, update the weights and biases using the optimizer's update rule.\n6. Output Shape (`output_shape`):\n- Return the shape of the output produced by the forward pass, which should be `(self.n_units,)`.\nObjective:\nExtend the `Layer` class by implementing the `Dense` class to ensure it functions correctly within a neural network framework.\n", "difficulty": "hard", "category": "Deep Learning", "examples": [{"input": "\n# Initialize a Dense layer with 3 neurons and input shape (2,)\ndense_layer = Dense(n_units=3, input_shape=(2,))\n\n# Define a mock optimizer with a simple update rule\nclass MockOptimizer:\n    def update(self, weights, grad):\n        return weights - 0.01 * grad\n\noptimizer = MockOptimizer()\n\n# Initialize the Dense layer with the mock optimizer\ndense_layer.initialize(optimizer)\n\n# Perform a forward pass with sample input data\nX = np.array([[1, 2]])\noutput = dense_layer.forward_pass(X)\nprint(\"Forward pass output:\", output)\n\n# Perform a backward pass with sample gradient\naccum_grad = np.array([[0.1, 0.2, 0.3]])\nback_output = dense_layer.backward_pass(accum_grad)\nprint(\"Backward pass output:\", back_output)\n", "output": "\nForward pass output: [[-0.00655782  0.01429615  0.00905812]]\nBackward pass output: [[ 0.00129588  0.00953634]]\n"}], "reasoning": ["\nThe code initializes a Dense layer with 3 neurons and input shape (2,). It then performs a forward pass with sample input data and a backward pass with sample gradients. The output demonstrates the forward and backward pass results.\n"], "import_code": "\nimport numpy as np\nimport copy\nimport math\n\nnp.random.seed(42)\n", "output_constrains": "\n", "starter_code": "# DO NOT CHANGE LAYER CLASS\nclass Layer(object):\n\n\tdef set_input_shape(self, shape):\n    \n\t\tself.input_shape = shape\n\n\tdef layer_name(self):\n\t\treturn self.__class__.__name__\n\n\tdef parameters(self):\n\t\treturn 0\n\n\tdef forward_pass(self, X, training):\n\t\traise NotImplementedError()\n\n\tdef backward_pass(self, accum_grad):\n\t\traise NotImplementedError()\n\n\tdef output_shape(self):\n\t\traise NotImplementedError()\n\n# Your task is to implement the Dense class based on the above structure\nclass Dense(Layer):\n\tdef __init__(self, n_units, input_shape=None):\n\t\tself.layer_input = None\n\t\tself.input_shape = input_shape\n\t\tself.n_units = n_units\n\t\tself.trainable = True\n\t\tself.W = None\n\t\tself.w0 = None\n\n\tdef forward_pass():\n    \n\n\tdef backward_pass(self, accum_grad):\n\n\n\tdef number_of_parameters():", "entry_point": "Dense", "reference_code": "\nclass Dense(object):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W  = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt  = copy.copy(optimizer)\n        self.w0_opt = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w = self.layer_input.T.dot(accum_grad)\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            self.W = self.W_opt.update(self.W, grad_w)\n            self.w0 = self.w0_opt.update(self.w0, grad_w0)\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units, )\n", "test_cases": ["\ndense_layer = Dense(n_units=3, input_shape=(2,)) \nclass MockOptimizer: \n    def update(self, weights, grad): \n        return weights - 0.01 * grad \noptimizer = MockOptimizer() \ndense_layer.initialize(optimizer) \nX = np.array([[1, 2]]) \noutput = dense_layer.forward_pass(X) \naccum_grad = np.array([[0.1, 0.2, 0.3]]) \nback_output = dense_layer.backward_pass(accum_grad) \nassert all(np.allclose(a, b) for a, b in zip(back_output, np.array([[ 0.20816524, -0.22928937]])))\n"]}
{"id": 41, "title": "Simple Convolutional 2D Layer", "description": "\nIn this problem, you need to implement a 2D convolutional layer in Python. This function will process an input matrix using a specified convolutional kernel, padding, and stride.\n", "difficulty": "medium", "category": "Deep Learning", "examples": [{"input": "\nimport numpy as np\n\ninput_matrix = np.array([\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n])\n\nkernel = np.array([\n    [1, 0],\n    [-1, 1]\n])\n\npadding = 1\nstride = 2\n\noutput = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(output)\n", "output": "\n[[ 1.  1. -4.],[ 9.  7. -4.],[ 0. 14. 16.]]\n"}], "reasoning": ["\nThe function performs a 2D convolution operation on the input matrix using the specified kernel, padding, and stride. The output matrix contains the results of the convolution operation.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):", "entry_point": "simple_conv2d", "reference_code": "\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n    input_height_padded, input_width_padded = padded_input.shape\n\n    output_height = (input_height_padded - kernel_height) // stride + 1\n    output_width = (input_width_padded - kernel_width) // stride + 1\n\n    output_matrix = np.zeros((output_height, output_width))\n\n    for i in range(output_height):\n        for j in range(output_width):\n            region = padded_input[i*stride:i*stride + kernel_height, j*stride:j*stride + kernel_width]\n            output_matrix[i, j] = np.sum(region * kernel)\n\n    return np.round(output_matrix, 4).tolist()\n", "test_cases": ["\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 0, 1)  == [[ 16., 21., 26., 31.], [ 41., 46., 51., 56.], [ 66., 71., 76., 81.], [ 91., 96., 101., 106.]]\n", "\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [.5, 3.2], [1., -1.], ]), 2, 2)  == [[ 0., 0., 0., 0. ], [ 0., 5.9, 13.3, 12.5], [ 0., 42.9, 50.3, 27.5], [ 0., 80.9, 88.3, 12.5],]\n", "\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2.], [3., -1.], ]), 1, 1)  == [[ -1., 1., 3., 5., 7., 15.], [ -4., 16., 21., 26., 31., 35.], [  1., 41., 46., 51., 56., 55.], [  6., 66., 71., 76., 81., 75.], [ 11., 91., 96., 101., 106., 95.], [ 42., 65., 68., 71., 74.,  25.],]\n", "\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 0, 1)  == [ [174., 194., 214.], [274., 294., 314.], [374., 394., 414.], ]\n", "\nassert simple_conv2d(np.array([ [1., 2., 3., 4., 5.], [6., 7., 8., 9., 10.], [11., 12., 13., 14., 15.], [16., 17., 18., 19., 20.], [21., 22., 23., 24., 25.], ]), np.array([ [1., 2., 3.], [-6., 2., 8.], [5., 2., 3.], ]), 1, 2)  == [ [51., 104., 51.], [234., 294., 110.], [301., 216., -35.], ]\n", "\nassert simple_conv2d(np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), np.array([ [1., 2., 3.], [1., 2., 3.], [1., 2., 3.],]), 1, 1)  == [[16., 28., 16.], [24., 42., 24.], [16., 28., 16.]]\n"]}
{"id": 42, "title": "Implement ReLU Activation Function", "description": "\nWrite a Python function `relu` that implements the Rectified Linear Unit (ReLU) activation function. The function should take a single float as input and return the value after applying the ReLU function. The ReLU function returns the input if it's greater than 0, otherwise, it returns 0.\n", "difficulty": "easy", "category": "Deep Learning", "examples": [{"input": "\nprint(relu(0)) \n", "output": "\n0\n"}, {"input": "\nprint(relu(1))\n", "output": "\n1\n"}, {"input": "\nprint(relu(-1))\n", "output": "\n0\n"}], "reasoning": ["\nThe ReLU function is applied to the input values 0, 1, and -1. The output is 0 for negative values and the input value for non-negative values.\n"], "import_code": "\n", "output_constrains": "\n", "starter_code": "def relu(z: float) -> float:", "entry_point": "relu", "reference_code": "\ndef relu(z: float) -> float:\n    return max(0, z)\n", "test_cases": ["\nassert relu(0) == 0\n", "\nassert relu(1) == 1\n", "\nassert relu(-1) == 0\n"]}
{"id": 43, "title": "Implement Ridge Regression Loss Function", "description": "\nWrite a Python function `ridge_loss` that implements the Ridge Regression loss function. The function should take a 2D numpy array `X` representing the feature matrix, a 1D numpy array `w` representing the coefficients, a 1D numpy array `y_true` representing the true labels, and a float `alpha` representing the regularization parameter. The function should return the Ridge loss, which combines the Mean Squared Error (MSE) and a regularization term.\n", "difficulty": "easy", "category": "Machine Learning", "examples": [{"input": "\nimport numpy as np\n\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\nw = np.array([0.2, 2])\ny_true = np.array([2, 3, 4, 5])\nalpha = 0.1\n\nloss = ridge_loss(X, w, y_true, alpha)\nprint(loss)\n", "output": "\n2.204\n"}], "reasoning": ["\nThe Ridge loss is calculated using the Mean Squared Error (MSE) and a regularization term. The output represents the combined loss value.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\n", "starter_code": "def ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:", "entry_point": "ridge_loss", "reference_code": "\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    loss = np.mean((y_true - X @ w)**2) + alpha * np.sum(w**2)\n    return loss\n", "test_cases": ["\nassert ridge_loss(np.array([[1,1],[2,1],[3,1],[4,1]]), np.array([.2,2]), np.array([2,3,4,5]), 0.1) == 2.204\n", "\nassert ridge_loss(np.array([[1,1,4],[2,1,2],[3,1,.1],[4,1,1.2],[1,2,3]]), np.array([.2,2,5]), np.array([2,3,4,5,2]), 0.1) == 164.402\n"]}
{"id": 44, "title": "Leaky ReLU Activation Function", "description": "\nWrite a Python function `leaky_relu` that implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function. The function should take a float `z` as input and an optional float `alpha`, with a default value of 0.01, as the slope for negative inputs. The function should return the value after applying the Leaky ReLU function.\n", "difficulty": "easy", "category": "Deep Learning", "examples": [{"input": "\nprint(leaky_relu(0)) \n", "output": "\n0\n"}, {"input": "\nprint(leaky_relu(1)) \n", "output": "\n1\n"}, {"input": "\nprint(leaky_relu(-1)) \n", "output": "\n-0.01\n"}, {"input": "\nprint(leaky_relu(-2, alpha=0.1)) \n", "output": "\n-0.2\n"}], "reasoning": ["\nFor z = 0, the output is 0.\nFor z = 1, the output is 1.\nFor z = -1, the output is -0.01 (0.01 * -1).\nFor z = -2 with alpha = 0.1, the output is -0.2 (0.1 * -2).\n"], "import_code": "\n", "output_constrains": "\n", "starter_code": "def leaky_relu(z: float, alpha: float = 0.01) -> float|int:", "entry_point": "leaky_relu", "reference_code": "\ndef leaky_relu(z: float, alpha: float = 0.01) -> float|int:\n    return z if z > 0 else alpha * z\n", "test_cases": ["\nassert leaky_relu(5) == 5\n", "\nassert leaky_relu(1) == 1\n", "\nassert leaky_relu(-1) == -0.01\n", "\nassert leaky_relu(0) == 0\n", "\nassert leaky_relu(-2, alpha=0.1) == -0.2\n"]}
{"id": 45, "title": "Linear Kernel Function", "description": "\n    Write a Python function `kernel_function` that computes the linear kernel between two input vectors `x1` and `x2`. The linear kernel is defined as the dot product (inner product) of two vectors.\n", "difficulty": "easy", "category": "Machine Learning", "examples": [{"input": "\nimport numpy as np\n\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\n\nresult = kernel_function(x1, x2)\nprint(result)\n", "output": "\n32\n"}], "reasoning": ["\nThe linear kernel between x1 and x2 is computed as:1*4 + 2*5 + 3*6 = 32\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\n", "starter_code": "def kernel_function(x1, x2):", "entry_point": "kernel_function", "reference_code": "\ndef kernel_function(x1, x2):\n    return np.inner(x1, x2)\n", "test_cases": ["\nassert kernel_function(np.array([1, 2, 3]) , np.array([4, 5, 6]) ) == 32\n", "\nassert kernel_function(np.array([0, 1, 2]) , np.array([3, 4, 5]) ) == 14\n"]}
{"id": 46, "title": "Implement Precision Metric", "description": "\n    Write a Python function `precision` that calculates the precision metric given two numpy arrays: `y_true` and `y_pred`. The `y_true` array contains the true binary labels, and the `y_pred` array contains the predicted binary labels. Precision is defined as the ratio of true positives to the sum of true positives and false positives.\n", "difficulty": "easy", "category": "Machine Learning", "examples": [{"input": "\nimport numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\n\nresult = precision(y_true, y_pred)\nprint(result)\n", "output": "\n1.0\n"}], "reasoning": ["\nTrue Positives (TP) = 3\nFalse Positives (FP) = 0\nPrecision = TP / (TP + FP) = 3 / (3 + 0) = 1.0\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\n", "starter_code": "def precision(y_true, y_pred):", "entry_point": "precision", "reference_code": "\ndef precision(y_true, y_pred):\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    return true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n\n", "test_cases": ["\nassert precision(np.array([1, 0, 1, 1, 0, 1])  , np.array([1, 0, 1, 0, 0, 1]) ) == 1.0\n", "\nassert precision(np.array([1, 0, 1, 1, 0, 0])  , np.array([1, 0, 0, 0, 0, 1]) ) == 0.5\n"]}
{"id": 47, "title": "Implement Gradient Descent Variants with MSE Loss", "description": "\n    In this problem, you need to implement a single function that can perform three variants of gradient descent\u00e2\u0080\u0094Stochastic Gradient Descent (SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent\u00e2\u0080\u0094using Mean Squared Error (MSE) as the loss function. The function will take an additional parameter to specify which variant to use.\n", "difficulty": "medium", "category": "Machine Learning", "examples": [{"input": "\nimport numpy as np\n\n# Sample data\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\ny = np.array([2, 3, 4, 5])\n\n# Parameters\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 2\n\n# Initialize weights\nweights = np.zeros(X.shape[1])\n\n# Test Batch Gradient Descent\nfinal_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\n", "output": "\n[float,float]\n"}, {"input": "\nimport numpy as np\n\n# Sample data\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\ny = np.array([2, 3, 4, 5])\n\n# Parameters\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 2\n\n# Initialize weights\nweights = np.zeros(X.shape[1])\n\n# Test Stochastic Gradient Descent\nfinal_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\n", "output": "\n[float,float]\n"}, {"input": "\nimport numpy as np\n\n# Sample data\nX = np.array([[1, 1], [2, 1], [3, 1], [4, 1]])\ny = np.array([2, 3, 4, 5])\n\n# Parameters\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 2\n\n# Initialize weights\nweights = np.zeros(X.shape[1])\n\n# Test Mini-Batch Gradient Descent\nfinal_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini_batch')\n", "output": "\n[float,float]\n"}], "reasoning": ["\nThe function should return the final weights after performing the specified variant of gradient descent.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):", "entry_point": "gradient_descent", "reference_code": "\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    m = len(y)\n    \n    for _ in range(n_iterations):\n        if method == 'batch':\n            # Calculate the gradient using all data points\n            predictions = X.dot(weights)\n            errors = predictions - y\n            gradient = 2 * X.T.dot(errors) / m\n            weights = weights - learning_rate * gradient\n        \n        elif method == 'stochastic':\n            # Update weights for each data point individually\n            for i in range(m):\n                prediction = X[i].dot(weights)\n                error = prediction - y[i]\n                gradient = 2 * X[i].T.dot(error)\n                weights = weights - learning_rate * gradient\n        \n        elif method == 'mini_batch':\n            # Update weights using sequential batches of data points without shuffling\n            for i in range(0, m, batch_size):\n                X_batch = X[i:i+batch_size]\n                y_batch = y[i:i+batch_size]\n                predictions = X_batch.dot(weights)\n                errors = predictions - y_batch\n                gradient = 2 * X_batch.T.dot(errors) / batch_size\n                weights = weights - learning_rate * gradient\n                \n    return np.round(weights, 4).tolist()\n", "test_cases": ["\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='batch') == [1.1491, 0.5618]\n", "\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, method='stochastic') == [1.0508, 0.8366]\n", "\nassert gradient_descent(np.array([[1, 1], [2, 1], [3, 1], [4, 1]]), np.array([2, 3, 4, 5]), np.zeros(2), 0.01, 100, 2, method='mini_batch') == [1.1033, 0.6833]\n"]}
{"id": 48, "title": "Implement Reduced Row Echelon Form (RREF) Function", "description": "\nIn this problem, your task is to implement a function that converts a given matrix into its Reduced Row Echelon Form (RREF). The RREF of a matrix is a special form where each leading entry in a row is 1, and all other elements in the column containing the leading 1 are zeros, except for the leading 1 itself.\nHowever, there are some additional details to keep in mind:\n- Diagonal entries can be 0 if the matrix is reducible (i.e., the row corresponding to that position can be eliminated entirely).\n- Some rows may consist entirely of zeros.\n- If a column contains a pivot (a leading 1), all other entries in that column should be zero.\nYour task is to implement the RREF algorithm, which must handle these cases and convert any given matrix into its RREF.\n", "difficulty": "medium", "category": "Linear Algebra", "examples": [{"input": "\nimport numpy as np\n\nmatrix = np.array([\n    [1, 2, -1, -4],\n    [2, 3, -1, -11],\n    [-2, 0, -3, 22]\n])\n\nrref_matrix = rref(matrix)\nprint(rref_matrix)\n", "output": "\n[[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\n"}], "reasoning": ["\nThe given matrix is converted to its Reduced Row Echelon Form (RREF) where each leading entry is 1, and all other entries in the leading columns are zero.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def rref(matrix):", "entry_point": "rref", "reference_code": "\ndef rref(matrix):\n    # Convert to float for division operations\n    A = matrix.astype(np.float32)\n    n, m = A.shape\n    \n    for i in range(n):\n        if A[i, i] == 0:\n            nonzero_rel_id = np.nonzero(A[i:, i])[0]\n            if len(nonzero_rel_id) == 0: continue\n            \n            A[i] = A[i] + A[nonzero_rel_id[0] + i]\n\n        A[i] = A[i] / A[i, i]\n        for j in range(n):\n            if i != j:\n                A[j] -= A[j, i] * A[i]\n\n    return A.tolist()\n", "test_cases": ["\nassert rref(np.array([ [1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22] ])) == [[ 1., 0., 0., -8.], [ 0., 1., 0., 1.], [-0., -0., 1., -2.]]\n", "\nassert rref(np.array([ [2, 4, -2], [4, 9, -3], [-2, -3, 7] ])) == [[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]\n", "\nassert rref(np.array([ [0, 2, -1, -4], [2, 0, -1, -11], [-2, 0, 0, 22] ])) == [[ 1., 0., 0., -11.],[-0., 1., 0., -7.5],[-0., -0., 1., -11.]]\n", "\nassert rref(np.array([ [1, 2, -1], [2, 4, -1], [-2, -4, -3]])) == [[ 1., 2., 0.],[ 0., 0., 0.],[-0., -0., 1.]]\n"]}
{"id": 49, "title": "Implement Adam Optimization Algorithm", "description": "\nImplement the Adam (Adaptive Moment Estimation) optimization algorithm in Python. Adam is an optimization algorithm that adapts the learning rate for each parameter. Your task is to write a function `adam_optimizer` that updates the parameters of a given function using the Adam algorithm.\nThe function should take the following parameters:\n- `f`: The objective function to be optimized\n- `grad`: A function that computes the gradient of `f`\n- `x0`: Initial parameter values\n- `learning_rate`: The step size (default: 0.001)\n- `beta1`: Exponential decay rate for the first moment estimates (default: 0.9)\n- `beta2`: Exponential decay rate for the second moment estimates (default: 0.999)\n- `epsilon`: A small constant for numerical stability (default: 1e-8)\n- `num_iterations`: Number of iterations to run the optimizer (default: 1000)\nThe function should return the optimized parameters.\n", "difficulty": "medium", "category": "Deep Learning", "examples": [{"input": "\nimport numpy as np\n\ndef objective_function(x):\n    return x[0]**2 + x[1]**2\n\ndef gradient(x):\n    return np.array([2*x[0], 2*x[1]])\n\nx0 = np.array([1.0, 1.0])\nx_opt = adam_optimizer(objective_function, gradient, x0)\n\nprint(\"Optimized parameters:\", x_opt)\n", "output": "\n# Optimized parameters: [0.99000325 0.99000325]\n"}], "reasoning": ["\nThe Adam optimizer updates the parameters to minimize the objective function. In this case, the objective function is the sum of squares of the parameters, and the optimizer finds the optimal values for the parameters.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):", "entry_point": "adam_optimizer", "reference_code": "\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    x = x0\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g**2\n        m_hat = m / (1 - beta1**t)\n        v_hat = v / (1 - beta2**t)\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n\n    return np.round(x, 4).tolist()\n", "test_cases": ["\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1.0, 1.0])) == [0.99, 0.99]\n", "\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([0.2, 12.3])) == [ 0.19, 12.29]\n", "\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([1, 3])) == [0.99, 2.99]\n", "\nassert adam_optimizer(lambda x: x[0]**2 + x[1]**2, lambda y: np.array([2*y[0], 2*y[1]]), np.array([5, 8])) == [4.99, 7.99]\n"]}
{"id": 50, "title": "Implement Lasso Regression using Gradient Descent", "description": "\nIn this problem, you need to implement the Lasso Regression algorithm using Gradient Descent. Lasso Regression (L1 Regularization) adds a penalty equal to the absolute value of the coefficients to the loss function. Your task is to update the weights and bias iteratively using the gradient of the loss function and the L1 penalty.\nThe objective function of Lasso Regression is:\n$J(w, b) = \frac{1}{2n} \\sum\\limits_{i=1}^n (y_i - (\\sum\\limits_{j=1}^p X_{ij} w_j + b))^2$\nWhere:\n- $y_i$  is the actual value for the i-th sample\n- $\\hat{y_i} = \\sum_{j=1}^p X_{ij}w_j + b$ is the predicted value for the i-th sample\n- $w_j$ is the weight associated with the j-th feature\n- $\u0007lpha$ is the regularization parameter\n- $b$ is the bias\nYour task is to use the L1 penalty to shrink some of the feature coefficients to zero during gradient descent, thereby helping with feature selection.\n", "difficulty": "medium", "category": "Machine Learning", "examples": [{"input": "\nX = np.array([[0, 0], [1, 1], [2, 2]])\ny = np.array([0, 1, 2])\n\nalpha = 0.1\nweights, bias = l1_regularization_gradient_descent(X, y, alpha=alpha, learning_rate=0.01, max_iter=1000)\n", "output": "\n([0.4237, 0.4237], 0.1539)\n"}], "reasoning": ["\nThe Lasso Regression algorithm is used to optimize the weights and bias for the given data. The weights are adjusted to minimize the loss function with the L1 penalty.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:", "entry_point": "l1_regularization_gradient_descent", "reference_code": "\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    n_samples, n_features = X.shape\n    # Zero out weights and bias\n    weights = np.zeros(n_features)\n    bias = 0\n    \n    for iteration in range(max_iter):\n        # Predict values\n        y_pred = np.dot(X, weights) + bias\n        # Calculate error\n        error = y_pred - y\n        # Gradient for weights with L1 penalty\n        grad_w = (1 / n_samples) * np.dot(X.T, error) + alpha * np.sign(weights)\n        # Gradient for bias (no penalty for bias)\n        grad_b = (1 / n_samples) * np.sum(error)\n        \n        # Update weights and bias\n        weights -= learning_rate * grad_w\n        bias -= learning_rate * grad_b\n        \n        # Check for convergence\n        if np.linalg.norm(grad_w, ord=1) < tol:\n            break\n    \n    return np.round(weights, 4).tolist(), np.round(bias, 4)\n", "test_cases": ["\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.4237, 0.4237], 0.1539)\n", "\nassert l1_regularization_gradient_descent(np.array([[0, 0], [1, 1], [2, 2]]), np.array([0, 1, 2]), alpha=0.1, learning_rate=0.01, max_iter=5000)  == ([0.4249, 0.4249], 0.1504)\n", "\nassert l1_regularization_gradient_descent(np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]]), np.array([1, 2, 3, 4, 5]), alpha=0.1, learning_rate=0.01, max_iter=1000)  == ([0.2728, 0.6811], 0.4083)\n\n"]}
{"id": 51, "title": "Optimal String Alignment Distance", "description": "\nIn this problem, you need to implement a function that calculates the Optimal String Alignment (OSA) distance between two given strings. The OSA distance represents the minimum number of edits required to transform one string into another. The allowed edit operations are:\n- Insert a character\n- Delete a character\n- Substitute a character\n- Transpose two adjacent characters\nEach of these operations costs 1 unit.\nYour task is to find the minimum number of edits needed to convert the first string (s1) into the second string (s2).\nFor example, the OSA distance between the strings `caper` and `acer` is 2: one deletion (removing \"p\") and one transposition (swapping \"a\" and \"c\").\n", "difficulty": "medium", "category": "NLP", "examples": [{"input": "\nsource = \"butterfly\"\ntarget = \"dragonfly\"\n\ndistance = OSA(source, target)\nprint(distance)\n", "output": "\n6\n"}], "reasoning": ["\nThe OSA distance between the strings \"butterfly\" and \"dragonfly\" is 6. The minimum number of edits required to transform the source string into the target string is 6.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\n", "starter_code": "def OSA(source: str, target: str) -> int:", "entry_point": "OSA", "reference_code": "\ndef OSA(source: str, target: str) -> int:\n    source_len, target_len = len(source), len(target)\n\n    # Initialize matrix with zeros\n    osa_matrix = [[0] * (target_len + 1) for _ in range(source_len + 1)]\n\n    # Fill the first row and first column with index values\n    for j in range(1, target_len + 1):\n        osa_matrix[0][j] = j\n    for i in range(1, source_len + 1):\n        osa_matrix[i][0] = i\n\n    # Compute the OSA distance\n    for i in range(1, source_len + 1):\n        for j in range(1, target_len + 1):\n            osa_matrix[i][j] = min(\n                osa_matrix[i - 1][j] + 1,  # Deletion\n                osa_matrix[i][j - 1] + 1,  # Insertion\n                osa_matrix[i - 1][j - 1] + (1 if source[i - 1] != target[j - 1] else 0)  # Substitution\n            )\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                osa_matrix[i][j] = min(osa_matrix[i][j], osa_matrix[i - 2][j - 2] + 1)  # Transposition\n\n    return osa_matrix[-1][-1]\n", "test_cases": ["\nassert OSA(\"butterfly\", \"dragonfly\") == 6\n", "\nassert OSA(\"caper\", \"acer\") == 2\n", "\nassert OSA(\"telescope\", \"microscope\") == 5\n", "\nassert OSA(\"london\", \"paris\") == 6\n", "\nassert OSA(\"shower\", \"grower\") == 2\n", "\nassert OSA(\"labyrinth\", \"puzzle\") == 9\n", "\nassert OSA(\"silhouette\", \"shadow\") == 8\n", "\nassert OSA(\"whisper\", \"screaming\") == 9\n", "\nassert OSA(\"enigma\", \"mystery\") == 7\n", "\nassert OSA(\"symphony\", \"cacophony\") == 4\n", "\nassert OSA(\"mirage\", \"oasis\") == 6\n", "\nassert OSA(\"asteroid\", \"meteorite\") == 5\n", "\nassert OSA(\"palindrome\", \"palladium\") == 5\n"]}
{"id": 52, "title": "Implement Recall Metric in Binary Classification", "description": "\nTask: Implement Recall in Binary Classification\nYour task is to implement the recall metric in a binary classification setting. Recall is a performance measure that evaluates how effectively a machine learning model identifies positive instances from all the actual positive cases in a dataset.\nYou need to write a function `recall(y_true, y_pred)` that calculates the recall metric. The function should accept two inputs:\n- `y_true`: A list of true binary labels (0 or 1) for the dataset.\n- `y_pred`: A list of predicted binary labels (0 or 1) from the model.\nYour function should return the recall value rounded to three decimal places. If the denominator (TP + FN) is zero, the recall should be 0.0 to avoid division by zero.\n", "difficulty": "easy", "category": "Machine Learning", "examples": [{"input": "\nimport numpy as np\n\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\n\nprint(recall(y_true, y_pred))\n", "output": "\n# 0.75\n"}], "reasoning": ["\nThe recall value for the given true labels and predicted labels is 0.75. The model correctly identified 3 out of 4 positive instances in the dataset.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\n", "starter_code": "def recall(y_true, y_pred):", "entry_point": "recall", "reference_code": "\ndef recall(y_true, y_pred):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n\n    try:\n        return round(tp / (tp + fn), 3)\n    except ZeroDivisionError:\n        return 0.0\n", "test_cases": ["\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1])) == 0.75\n", "\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1])) == 0.333\n", "\nassert recall(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0])) == 1.0\n", "\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1])) == 0.5\n", "\nassert recall(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 0, 1, 0])) == 0.0\n", "\nassert recall(np.array([1, 0, 0, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 0])) == 0.667\n"]}
{"id": 53, "title": "Implement Self-Attention Mechanism", "description": "\nTask: Implement the Self-Attention Mechanism\nYour task is to implement the self-attention mechanism, which is a fundamental component of transformer models, widely used in natural language processing and computer vision tasks. The self-attention mechanism allows a model to dynamically focus on different parts of the input sequence when generating a contextualized representation.\nYour function should return the self-attention output as a numpy array.\n", "difficulty": "medium", "category": "Deep Learning", "examples": [{"input": "\nimport numpy as np\n\nX = np.array([[1, 0], [0, 1]])\nW_q = np.array([[1, 0], [0, 1]])\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 2], [3, 4]])\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = self_attention(Q, K, V)\n\nprint(output)\n", "output": "\n[[1.6605, 2.6605], [2.3395, 3.3395]]\n"}], "reasoning": ["\nThe self-attention mechanism calculates the attention scores for each input, determining how much focus to put on other inputs when generating a contextualized representation. The output is the weighted sum of the values based on the attention scores.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def self_attention(Q, K, V):", "entry_point": "self_attention", "reference_code": "\ndef compute_qkv(X, W_q, W_k, W_v):\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q, K, V):\n    d_k = Q.shape[1]\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True)\n    attention_output = np.matmul(attention_weights, V)\n    return np.round(attention_output, 4).tolist()\n", "test_cases": ["\nassert self_attention(compute_qkv(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]]))[0], compute_qkv(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]]))[1], compute_qkv(np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]]))[2]) == [[1.6605, 2.6605], [2.3395, 3.3395]]\n", "\nassert self_attention(compute_qkv(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]]))[0], compute_qkv(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]]))[1], compute_qkv(np.array([[1, 1], [1, 0]]), np.array([[1, 0], [0, 1]]), np.array([[1, 0], [0, 1]]), np.array([[1, 2], [3, 4]]))[2]) == [[3.0093, 4.679], [2.5, 4.0]]\n", "\nassert self_attention(compute_qkv(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))[0], compute_qkv(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))[1], compute_qkv(np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 1, 0], [0, 1, 1], [1, 0, 1]]), np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))[2]) == [[8.0, 10.0, 12.0], [8.6199, 10.6199, 12.6199], [7.3801, 9.3801, 11.3801]]\n"]}
{"id": 54, "title": "Implementing a Simple RNN", "description": "\nWrite a Python function that implements a simple Recurrent Neural Network (RNN) cell. The function should process a sequence of input vectors and produce the final hidden state. Use the tanh activation function for the hidden state updates. The function should take as inputs the sequence of input vectors, the initial hidden state, the weight matrices for input-to-hidden and hidden-to-hidden connections, and the bias vector. The function should return the final hidden state after processing the entire sequence, rounded to four decimal places.\n", "difficulty": "medium", "category": "Deep Learning", "examples": [{"input": "\ninput_sequence = [[1.0], [2.0], [3.0]]\n    initial_hidden_state = [0.0]\n    Wx = [[0.5]]  # Input to hidden weights\n    Wh = [[0.8]]  # Hidden to hidden weights\n    b = [0.0]     # Bias\n", "output": "\nfinal_hidden_state = [0.9993]\n"}], "reasoning": ["\nThe RNN processes each input in the sequence, updating the hidden state at each step using the tanh activation function.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:", "entry_point": "rnn_forward", "reference_code": "\ndef rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b):\n    h = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x = np.array(x)\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    final_hidden_state = np.round(h, 4)\n    return final_hidden_state.tolist()\n\n", "test_cases": ["\nassert rnn_forward([[1.0], [2.0], [3.0]], [0.0], [[0.5]], [[0.8]], [0.0]) == [0.9759]\n", "\nassert rnn_forward([[0.5], [0.1], [-0.2]], [0.0], [[1.0]], [[0.5]], [0.1]) == [0.118]\n", "\nassert rnn_forward( [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [0.0, 0.0], [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8], [0.9, 1.0]], [0.1, 0.2] ) == [0.7474, 0.9302]\n"]}
{"id": 55, "title": "2D Translation Matrix Implementation", "description": "\nTask: Implement a 2D Translation Matrix\nYour task is to implement a function that applies a 2D translation matrix to a set of points. A translation matrix is used to move points in 2D space by a specified distance in the x and y directions.\nWrite a function `translate_object(points, tx, ty)` where `points` is a list of [x, y] coordinates and `tx` and `ty` are the translation distances in the x and y directions, respectively.\nThe function should return a new list of points after applying the translation matrix.\n", "difficulty": "medium", "category": "Linear Algebra", "examples": [{"input": "\npoints = [[0, 0], [1, 0], [0.5, 1]]\ntx, ty = 2, 3\n\nprint(translate_object(points, tx, ty))\n", "output": "\n[[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\n"}], "reasoning": ["\nThe translation matrix moves the points by 2 units in the x-direction and 3 units in the y-direction. The resulting points are [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]].\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def translate_object(points, tx, ty):", "entry_point": "translate_object", "reference_code": "\ndef translate_object(points, tx, ty):\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1]\n    ])\n    \n    homogeneous_points = np.hstack([np.array(points), np.ones((len(points), 1))])\n    \n    translated_points = np.dot(homogeneous_points, translation_matrix.T)\n    \n    return translated_points[:, :2].tolist()\n", "test_cases": ["\nassert translate_object([[0, 0], [1, 0], [0.5, 1]], 2, 3) ==  [[2.0, 3.0], [3.0, 3.0], [2.5, 4.0]]\n", "\nassert translate_object([[0, 0], [1, 0], [1, 1], [0, 1]], -1, 2) == [[-1.0, 2.0], [0.0, 2.0], [0.0, 3.0], [-1.0, 3.0]]\n"]}
{"id": 56, "title": "KL Divergence Between Two Normal Distributions", "description": "\nTask: Implement KL Divergence Between Two Normal Distributions\nYour task is to compute the Kullback-Leibler (KL) divergence between two normal distributions. KL divergence measures how one probability distribution differs from a second, reference probability distribution.\nWrite a function `kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)` that calculates the KL divergence between two normal distributions, where $ P \\sim N(\\mu_P, \\sigma_P^2) $ and $ Q \\sim N(\\mu_Q, \\sigma_Q^2) $.\nThe function should return the KL divergence as a floating-point number.\n", "difficulty": "easy", "category": "Deep Learning", "examples": [{"input": "\nmu_p = 0.0\nsigma_p = 1.0\nmu_q = 1.0\nsigma_q = 1.0\n\nprint(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))\n", "output": "\n0.5\n"}], "reasoning": ["\nThe KL divergence between the normal distributions $ P $ and $ Q $ with parameters $ \\mu_P = 0.0 $, $ \\sigma_P = 1.0 $ and $ \\mu_Q = 1.0 $, $ \\sigma_Q = 1.0 $ is 0.5.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\n", "starter_code": "def kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):", "entry_point": "kl_divergence_normal", "reference_code": "\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    kl_div = term1 + term2 - 0.5\n    return kl_div\n\n", "test_cases": ["\nassert kl_divergence_normal(0.0, 1.0, 0.0, 1.0) == 0.0\n", "\nassert kl_divergence_normal(0.0, 1.0, 1.0, 1.0) == 0.5\n", "\nassert kl_divergence_normal(0.0, 1.0, 0.0, 2.0) == 0.3181471805599453\n", "\nassert kl_divergence_normal(1.0, 1.0, 0.0, 2.0) == 0.4431471805599453\n", "\nassert kl_divergence_normal(2.0, 1.0, 3.0, 2.0) == 0.4431471805599453\n", "\nassert kl_divergence_normal(0.0, 2.0, 0.0, 3.0) == 0.1276873303303866\n"]}
{"id": 57, "title": "Gauss-Seidel Method for Solving Linear Systems", "description": "\nTask: Implement the Gauss-Seidel Method\nYour task is to implement the Gauss-Seidel method, an iterative technique for solving a system of linear equations (Ax = b).\nThe function should iteratively update the solution vector (x) by using the most recent values available during the iteration process.\nWrite a function `gauss_seidel(A, b, n, x_ini=None)` where:\n- `A` is a square matrix of coefficients,\n- `b` is the right-hand side vector,\n- `n` is the number of iterations,\n- `x_ini` is an optional initial guess for (x) (if not provided, assume a vector of zeros).\nThe function should return the approximated solution vector (x) after performing the specified number of iterations.\n", "difficulty": "medium", "category": "Linear Algebra", "examples": [{"input": "\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float)\nb = np.array([4, 7, 3], dtype=float)\n\nn = 100\nprint(gauss_seidel(A, b, n))\n", "output": "\n[0.2, 1.4, 0.8]  (Approximate, values may vary depending on iterations)\n"}], "reasoning": ["\nThe Gauss-Seidel method iteratively updates the solution vector (x) until convergence. The output is an approximate solution to the linear system.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "entry_point": "gauss_seidel", "starter_code": "def gauss_seidel(A, b, n, x_ini=None):", "reference_code": "\ndef gauss_seidel_it(A, b, x):\n    rows, cols = A.shape\n    for i in range(rows):\n        x_new = b[i]\n        for j in range(cols):\n            if i != j:\n                x_new -= A[i, j] * x[j]\n        x[i] = x_new / A[i, i]\n    return x\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    x = x_ini or np.zeros_like(b)\n    for _ in range(n):\n        x = gauss_seidel_it(A, b, x)\n    return np.round(x, 4).tolist()\n\n", "test_cases": ["\nassert gauss_seidel(np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=float), np.array([4, 7, 3], dtype=float), 5) == [0.5008, 0.9997, 0.4998]\n", "\nassert gauss_seidel(np.array([[4, -1, 0, 1], [-1, 4, -1, 0], [0, -1, 4, -1], [1, 0, -1, 4]], dtype=float), np.array([15, 10, 10, 15], dtype=float), 1) == [3.75, 3.4375, 3.3594, 3.6523]\n", "\nassert gauss_seidel(np.array([[10, -1, 2], [-1, 11, -1], [2, -1, 10]], dtype=float), np.array([6, 25, -11], dtype=float), 100) == [1.0433, 2.2692, -1.0817]\n"]}
{"id": 58, "title": "Gaussian Elimination for Solving Linear Systems", "description": "\nTask: Implement the Gaussian Elimination Method\nYour task is to implement the Gaussian Elimination method, which transforms a system of linear equations into an upper triangular matrix. This method can then be used to solve for the variables using backward substitution.\nWrite a function `gaussian_elimination(A, b)` that performs Gaussian Elimination with partial pivoting to solve the system (Ax = b).\nThe function should return the solution vector (x).\n", "difficulty": "medium", "category": "Linear Algebra", "examples": [{"input": "\nA = np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float)\nb = np.array([2,5,1], dtype=float)\n\nprint(gaussian_elimination(A, b))\n", "output": "\n[11.0, -4.0, 3.0]\n"}], "reasoning": ["\nThe Gaussian Elimination method transforms the system of equations into an upper triangular matrix and then uses backward substitution to solve for the variables.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def gaussian_elimination(A, b):", "entry_point": "gaussian_elimination", "reference_code": "\ndef partial_pivoting(A_aug, row_num, col_num):\n    rows, cols = A_aug.shape\n    max_row = row_num\n    max_val = abs(A_aug[row_num, col_num])\n    for i in range(row_num, rows):\n        current_val = abs(A_aug[i, col_num])\n        if current_val > max_val:\n            max_val = current_val\n            max_row = i\n    if max_row != row_num:\n        A_aug[[row_num, max_row]] = A_aug[[max_row, row_num]]\n    return A_aug\n\ndef gaussian_elimination(A, b):\n    rows, cols = A.shape\n    A_aug = np.hstack((A, b.reshape(-1, 1)))\n\n    for i in range(rows-1):\n        A_aug = partial_pivoting(A_aug, i, i)\n        for j in range(i+1, rows):\n            A_aug[j, i:] -= (A_aug[j, i] / A_aug[i, i]) * A_aug[i, i:]\n\n    x = np.zeros_like(b, dtype=float)\n    for i in range(rows-1, -1, -1):\n        x[i] = (A_aug[i, -1] - np.dot(A_aug[i, i+1:cols], x[i+1:])) / A_aug[i, i]\n    return np.round(x, 4).tolist()\n\n", "test_cases": ["\nassert gaussian_elimination(np.array([[2,8,4], [2,5,1], [4,10,-1]], dtype=float), np.array([2,5,1], dtype=float)) == [11.0, -4.0, 3.0]\n", "\nassert gaussian_elimination(np.array([ [0, 2, 1, 0, 0, 0, 0], [2, 6, 2, 1, 0, 0, 0], [1, 2, 7, 2, 1, 0, 0], [0, 1, 2, 8, 2, 1, 0], [0, 0, 1, 2, 9, 2, 1], [0, 0, 0, 1, 2, 10, 2], [0, 0, 0, 0, 1, 2, 11] ], dtype=float), np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)) == [-0.4894, 0.3617, 0.2766, 0.2554, 0.319, 0.4039, 0.5339]\n", "\nassert gaussian_elimination(np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float), np.array([8, -11, -3], dtype=float)) == [2.0, 3.0, -1.0]\n"]}
{"id": 59, "title": "Implement Long Short-Term Memory (LSTM) Network", "description": "\nTask: Implement Long Short-Term Memory (LSTM) Network\nYour task is to implement an LSTM network that processes a sequence of inputs and produces the final hidden state and cell state after processing all inputs.\nWrite a class `LSTM` with the following methods:\n- `__init__(self, input_size, hidden_size)`: Initializes the LSTM with random weights and zero biases.\n- `forward(self, x, initial_hidden_state, initial_cell_state)`: Processes a sequence of inputs and returns the hidden states at each time step, as well as the final hidden state and cell state.\nThe LSTM should compute the forget gate, input gate, candidate cell state, and output gate at each time step to update the hidden state and cell state.\n", "difficulty": "medium", "category": "Deep Learning", "examples": [{"input": "\ninput_sequence = np.array([[1.0], [2.0], [3.0]])\ninitial_hidden_state = np.zeros((1, 1))\ninitial_cell_state = np.zeros((1, 1))\n\nlstm = LSTM(input_size=1, hidden_size=1)\noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\n\nprint(final_h)\n", "output": "\n[[0.73698596]] (approximate)\n"}], "reasoning": ["\nThe LSTM processes the input sequence [1.0, 2.0, 3.0] and produces the final hidden state [0.73698596].\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\n", "starter_code": "class LSTM:\n\tdef __init__(self, input_size, hidden_size):\n\t\tself.input_size = input_size\n\t\tself.hidden_size = hidden_size\n\n\t\t# Initialize weights and biases\n\t\tself.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n\t\tself.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n\t\tself.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n\t\tself.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n\t\tself.bf = np.zeros((hidden_size, 1))\n\t\tself.bi = np.zeros((hidden_size, 1))\n\t\tself.bc = np.zeros((hidden_size, 1))\n\t\tself.bo = np.zeros((hidden_size, 1))\n\n\tdef forward(self, x, initial_hidden_state, initial_cell_state):\n\t\t\"\"\"\n\t\tProcesses a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n\t\t\"\"\"\n\t\tpass", "entry_point": "LSTM", "reference_code": "\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        h = initial_hidden_state\n        c = initial_cell_state\n        outputs = []\n\n        for t in range(len(x)):\n            xt = x[t].reshape(-1, 1)\n            concat = np.vstack((h, xt))\n\n            # Forget gate\n            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n\n            # Input gate\n            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)\n\n            # Cell state update\n            c = ft * c + it * c_tilde\n\n            # Output gate\n            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n\n            # Hidden state update\n            h = ot * np.tanh(c)\n\n            outputs.append(h)\n\n        return np.array(outputs), h, c\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n", "test_cases": ["\ninput_sequence = np.array([[1.0], [2.0], [3.0]]) \ninitial_hidden_state = np.zeros((1, 1)) \ninitial_cell_state = np.zeros((1, 1)) \nlstm = LSTM(input_size=1, hidden_size=1) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.5, 0.5]]) \nlstm.Wi = np.array([[0.5, 0.5]]) \nlstm.Wc = np.array([[0.3, 0.3]]) \nlstm.Wo = np.array([[0.5, 0.5]]) \nlstm.bf = np.array([[0.1]]) \nlstm.bi = np.array([[0.1]]) \nlstm.bc = np.array([[0.1]]) \nlstm.bo = np.array([[0.1]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert all(np.allclose(a, b) for a, b in zip(final_h, [[0.73698596]]))\n", "\ninput_sequence = np.array([[0.1, 0.2], [0.3, 0.4]]) \ninitial_hidden_state = np.zeros((2, 1)) \ninitial_cell_state = np.zeros((2, 1)) \nlstm = LSTM(input_size=2, hidden_size=2) # Set weights and biases for reproducibility \nlstm.Wf = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wi = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wc = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.Wo = np.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]]) \nlstm.bf = np.array([[0.1], [0.2]]) \nlstm.bi = np.array([[0.1], [0.2]]) \nlstm.bc = np.array([[0.1], [0.2]]) \nlstm.bo = np.array([[0.1], [0.2]]) \noutputs, final_h, final_c = lstm.forward(input_sequence, initial_hidden_state, initial_cell_state)\nassert all(np.allclose(a, b) for a, b in zip(final_h, [[0.16613133], [0.40299449]]))\n"]}
{"id": 60, "title": "Implement TF-IDF (Term Frequency-Inverse Document Frequency)", "description": "\nTask: Implement TF-IDF (Term Frequency-Inverse Document Frequency)\nYour task is to implement a function that computes the TF-IDF scores for a query against a given corpus of documents.\nFunction Signature\nWrite a function `compute_tf_idf(corpus, query)` that takes the following inputs:\n- `corpus`: A list of documents, where each document is a list of words.\n- `query`: A list of words for which you want to compute the TF-IDF scores.\nOutput\nThe function should return a list of lists containing the TF-IDF scores for the query words in each document, rounded to five decimal places.\nImportant Considerations\n1. Handling Division by Zero:\nWhen implementing the Inverse Document Frequency (IDF) calculation, you must account for cases where a term does not appear in any document (`df = 0`). This can lead to division by zero in the standard IDF formula. Add smoothing (e.g., adding 1 to both numerator and denominator) to avoid such errors.\n2. Empty Corpus:\nEnsure your implementation gracefully handles the case of an empty corpus. If no documents are provided, your function should either raise an appropriate error or return an empty result. This will ensure the program remains robust and predictable.\n3. Edge Cases:\n- Query terms not present in the corpus.\n- Documents with no words.\n- Extremely large or small values for term frequencies or document frequencies.\nBy addressing these considerations, your implementation will be robust and handle real-world scenarios effectively.\n", "difficulty": "medium", "category": "NLP", "examples": [{"input": "\ncorpus = [\n    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n    [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"]\n]\nquery = [\"cat\"]\n\nprint(compute_tf_idf(corpus, query))\n", "output": "\n[[0.21461], [0.25754], [0.0]]\n"}], "reasoning": ["\nThe TF-IDF scores for the word \"cat\" in each document are computed and rounded to five decimal places.\n"], "import_code": "\nimport numpy as np\n", "output_constrains": "\nMake sure all results are rounded to the nearest 4th decimal.\nReturn python list after reshaping by using numpy's tolist() method.\n", "starter_code": "def compute_tf_idf(corpus, query):", "entry_point": "compute_tf_idf", "reference_code": "\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Compute TF-IDF scores for a query against a corpus of documents using only NumPy.\n    The output TF-IDF scores retain five decimal places.\n    \"\"\"\n    vocab = sorted(set(word for document in corpus for word in document).union(query))\n    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n\n    tf = np.zeros((len(corpus), len(vocab)))\n\n    for doc_idx, document in enumerate(corpus):\n        for word in document:\n            word_idx = word_to_index[word]\n            tf[doc_idx, word_idx] += 1\n        tf[doc_idx, :] /= len(document)\n\n    df = np.count_nonzero(tf > 0, axis=0)\n\n    num_docs = len(corpus)\n    idf = np.log((num_docs + 1) / (df + 1)) + 1\n\n    tf_idf = tf * idf\n\n    query_indices = [word_to_index[word] for word in query]\n    tf_idf_scores = tf_idf[:, query_indices]\n\n    tf_idf_scores = np.round(tf_idf_scores, 5)\n\n    return np.round(tf_idf_scores, 4).tolist()\n\n", "test_cases": ["\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ] , [\"cat\"]) == [[0.2146], [0.2575], [0.0]]\n", "\nassert compute_tf_idf([ [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"], [\"the\", \"dog\", \"chased\", \"the\", \"cat\"], [\"the\", \"bird\", \"flew\", \"over\", \"the\", \"mat\"] ], [\"cat\", \"mat\"]) == [[0.2146, 0.2146], [0.2575, 0.0], [0.0, 0.2146]]\n", "\nassert compute_tf_idf([ [\"this\", \"is\", \"a\", \"sample\"], [\"this\", \"is\", \"another\", \"example\"], [\"yet\", \"another\", \"sample\", \"document\"], [\"one\", \"more\", \"document\", \"for\", \"testing\"] ], [\"sample\", \"document\", \"test\"]) == [[0.3777, 0.0, 0.0], [0.0, 0.0, 0.0], [0.3777, 0.3777, 0.0], [0.0, 0.3022, 0.0]]\n"]}
{"id": 61, "title": "Implement F-Score Calculation for Binary Classification", "description": "\nTask: Implement F-Score Calculation for Binary Classification\n\nYour task is to implement a function that calculates the F-Score for a binary classification task. The F-Score combines both Precision and Recall into a single metric, providing a balanced measure of a model's performance.\n\nWrite a function `f_score(y_true, y_pred, beta)` where:\n\n- `y_true`: A numpy array of true labels (binary).\n- `y_pred`: A numpy array of predicted labels (binary).\n- `beta`: A float value that adjusts the importance of Precision and Recall. When `beta`=1, it computes the F1-Score, a balanced measure of both Precision and Recall.\n\nThe function should return the F-Score rounded to three decimal places.\n", "examples": [{"input": "\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nbeta = 1\n\nprint(f_score(y_true, y_pred, beta))\n", "output": "\n0.857\n"}], "reasoning": ["\nThe F-Score for the binary classification task is calculated using the true labels, predicted labels, and beta value.\n"], "entry_point": "f_score", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"", "reference_code": "\ndef f_score(y_true, y_pred, beta):\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n\n    op = precision * recall\n    div = ((beta**2) * precision) + recall\n\n    if div == 0 or op == 0:\n        return 0.0\n    score = (1 + (beta ** 2)) * op / div\n    return round(score, 3)\n", "test_cases": ["\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 1]), 1) == 0.857\n", "\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 0, 0, 0, 1]), 1) == 0.4\n", "\nassert f_score(np.array([1, 0, 1, 1, 0, 0]), np.array([1, 0, 1, 1, 0, 0]), 2) == 1.0\n", "\nassert f_score(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 0, 0, 1, 0, 1]), 2) == 0.556\n", "\nassert f_score(np.array([1, 1, 1, 1, 0, 0, 0]), np.array([0, 1, 0, 1, 1, 0, 0]), 3) == 0.513\n"], "difficulty": "easy", "category": "Machine Learning"}
{"id": 62, "title": "Implement a Simple RNN with Backpropagation Through Time (BPTT)", "description": "\nTask: Implement a Simple RNN with Backpropagation Through Time (BPTT)\n\nYour task is to implement a simple Recurrent Neural Network (RNN) and backpropagation through time (BPTT) to learn from sequential data. The RNN will process input sequences, update hidden states, and perform backpropagation to adjust weights based on the error gradient.\n\nWrite the following methods to complete the initialization, forward pass, and backward pass of the RNN:\n\n- `initialize_weights(input_size, hidden_size, output_size)`: Initializes the RNN with weights set to random values multiplied by 0.01 and biases set to zero.\n- `rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence)`: Processes a sequence of inputs and returns the output, the last inputs and the hidden states.\n- `rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)`: Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n\nIn this task, the RNN will be trained on sequence prediction, where the network will learn to predict the next item in a sequence. You should use 1/2 * Mean Squared Error (MSE) as the loss function and make sure to aggregate the losses at each time step by summing.\n", "examples": [{"input": "\nimport numpy as np\ninput_sequence = np.array([[1.0], [2.0], [3.0], [4.0]])\nexpected_output = np.array([[2.0], [3.0], [4.0], [5.0]])\n# Initialize RNN\nW_xh, W_hh, W_hy, b_h, b_y = initialize_weights(input_size, hidden_size, output_size)\n# Forward pass\noutputs, last_inputs, last_hiddens = rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence)\n# Backward pass\nW_xh, W_hh, W_hy, b_h, b_y = rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n# The output should show the RNN predictions for each step of the input sequence.\nprint(np.round(np.array(outputs), 5).tolist())\n", "output": "\n[[x1], [x2], [x3], [x4]]\n"}], "reasoning": ["\nThe RNN processes the input sequence [1.0, 2.0, 3.0, 4.0] and predicts the next item in the sequence at each step.\n"], "entry_point": "entrypoint", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def entrypoint(input_sequence, expected_output, input_size, hidden_size, output_size, learning_rate=0.01, epochs=100, random_seed=42):\n\n    np.random.seed(random_seed)  # Set random seed for reproducibility\n    W_xh, W_hh, W_hy, b_h, b_y = initialize_weights(input_size, hidden_size, output_size)\n    for epoch in range(epochs):\n        outputs, last_inputs, last_hiddens = rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence)\n        W_xh, W_hh, W_hy, b_h, b_y = rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n    return np.round(np.array(outputs), 5).tolist()\n\ndef initialize_weights(input_size, hidden_size, output_size):\n    \"\"\"\n    Initialize weights and biases for a simple RNN.\n\n    Args:\n        input_size (int): The size of the input features.\n        hidden_size (int): The size of the hidden state.\n        output_size (int): The size of the output features.\n\n    Returns:\n        tuple: Initialized weights and biases.\n    \"\"\"\n\ndef rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n    \"\"\"\n    Forward pass through the RNN.\n    Args:\n        W_xh (numpy.ndarray): Weights from input to hidden state.\n        W_hh (numpy.ndarray): Weights from hidden state to hidden state.\n        W_hy (numpy.ndarray): Weights from hidden state to output.\n        b_h (numpy.ndarray): Bias for hidden state.\n        b_y (numpy.ndarray): Bias for output.\n        hidden_size (int): The size of the hidden state.\n        input_sequence (list): The input sequence.\n    Returns:\n        tuple: list of outputs, list of last inputs, and list of last hidden states.\n    \"\"\"\n\ndef rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n    \"\"\"\n    Backward pass through the RNN.\n    Args:\n        W_xh (numpy.ndarray): Weights from input to hidden state.\n        W_hh (numpy.ndarray): Weights from hidden state to hidden state.\n        W_hy (numpy.ndarray): Weights from hidden state to output.\n        b_h (numpy.ndarray): Bias for hidden state.\n        b_y (numpy.ndarray): Bias for output.\n        hidden_size (int): The size of the hidden state.\n        input_sequence (list): The input sequence.\n        expected_output (list): The expected output sequence.\n        outputs (list): The outputs from the forward pass.\n        last_inputs (list): The last inputs from the forward pass.\n        last_hiddens (list): The last hidden states from the forward pass.\n        learning_rate (float): The learning rate for training.\n    Returns:\n        tuple: updated weights and biases.\n    \"\"\"", "reference_code": "\ndef entrypoint(input_sequence, expected_output, input_size, hidden_size, output_size, learning_rate=0.01, epochs=100, random_seed=42):\n\n    np.random.seed(random_seed)  # Set random seed for reproducibility\n    W_xh, W_hh, W_hy, b_h, b_y = initialize_weights(input_size, hidden_size, output_size)\n    for epoch in range(epochs):\n        outputs, last_inputs, last_hiddens = rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence)\n        W_xh, W_hh, W_hy, b_h, b_y = rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate)\n    return np.round(np.array(outputs), 5).tolist()\n\ndef initialize_weights(input_size, hidden_size, output_size):\n    W_xh = np.random.randn(hidden_size, input_size) * 0.01\n    W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n    W_hy = np.random.randn(output_size, hidden_size) * 0.01\n    b_h = np.zeros((hidden_size, 1))\n    b_y = np.zeros((output_size, 1))\n    return W_xh, W_hh, W_hy, b_h, b_y\n\ndef rnn_forward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence):\n    h = np.zeros((hidden_size, 1))  # Initialize hidden state\n    outputs = []\n    last_inputs = []\n    last_hiddens = [h]\n    # Forward pass\n    for t in range(len(input_sequence)):\n        last_inputs.append(input_sequence[t].reshape(-1, 1))\n        h = np.tanh(np.dot(W_xh, last_inputs[t]) + np.dot(W_hh, h) + b_h)\n        y = np.dot(W_hy, h) + b_y\n        outputs.append(y)\n        last_hiddens.append(h)\n    return outputs, last_inputs, last_hiddens\n\ndef rnn_backward(W_xh, W_hh, W_hy, b_h, b_y, hidden_size, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n    dW_xh = np.zeros_like(W_xh)\n    dW_hh = np.zeros_like(W_hh)\n    dW_hy = np.zeros_like(W_hy)\n    db_h = np.zeros_like(b_h)\n    db_y = np.zeros_like(b_y)\n\n    dh_next = np.zeros((hidden_size, 1))\n    for t in reversed(range(len(input_sequence))):\n        dy = outputs[t] - expected_output[t].reshape(-1, 1)\n        dW_hy += np.dot(dy, last_hiddens[t+1].T)\n        db_y += dy\n        dh = np.dot(W_hy.T, dy) + dh_next\n        dh_raw = (1 - last_hiddens[t+1] ** 2) * dh  # Derivative of tanh\n        dW_xh += np.dot(dh_raw, last_inputs[t].T)\n        dW_hh += np.dot(dh_raw, last_hiddens[t].T)\n        db_h += dh_raw\n\n        dh_next = np.dot(W_hh.T, dh_raw)\n\n    return W_xh - learning_rate * dW_xh, W_hh - learning_rate * dW_hh, W_hy - learning_rate * dW_hy, b_h - learning_rate * db_h, b_y - learning_rate * db_y\n", "test_cases": ["\nassert entrypoint(np.array([[1.0], [2.0], [3.0], [4.0]]), np.array([[2.0], [3.0], [4.0], [5.0]]), 1, 5, 1, 0.01, 100, 42) == [[[2.24144]], [[3.1845]], [[4.04306]], [[4.57419]]]\n", "\nassert entrypoint(np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]]), np.array([[2.0], [3.0], [4.0], [5.0]]), 2, 3, 1, 0.01, 100, 42) == [[[2.42201]], [[3.44168]], [[3.613]], [[4.5066]]]\n", "\nassert entrypoint(np.array([[1.0,2.0], [7.0,2.0], [1.0,3.0], [12.0,4.0]]), np.array([[2.0,1.0], [3.0,7.0], [4.0,8.0], [5.0,10.0]]), 2, 10, 2, 0.01, 50, 42) == [[[3.28425], [5.93532]], [[3.60394], [6.82013]], [[3.52587], [6.58278]], [[3.61336], [6.84916]]]\n"], "difficulty": "hard", "category": "Deep Learning"}
{"id": 63, "title": "Implement the Conjugate Gradient Method for Solving Linear Systems", "description": "\nTask: Implement the Conjugate Gradient Method for Solving Linear Systems\n\nYour task is to implement the Conjugate Gradient (CG) method, an efficient iterative algorithm for solving large, sparse, symmetric, positive-definite linear systems. Given a matrix `A` and a vector `b`, the algorithm will solve for `x` in the system ( Ax = b ).\n\nWrite a function `conjugate_gradient(A, b, n, x0=None, tol=1e-8)` that performs the Conjugate Gradient method as follows:\n\n- `A`: A symmetric, positive-definite matrix representing the linear system.\n- `b`: The vector on the right side of the equation.\n- `n`: Maximum number of iterations.\n- `x0`: Initial guess for the solution vector.\n- `tol`: Tolerance for stopping criteria.\n\nThe function should return the solution vector `x`.\n", "examples": [{"input": "\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 5\n\nprint(conjugate_gradient(A, b, n))\n", "output": "\n[0.09090909, 0.63636364]\n"}], "reasoning": ["\nThe Conjugate Gradient method is applied to the linear system Ax = b with the given matrix A and vector b. The algorithm iteratively refines the solution to converge to the exact solution.\n"], "entry_point": "conjugate_gradient", "import_code": "\nimport numpy as np\n", "output_constrains": "\nThe final solution vector x should be rounded to 8 decimal places and converted to a list using tolist() when returned.\n", "starter_code": "def conjugate_gradient(A, b, n, x0=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"", "reference_code": "\ndef residual(A: np.array, b: np.array, x: np.array) -> np.array:\n    # calculate linear system residuals\n    return b - A @ x\n\ndef alpha(A: np.array, r: np.array, p: np.array) -> float:\n\n    # calculate step size\n    alpha_num = np.dot(r, r)\n    alpha_den = np.dot(p @ A, p)\n\n    return alpha_num/alpha_den\n\ndef beta(r: np.array, r_plus1: np.array) -> float:\n\n    # calculate direction scaling\n    beta_num = np.dot(r_plus1, r_plus1)\n    beta_den = np.dot(r, r)\n\n    return beta_num/beta_den\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8) -> np.array:\n\n    # calculate initial residual vector\n    x = np.zeros_like(b)\n    r = residual(A, b, x) # residual vector\n    rPlus1 = r\n    p = r # search direction vector\n\n    for i in range(n):\n\n        # line search step value - this minimizes the error along the current search direction\n        alp = alpha(A, r, p)\n\n        # new x and r based on current p (the search direction vector)\n        x = x + alp * p\n        rPlus1 = r - alp * (A@p)\n\n        # calculate beta - this ensures that all vectors are A-orthogonal to each other\n        bet = beta(r, rPlus1)\n\n        # update x and r\n        # using a othogonal search direction ensures we get all the information we need in more direction and then don't have to search in that direction again\n        p = rPlus1 + bet * p\n\n        # update residual vector\n        r = rPlus1\n\n        # break if less than tolerance\n        if np.linalg.norm(residual(A,b,x)) < tol:\n            break\n\n    return np.round(x, 8).tolist()\n", "test_cases": ["\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\n", "\nassert conjugate_gradient(np.array([[4, 1, 2], [1, 3, 0], [2, 0, 5]]), np.array([7, 8, 5]), 1) == [1.2627451, 1.44313725, 0.90196078]\n", "\nassert conjugate_gradient(np.array([[6, 2, 1, 1, 0], [2, 5, 2, 1, 1], [1, 2, 6, 1, 2], [1, 1, 1, 7, 1], [0, 1, 2, 1, 8]]), np.array([1, 2, 3, 4, 5]), 100) == [0.01666667, 0.11666667, 0.21666667, 0.45, 0.5]\n"], "difficulty": "hard", "category": "Linear Algebra"}
{"id": 64, "title": "Implement Gini Impurity Calculation for a Set of Classes", "description": "\nTask: Implement Gini Impurity Calculation\n\nYour task is to implement a function that calculates the Gini Impurity for a set of classes. Gini impurity is commonly used in decision tree algorithms to measure the impurity or disorder within a node.\n", "examples": [{"input": "\ny = [0, 1, 1, 1, 0]\nprint(gini_impurity(y))\n", "output": "\n0.48\n"}], "reasoning": ["\nThe Gini Impurity is calculated as 1 - (p_0^2 + p_1^2), where p_0 and p_1 are the probabilities of each class. In this case, p_0 = 2/5 and p_1 = 3/5, resulting in a Gini Impurity of 0.48.\n"], "entry_point": "gini_impurity", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def gini_impurity(y):\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"", "reference_code": "\ndef gini_impurity(y: list[int]) -> float:\n\n    classes = set(y)\n    n = len(y)\n\n    gini_impurity = 0\n\n    for cls in classes:\n        gini_impurity += (y.count(cls)/n)**2\n\n    return round(1-gini_impurity,3)\n", "test_cases": ["\nassert gini_impurity([0, 0, 0, 0, 1, 1, 1, 1]) == 0.5\n", "\nassert gini_impurity([0, 0, 0, 0, 0, 1]) == 0.278\n", "\nassert gini_impurity([0, 1, 2, 2, 2, 1, 2]) == 0.571\n"], "difficulty": "easy", "category": "Machine Learning"}
{"id": 65, "title": "Implement Compressed Row Sparse Matrix (CSR) Format Conversion", "description": "\nTask: Convert a Dense Matrix to Compressed Row Sparse (CSR) Format\n\nYour task is to implement a function that converts a given dense matrix into the Compressed Row Sparse (CSR) format, an efficient storage representation for sparse matrices. The CSR format only stores non-zero elements and their positions, significantly reducing memory usage for matrices with a large number of zeros.\n\nWrite a function `compressed_row_sparse_matrix(dense_matrix)` that takes a 2D list `dense_matrix` as input and returns a tuple containing three lists:\n\n- **Values array**: List of all non-zero elements in row-major order.\n- **Column indices array**: Column index for each non-zero element in the values array.\n- **Row pointer array**: Cumulative number of non-zero elements per row, indicating the start of each row in the values array.\n", "examples": [{"input": "\ndense_matrix = [\n    [1, 0, 0, 0],\n    [0, 2, 0, 0],\n    [3, 0, 4, 0],\n    [1, 0, 0, 5]\n]\n\nvals, col_idx, row_ptr = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values array:\", vals)\nprint(\"Column indices array:\", col_idx)\nprint(\"Row pointer array:\", row_ptr)\n", "output": "\nValues array: [1, 2, 3, 4, 1, 5]\nColumn indices array: [0, 1, 0, 2, 0, 3]\nRow pointer array: [0, 1, 2, 4, 6]\n"}], "reasoning": ["\nThe dense matrix is converted to CSR format with the values array containing non-zero elements, column indices array storing the corresponding column index, and row pointer array indicating the start of each row in the values array.\n"], "entry_point": "compressed_row_sparse_matrix", "import_code": "", "output_constrains": "", "starter_code": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"", "reference_code": "\ndef compressed_row_sparse_matrix(dense_matrix):\n    vals = []\n    col_idx = []\n    row_ptr = [0]\n\n    for row in dense_matrix:\n        for j, val in enumerate(row):\n            if val != 0:\n                vals.append(val)\n                col_idx.append(j)\n        row_ptr.append(len(vals))\n\n    return vals, col_idx, row_ptr\n", "test_cases": ["\nassert compressed_row_sparse_matrix([[1, 0, 0, 0], [0, 2, 0, 0], [3, 0, 4, 0], [1, 0, 0, 5]]) == ([1, 2, 3, 4, 1, 5], [0, 1, 0, 2, 0, 3], [0, 1, 2, 4, 6])\n", "\nassert compressed_row_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [0, 1, 1, 2], [0, 0, 2, 4])\n", "\nassert compressed_row_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([3, 4, 5, 6, 7, 8], [2, 1, 0, 3, 1, 4], [0, 1, 2, 4, 4, 6])\n"], "difficulty": "easy", "category": "Linear Algebra"}
{"id": 66, "title": "Implement Orthogonal Projection of a Vector onto a Line", "description": "\nTask: Compute the Orthogonal Projection of a Vector\n\nYour task is to implement a function that calculates the orthogonal projection of a vector **v** onto another vector **L**. This projection results in the vector on **L** that is closest to **v**.\n\nWrite a function `orthogonal_projection(v, L)` that takes in two lists, `v` (the vector to be projected) and `L` (the line vector), and returns the orthogonal projection of `v` onto `L`. The function should output a list representing the projection vector rounded to three decimal places.\n", "examples": [{"input": "\nv = [3, 4]\nL = [1, 0]\nprint(orthogonal_projection(v, L))\n", "output": "\n[3.0, 0.0]\n"}], "reasoning": ["\nThe orthogonal projection of vector [3, 4] onto the line defined by [1, 0] results in the projection vector [3, 0], which lies on the line [1, 0].\n"], "entry_point": "orthogonal_projection", "import_code": "", "output_constrains": "", "starter_code": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"", "reference_code": "\ndef dot(v1, v2):\n    return sum([ax1 * ax2 for ax1, ax2 in zip(v1, v2)])\n\ndef scalar_mult(scalar, v):\n    return [scalar * ax for ax in v]\n\ndef orthogonal_projection(v, L):\n    L_mag_sq = dot(L, L)\n    proj_scalar = dot(v, L) / L_mag_sq\n    proj_v = scalar_mult(proj_scalar, L)\n    return [round(x, 3) for x in proj_v]\n", "test_cases": ["\nassert orthogonal_projection([3, 4], [1, 0]) == [3.0, 0.0]\n", "\nassert orthogonal_projection([1, 2, 3], [0, 0, 1]) == [0.0, 0.0, 3.0]\n", "\nassert orthogonal_projection([5, 6, 7], [2, 0, 0]) == [5.0, 0.0, 0.0]\n"], "difficulty": "easy", "category": "Linear Algebra"}
{"id": 67, "title": "Implement Compressed Column Sparse Matrix Format (CSC)", "description": "\nTask: Create a Compressed Column Sparse Matrix Representation\n\nYour task is to implement a function that converts a dense matrix into its Compressed Column Sparse (CSC) representation. The CSC format stores only non-zero elements of the matrix and is efficient for matrices with a high number of zero elements.\n\nWrite a function `compressed_col_sparse_matrix(dense_matrix)` that takes in a two-dimensional list `dense_matrix` and returns a tuple of three lists:\n\n- `values`: List of non-zero elements, stored in column-major order.\n- `row indices`: List of row indices corresponding to each value in the values array.\n- `column pointer`: List that indicates the starting index of each column in the values array.\n", "examples": [{"input": "\ndense_matrix = [\n    [0, 0, 3, 0],\n    [1, 0, 0, 4],\n    [0, 2, 0, 0]\n]\n\nvals, row_idx, col_ptr = compressed_col_sparse_matrix(dense_matrix)\n", "output": "\n[1, 2, 3, 4] [1, 2, 0, 1] [0, 1, 2, 3, 4]\n"}], "reasoning": ["\nThe dense matrix is converted to CSC format with the values array containing non-zero elements, row indices array storing the corresponding row index, and column pointer array indicating the start of each column in the values array.\n"], "entry_point": "compressed_col_sparse_matrix", "import_code": "", "output_constrains": "", "starter_code": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"", "reference_code": "\ndef compressed_col_sparse_matrix(dense_matrix):\n    vals = []\n    row_idx = []\n    col_ptr = [0]\n\n    rows, cols = len(dense_matrix), len(dense_matrix[0])\n\n    for i in range(cols):\n        for j in range(rows):\n            val = dense_matrix[j][i]\n            if val != 0:\n                vals.append(val)\n                row_idx.append(j)\n        col_ptr.append(len(vals))\n\n    return vals, row_idx, col_ptr\n", "test_cases": ["\nassert compressed_col_sparse_matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]]) == ([], [], [0, 0, 0, 0])\n", "\nassert compressed_col_sparse_matrix([[0, 0, 0], [1, 2, 0], [0, 3, 4]]) == ([1, 2, 3, 4], [1, 1, 2, 2], [0, 1, 3, 4])\n", "\nassert compressed_col_sparse_matrix([[0, 0, 3, 0, 0], [0, 4, 0, 0, 0], [5, 0, 0, 6, 0], [0, 0, 0, 0, 0], [0, 7, 0, 0, 8]]) == ([5, 4, 7, 3, 6, 8], [2, 1, 4, 0, 2, 4], [0, 1, 3, 4, 5, 6])\n"], "difficulty": "easy", "category": "Linear Algebra"}
{"id": 68, "title": "Find the Image of a Matrix Using Row Echelon Form", "description": "\nTask: Compute the Column Space of a Matrix\n\nIn this task, you are required to implement a function `matrix_image(A)` that calculates the column space of a given matrix `A`. The column space, also known as the image or span, consists of all linear combinations of the columns of `A`. To find this, you'll use concepts from linear algebra, focusing on identifying independent columns that span the matrix's image. **Your task**: Implement the function `matrix_image(A)` to return the basis vectors that span the column space of `A`. These vectors should be extracted from the original matrix and correspond to the independent columns.\n", "examples": [{"input": "\nmatrix = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n])\nprint(matrix_image(matrix))\n", "output": "\n[[1, 2], [4, 5], [7, 8]]\n"}], "reasoning": ["\nThe column space of the matrix is spanned by the independent columns [1, 2], [4, 5], and [7, 8]. These columns form the basis vectors that represent the image of the matrix.\n"], "entry_point": "matrix_image", "import_code": "\nimport numpy as np\n", "output_constrains": "\nThe matrix representing basis vectors should be rounded to 8 decimal places and converted to a list using tolist() when returned.\n", "starter_code": "def matrix_image(A):", "reference_code": "\ndef rref(A):\n    # Convert to float for division operations\n    A = A.astype(np.float32)\n    n, m = A.shape\n\n    for i in range(n):\n        if A[i, i] == 0:\n            nonzero_current_row = np.nonzero(A[i:, i])[0] + i\n            if len(nonzero_current_row) == 0:\n                continue\n            A[[i, nonzero_current_row[0]]] = A[[nonzero_current_row[0], i]]\n\n        A[i] = A[i] / A[i, i]\n\n        for j in range(n):\n            if i != j:\n                A[j] -= A[i] * A[j, i]\n    return A\n\ndef find_pivot_columns(A):\n    n, m = A.shape\n    pivot_columns = []\n    for i in range(n):\n        nonzero = np.nonzero(A[i, :])[0]\n        if len(nonzero) != 0:\n            pivot_columns.append(nonzero[0])\n    return pivot_columns\n\ndef matrix_image(A):\n    # Find the RREF of the matrix\n    Arref = rref(A)\n    # Find the pivot columns\n    pivot_columns = find_pivot_columns(Arref)\n    # Extract the pivot columns from the original matrix\n    image_basis = A[:, pivot_columns]\n    return np.round(image_basis, 8).tolist()\n", "test_cases": ["\nassert matrix_image(np.array([[1, 0], [0, 1]])) == [[1, 0], [0, 1]]\n", "\nassert matrix_image(np.array([[1, 2], [2, 4]])) == [[1], [2]]\n", "\nassert matrix_image(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == [[1, 2], [4, 5], [7, 8]]\n", "\nassert matrix_image(np.array([[3, 9, 6], [1, 4, 7], [2, 5, 8]])) == [[3, 9, 6], [1, 4, 7], [2, 5, 8]]\n", "\nassert matrix_image(np.array([[3, 3, 3], [1, 1, 1], [2, 2, 2]])) == [[3], [1], [2]]\n"], "difficulty": "medium", "category": "Linear Algebra"}
{"id": 69, "title": "Calculate R-squared for Regression Analysis", "description": "\nTask: Compute the R-squared Value in Regression Analysis\n\n- R-squared, also known as the coefficient of determination, is a measure that indicates how well the independent variables explain the variability of the dependent variable in a regression model.\n\n- **Your Task**: To implement the function `r_squared(y_true, y_pred)` that calculates the R-squared value, given arrays of true values `y_true` and predicted values `y_pred`.\n", "examples": [{"input": "\nimport numpy as np\n\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1.1, 2.1, 2.9, 4.2, 4.8])\nprint(r_squared(y_true, y_pred))\n", "output": "\n0.989"}], "reasoning": ["\nThe R-squared value is calculated to be 0.989, indicating that the regression model explains 98.9% of the variance in the dependent variable.\n"], "entry_point": "r_squared", "import_code": "\nimport numpy as np\n", "output_constrains": "\nYour code should return the R-squared value rounded to three decimal places.\n", "starter_code": "def r_squared(y_true, y_pred):", "reference_code": "\ndef r_squared(y_true, y_pred):\n    if np.array_equal(y_true, y_pred):\n        return 1.0\n\n    # Calculate mean of true values\n    y_mean = np.mean(y_true)\n\n    # Calculate Sum of Squared Residuals (SSR)\n    ssr = np.sum((y_true - y_pred) ** 2)\n\n    # Calculate Total Sum of Squares (SST)\n    sst = np.sum((y_true - y_mean) ** 2)\n\n    try:\n        # Calculate R-squared\n        r2 = 1 - (ssr / sst)\n        if np.isinf(r2):\n            return 0.0\n        return round(r2, 3)\n    except ZeroDivisionError:\n        return 0.0\n", "test_cases": ["\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1, 2, 3, 4, 5])) == 1.0\n", "\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([1.1, 2.1, 2.9, 4.2, 4.8])) == 0.989\n", "\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([2, 1, 4, 3, 5])) == 0.6\n", "\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([3, 3, 3, 3, 3])) == 0.0\n", "\nassert r_squared(np.array([3, 3, 3, 3, 3]), np.array([1, 2, 3, 4, 5])) == 0.0\n", "\nassert r_squared(np.array([1, 2, 3, 4, 5]), np.array([5, 4, 3, 2, 1])) == -3.0\n", "\nassert r_squared(np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 0])) == 1.0\n", "\nassert r_squared(np.array([-2, -2, -2]), np.array([-2, -2, -2 + 1e-8])) == 0.0\n"], "difficulty": "easy", "category": "Machine Learning"}
{"id": 70, "title": "Calculate Image Brightness", "description": "\nTask: Image Brightness Calculator\n\nIn this task, you will implement a function `calculate_brightness(img)` that calculates the average brightness of a grayscale image. The image is represented as a 2D matrix, where each element represents a pixel value between 0 (black) and 255 (white).\n\nYour Task:\nImplement the function `calculate_brightness(img)` to:\n\n1. Return the average brightness of the image rounded to two decimal places.\n2. Handle edge cases:\n    - If the image matrix is empty.\n    - If the rows in the matrix have inconsistent lengths.\n    - If any pixel values are outside the valid range (0-255).\n\nFor any of these edge cases, the function should return `-1`.\n", "examples": [{"input": "\nimg = [\n    [100, 200],\n    [50, 150]\n]\nprint(calculate_brightness(img))\n", "output": "\n125.0"}], "reasoning": ["\nThe average brightness is calculated as (100 + 200 + 50 + 150) / 4 = 125.0\n"], "entry_point": "calculate_brightness", "import_code": "", "output_constrains": "", "starter_code": "def calculate_brightness(img):", "reference_code": "\ndef calculate_brightness(img):\n    # Check if image is empty or has no columns\n    if not img or not img[0]:\n        return -1\n\n    rows, cols = len(img), len(img[0])\n\n    # Check if all rows have same length and values are valid\n    for row in img:\n        if len(row) != cols:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n\n    # Calculate average brightness\n    total = sum(sum(row) for row in img)\n    return round(total / (rows * cols), 2)\n", "test_cases": ["\nassert calculate_brightness([]) == -1\n", "\nassert calculate_brightness([[100, 200], [150]]) == -1\n", "\nassert calculate_brightness([[100, 300]]) == -1\n", "\nassert calculate_brightness([[100, -1]]) == -1\n", "\nassert calculate_brightness([[128]]) == 128.0\n", "\nassert calculate_brightness([[100, 200], [50, 150]]) == 125.0\n"], "difficulty": "easy", "category": "Computer Vision"}
{"id": 71, "title": "Calculate Root Mean Square Error (RMSE)", "description": "\nTask: Compute Root Mean Square Error (RMSE)\n\nIn this task, you are required to implement a function `rmse(y_true, y_pred)` that calculates the Root Mean Square Error (RMSE) between the actual values and the predicted values. RMSE is a commonly used metric for evaluating the accuracy of regression models, providing insight into the standard deviation of residuals.\n\nYour Task:\nImplement the function `rmse(y_true, y_pred)` to:\n\n1. Calculate the RMSE between the arrays y_true and y_pred.\n2. Return the RMSE value rounded to three decimal places.\n3. Ensure the function handles edge cases such as:\n    - Mismatched array shapes.\n    - Empty arrays.\n    - Invalid input types.\nThe RMSE is defined as:\n\n\\[\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2}\n\\]\n\nWhere:\n- $n$ is the number of observations.\n- $y_{true,i}$ and $y_{pred,i}$ are the actual and predicted values for the $i$-th observation.\n", "examples": [{"input": "\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\nprint(rmse(y_true, y_pred))\n", "output": "\n0.612\n"}], "reasoning": ["\nThe RMSE is calculated as sqrt((0.5^2 + 0.5^2 + 0^2 + 1^2) / 4) = 0.612\n"], "entry_point": "rmse", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def rmse(y_true, y_pred):", "reference_code": "\ndef rmse(y_true, y_pred):\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Arrays must have the same shape\")\n    if y_true.size == 0:\n        raise ValueError(\"Arrays cannot be empty\")\n    return round(np.sqrt(np.mean((y_true - y_pred) ** 2)), 3)\n", "test_cases": ["\nassert rmse(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.612\n", "\nassert rmse(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.842\n", "\nassert rmse(np.array([[1, 2], [3, 4]]), np.array([[1, 2], [3, 4]])) == 0.0\n"], "difficulty": "easy", "category": "Machine Learning"}
{"id": 72, "title": "Calculate Jaccard Index for Binary Classification", "description": "\nTask: Implement the Jaccard Index\n\nYour task is to implement a function `jaccard_index(y_true, y_pred)` that calculates the Jaccard Index, a measure of similarity between two binary sets. The Jaccard Index is widely used in binary classification tasks to evaluate the overlap between predicted and true labels.\n\nYour Task:\nImplement the function `jaccard_index(y_true, y_pred)` to:\n\n1. Calculate the Jaccard Index between the arrays `y_true` and `y_pred`.\n2. Return the Jaccard Index as a float value.\n3. Ensure the function handles cases where:\n    - There is no overlap between `y_true` and `y_pred`.\n    - Both arrays contain only zeros (edge cases).\n\nThe Jaccard Index is defined as:\n\n\\[\nJaccard Index = \\frac{\\text{Number of elements in the intersection of } y_{\\text{true}} \\text{ and } y_{\\text{pred}} }{\\text{Number of elements in the union of } y_{\\text{true}} \\text{ and } y_{\\text{pred}}}\n\\]\n\nWhere:\n- $y_{\\text{true}}$ and $y_{\\text{pred}}$ are binary arrays of the same length, representing true and predicted labels.\n- The result ranges from 0 (no overlap) to 1 (perfect overlap).\n", "examples": [{"input": "\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 0, 1, 0, 0, 1])\nprint(jaccard_index(y_true, y_pred))\n", "output": "\n0.75\n"}], "reasoning": ["\nThe Jaccard Index is calculated as 3 / 4 = 0.75, indicating a 75% overlap between the true and predicted labels.\n"], "entry_point": "jaccard_index", "import_code": "\nimport numpy as np\n", "output_constrains": "\nYour code should return the Jaccard Index rounded to three decimal places.\n", "starter_code": "def jaccard_index(y_true, y_pred):", "reference_code": "\ndef jaccard_index(y_true, y_pred):\n    intersection = np.sum((y_true == 1) & (y_pred == 1))\n    union = np.sum((y_true == 1) | (y_pred == 1))\n    result = intersection / union\n    if np.isnan(result):\n        return 0.0\n    return round(result, 3)\n", "test_cases": ["\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 1, 0, 1])) == 1.0\n", "\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 0]), np.array([0, 1, 0, 0, 1, 1])) == 0.0\n", "\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([1, 0, 1, 0, 0, 0])) == 0.5\n", "\nassert jaccard_index(np.array([1, 0, 1, 1, 0, 1]), np.array([0, 1, 0, 1, 1, 0])) == 0.167\n", "\nassert jaccard_index(np.array([1, 1, 1, 1, 1, 1]), np.array([0, 0, 0, 1, 1, 0])) == 0.333\n", "\nassert jaccard_index(np.array([1, 1, 1, 0, 1, 1]), np.array([1, 0, 0, 0, 0, 0])) == 0.2\n"], "difficulty": "easy", "category": "Machine Learning"}
{"id": 73, "title": "Calculate Dice Score for Classification", "description": "\nTask: Compute the Dice Score\n\nYour task is to implement a function `dice_score(y_true, y_pred)` that calculates the Dice Score, also known as the S\u00c3\u00b8rensen-Dice coefficient or F1-score, for binary classification. The Dice Score is used to measure the similarity between two sets and is particularly useful in tasks like image segmentation and binary classification.\n\nYour Task:\nImplement the function `dice_score(y_true, y_pred)` to:\n\n1. Calculate the Dice Score between the arrays `y_true` and `y_pred`.\n2. Return the Dice Score as a float value rounded to 3 decimal places.\n3. Handle edge cases appropriately, such as when there are no true or predicted positives.\n\nThe Dice Score is defined as:\n\\[\nDice Score = \\frac{2 \\times (\\text{Number of elements in the intersection of } y_{\\text{true}} \\text{ and } y_{\\text{pred}})}{\\text{Number of elements in } y_{\\text{true}} + \\text{Number of elements in } y_{\\text{pred}}}\n\\]\n\nWhere:\n- $y_{\\text{true}}$ and $y_{\\text{pred}}$ are binary arrays of the same length, representing true and predicted labels.\n- The result ranges from 0 (no overlap) to 1 (perfect overlap).\n", "examples": [{"input": "\ny_true = np.array([1, 1, 0, 1, 0, 1])\ny_pred = np.array([1, 1, 0, 0, 0, 1])\nprint(dice_score(y_true, y_pred))\n", "output": "\n0.857\n"}], "reasoning": ["\nThe Dice Score is calculated as (2 * 3) / (2 * 3 + 0 + 1) = 0.857, indicating an 85.7% overlap between the true and predicted labels.\n"], "entry_point": "dice_score", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def dice_score(y_true, y_pred):", "reference_code": "\ndef dice_score(y_true, y_pred):\n    intersection = np.logical_and(y_true, y_pred).sum()\n    true_sum = y_true.sum()\n    pred_sum = y_pred.sum()\n\n    # Handle edge cases\n    if true_sum == 0 or pred_sum == 0:\n        return 0.0\n\n    dice = (2.0 * intersection) / (true_sum + pred_sum)\n    return round(float(dice), 3)\n", "test_cases": ["\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 1, 0, 0])) == 1.0\n", "\nassert dice_score(np.array([1, 1, 0, 0]), np.array([0, 0, 1, 1])) == 0.0\n", "\nassert dice_score(np.array([1, 1, 0, 0]), np.array([1, 0, 0, 0])) == 0.667\n", "\nassert dice_score(np.array([0, 0, 0, 0]), np.array([0, 0, 0, 0])) == 0.0\n", "\nassert dice_score(np.array([1, 1, 1, 1]), np.array([1, 1, 1, 1])) == 1.0\n", "\nassert dice_score(np.array([0, 0, 0, 0]), np.array([1, 1, 1, 1])) == 0.0\n", "\nassert dice_score(np.array([1]), np.array([1])) == 1.0\n", "\nassert dice_score(np.array([True, True, False, False]), np.array([1, 1, 0, 0])) == 1.0\n"], "difficulty": "easy", "category": "Machine Learning"}
{"id": 74, "title": "Create Composite Hypervector for a Dataset Row", "description": "\nTask: Generate a Composite Hypervector Using Hyperdimensional Computing\n\nYour task is to implement the function `create_row_hv(row, dim, random_seeds)` to generate a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC). Each feature in the row is represented by binding hypervectors for the feature name and its value. The hypervectors for the values are created using the same feature seed provided in the `random_seeds` dictionary to ensure reproducibility. All feature hypervectors are then bundled to create a composite hypervector for the row.\n\nInput:\n- `row`: A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n- `dim`: The dimensionality of the hypervectors.\n- `random_seeds`: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\nOutput:\n- A composite hypervector representing the entire row.\n", "examples": [{"input": "\nrow = {\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}\ndim = 5\nrandom_seeds = {\"FeatureA\": 42, \"FeatureB\": 7}\nprint(create_row_hv(row, dim, random_seeds))\n", "output": "\n[ 1, -1,  1,  1,  1]\n"}], "reasoning": ["\nThe composite hypervector is created by binding hypervectors for each feature and bundling them together.\n"], "entry_point": "create_row_hv", "import_code": "\nimport numpy as np\n", "output_constrains": "\nThe composite hypervector should be converted to a list using tolist() when returned.\n", "starter_code": "def create_row_hv(row, dim, random_seeds):", "reference_code": "\ndef create_hv(dim):\n    return np.random.choice([-1, 1], dim)\n\ndef create_col_hvs(dim, seed):\n    np.random.seed(seed)\n    return create_hv(dim), create_hv(dim)\n\ndef bind(hv1, hv2):\n    return hv1 * hv2\n\ndef bundle(hvs, dim):\n    bundled = np.sum(list(hvs.values()), axis=0)\n    return sign(bundled)\n\ndef sign(vector, threshold=0.01):\n    return np.array([1 if v >= 0 else -1 for v in vector])\n\ndef create_row_hv(row, dim, random_seeds):\n    row_hvs = {col: bind(*create_col_hvs(dim, random_seeds[col])) for col in row.keys()}\n    return bundle(row_hvs, dim).tolist()\n", "test_cases": ["\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 5, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, 1]\n", "\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 10, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, -1, 1, 1, -1, -1, -1, -1, -1, -1]\n", "\nassert create_row_hv({\"FeatureA\": \"value1\", \"FeatureB\": \"value2\"}, 15, {\"FeatureA\": 42, \"FeatureB\": 7}) == [1, 1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, -1, -1, 1]\n"], "difficulty": "medium", "category": "Linear Algebra"}
{"id": 75, "title": "Generate a Confusion Matrix for Binary Classification", "description": "\nTask: Generate a Confusion Matrix\n\nYour task is to implement the function `confusion_matrix(data)` that generates a confusion matrix for a binary classification problem. The confusion matrix provides a summary of the prediction results on a classification problem, allowing you to visualize how many data points were correctly or incorrectly labeled.\n\nInput:\n    - A list of lists, where each inner list represents a pair\n    - `[y_true, y_pred]` for one observation. `y_true` is the actual label, and `y_pred` is the predicted label.\nOutput:\n    - A 2x2 confusion matrix represented as a list of lists.\n", "examples": [{"input": "\ndata = [[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]\nprint(confusion_matrix(data))\n", "output": "\n[[1, 1], [2, 1]]\n"}], "reasoning": ["\nThe confusion matrix shows the counts of true positives, false negatives, false positives, and true negatives.\n"], "entry_point": "confusion_matrix", "import_code": "\nfrom collections import Counter\n", "output_constrains": "", "starter_code": "def confusion_matrix(data):", "reference_code": "\ndef confusion_matrix(data):\n    # Count all occurrences\n    counts = Counter(tuple(pair) for pair in data)\n    # Get metrics\n    TP, FN, FP, TN = counts[(1, 1)], counts[(1, 0)], counts[(0, 1)], counts[(0, 0)]\n    # Define matrix and return\n    confusion_matrix = [[TP, FN], [FP, TN]]\n    return confusion_matrix\n", "test_cases": ["\nassert confusion_matrix([[1, 1], [1, 0], [0, 1], [0, 0], [0, 1]]) == [[1, 1], [2, 1]]\n", "\nassert confusion_matrix([[0, 1], [1, 0], [1, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 1], [0, 0], [1, 0], [1, 1], [0, 0], [1, 0], [0, 1], [1, 1], [1, 1], [1, 0]]) == [[5, 5], [4, 3]]\n", "\nassert confusion_matrix([[0, 1], [0, 1], [0, 0], [0, 1], [0, 0], [0, 1], [0, 1], [0, 0], [1, 0], [0, 1], [1, 0], [0, 0], [0, 1], [0, 1], [0, 1], [1, 0]]) == [[0, 3], [9, 4]]\n"], "difficulty": "easy", "category": "Machine Learning"}
{"id": 76, "title": "Calculate Cosine Similarity Between Vectors", "description": "\nTask: Implement Cosine Similarity\n\nIn this task, you need to implement a function `cosine_similarity(v1, v2)` that calculates the cosine similarity between two vectors. Cosine similarity measures the cosine of the angle between two vectors, indicating their directional similarity.\n\nInput:\n- `v1` and `v2`: Numpy arrays representing the input vectors.\nOutput:\n- A float representing the cosine similarity, rounded to three decimal places.\nConstraints:\n- Both input vectors must have the same shape.\n- Input vectors cannot be empty or have zero magnitude.\n", "examples": [{"input": "\nimport numpy as np\n\nv1 = np.array([1, 2, 3])\nv2 = np.array([2, 4, 6])\nprint(cosine_similarity(v1, v2))\n", "output": "\n1.0\n"}], "reasoning": ["\nThe cosine similarity between v1 and v2 is 1.0, indicating perfect similarity.\n"], "entry_point": "cosine_similarity", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def cosine_similarity(v1, v2):\n    # Implement your code here", "reference_code": "\ndef cosine_similarity(v1, v2):\n    if v1.shape != v2.shape:\n        raise ValueError(\"Arrays must have the same shape\")\n\n    if v1.size == 0:\n        raise ValueError(\"Arrays cannot be empty\")\n\n    # Flatten arrays in case of 2D\n    v1_flat = v1.flatten()\n    v2_flat = v2.flatten()\n\n    dot_product = np.dot(v1_flat, v2_flat)\n    magnitude1 = np.sqrt(np.sum(v1_flat**2))\n    magnitude2 = np.sqrt(np.sum(v2_flat**2))\n\n    if magnitude1 == 0 or magnitude2 == 0:\n        raise ValueError(\"Vectors cannot have zero magnitude\")\n\n    return round(dot_product / (magnitude1 * magnitude2), 3)\n", "test_cases": ["\nassert cosine_similarity(np.array([1, 2, 3]), np.array([2, 4, 6])) == 1.0\n", "\nassert cosine_similarity(np.array([1, 2, 3]), np.array([-1, -2, -3])) == -1.0\n", "\nassert cosine_similarity(np.array([1, 0, 7]), np.array([0, 1, 3])) == 0.939\n", "\nassert cosine_similarity(np.array([1, 0]), np.array([0, 1])) == 0.0\n"], "difficulty": "easy", "category": "Linear Algebra"}
{"id": 77, "title": "Calculate Performance Metrics for a Classification Model", "description": "\nTask: Implement Performance Metrics Calculation\n\nIn this task, you are required to implement a function `performance_metrics(actual, predicted)` that computes various performance metrics for a binary classification problem. These metrics include:\n\n- Confusion Matrix\n- Accuracy\n- F1 Score\n- Specificity\n- Negative Predictive Value\n\nThe function should take in two lists:\n\n- `actual`: The actual class labels (1 for positive, 0 for negative).\n- `predicted`: The predicted class labels from the model.\n\n**Output**\nThe function should return a tuple containing:\n\n- `confusion_matrix`: A 2x2 matrix.\n- `accuracy`: A float representing the accuracy of the model.\n- `f1_score`: A float representing the F1 score of the model.\n- `specificity`: A float representing the specificity of the model.\n- `negative_predictive_value`: A float representing the negative predictive value.\n\nConstraints\n- All elements in the `actual` and `predicted` lists must be either 0 or 1.\n- Both lists must have the same length.\n", "examples": [{"input": "\nactual = [1, 0, 1, 0, 1]\npredicted = [1, 0, 0, 1, 1]\nprint(performance_metrics(actual, predicted))\n", "output": "\n([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\n"}], "reasoning": ["\nThe function calculates the confusion matrix, accuracy, F1 score, specificity, and negative predictive value based on the input labels. The resulting values are rounded to three decimal places as required.\n"], "entry_point": "performance_metrics", "import_code": "\nfrom collections import Counter\n", "output_constrains": "\nWhen your code return `accuracy`, `f1_score`, `specificity`, and `negative_predictive_value`, their values should be rounded to three decimal places.\n", "starter_code": "def performance_metrics(actual: list[int], predicted: list[int]) -> tuple:", "reference_code": "\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    data = list(zip(actual, predicted))\n    counts = Counter(tuple(pair) for pair in data)\n    TP, FN, FP, TN = counts[(1, 1)], counts[(1, 0)], counts[(0, 1)], counts[(0, 0)]\n    confusion_matrix = [[TP, FN], [FP, TN]]\n    accuracy = (TP + TN) / (TP + TN + FP + FN)\n    precision = TP / (TP + FP)\n    recall = TP / (TP + FN)\n    f1 = 2 * precision * recall / (precision + recall)\n    negativePredictive = TN / (TN + FN)\n    specificity = TN / (TN + FP)\n    return confusion_matrix, round(accuracy, 3), round(f1, 3), round(specificity, 3), round(negativePredictive, 3)\n", "test_cases": ["\nassert performance_metrics([1, 0, 1, 0, 1], [1, 0, 0, 1, 1]) == ([[2, 1], [1, 1]], 0.6, 0.667, 0.5, 0.5)\n", "\nassert performance_metrics([1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]) == ([[6, 4], [2, 7]], 0.684, 0.667, 0.778, 0.636)\n", "\nassert performance_metrics([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]) == ([[4, 4], [5, 2]], 0.4, 0.471, 0.286, 0.333)\n", "\nassert performance_metrics([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1], [0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]) == ([[4, 5], [4, 2]], 0.4, 0.471, 0.333, 0.286)\n"], "difficulty": "medium", "category": "Machine Learning"}
{"id": 78, "title": "Descriptive Statistics Calculator", "description": "\nWrite a Python function to calculate various descriptive statistics metrics for a given dataset. The function should take a list or NumPy array of numerical values and return a dictionary containing mean, median, mode, variance, standard deviation, percentiles (25th, 50th, 75th), and interquartile range (IQR).\n", "examples": [{"input": "\n[10, 20, 30, 40, 50]\n", "output": "\n{'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\n"}], "reasoning": ["\nThe dataset is processed to calculate all descriptive statistics. The mean is the average value, the median is the central value, the mode is the most frequent value, and variance and standard deviation measure the spread of data. Percentiles and IQR describe data distribution.\n"], "entry_point": "descriptive_statistics", "import_code": "\nimport numpy as np\n", "output_constrains": "\nThe output should be a dictionary with the following keys:\n- 'mean'\n- 'median'\n- 'mode'\n- 'variance'\n- 'standard_deviation'\n- '25th_percentile'\n- '50th_percentile'\n- '75th_percentile'\n- 'interquartile_range'\nExcept for statistics that draw from the data, all other values should be rounded to four decimal places.\n", "starter_code": "def descriptive_statistics(data):", "reference_code": "\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics metrics for a given dataset.\n    :param data: List or numpy array of numerical values\n    :return: Dictionary containing mean, median, mode, variance, standard deviation,\n             percentiles (25th, 50th, 75th), and interquartile range (IQR)\n    \"\"\"\n    # Ensure data is a numpy array for easier calculations\n    data = np.array(data)\n\n    # Mean\n    mean = np.mean(data)\n\n    # Median\n    median = np.median(data)\n\n    # Mode\n    unique, counts = np.unique(data, return_counts=True)\n    mode = unique[np.argmax(counts)] if len(data) > 0 else None\n\n    # Variance\n    variance = np.var(data)\n\n    # Standard Deviation\n    std_dev = np.sqrt(variance)\n\n    # Percentiles (25th, 50th, 75th)\n    percentiles = np.percentile(data, [25, 50, 75])\n\n    # Interquartile Range (IQR)\n    iqr = percentiles[2] - percentiles[0]\n\n    # Compile results into a dictionary\n    stats_dict = {\n        \"mean\": mean,\n        \"median\": median,\n        \"mode\": mode,\n        \"variance\": np.round(variance,4),\n        \"standard_deviation\": np.round(std_dev,4),\n        \"25th_percentile\": percentiles[0],\n        \"50th_percentile\": percentiles[1],\n        \"75th_percentile\": percentiles[2],\n        \"interquartile_range\": iqr\n    }\n    return stats_dict\n", "test_cases": ["\nassert descriptive_statistics([10, 20, 30, 40, 50]) == {'mean': 30.0, 'median': 30.0, 'mode': 10, 'variance': 200.0, 'standard_deviation': 14.1421, '25th_percentile': 20.0, '50th_percentile': 30.0, '75th_percentile': 40.0, 'interquartile_range': 20.0}\n", "\nassert descriptive_statistics([1, 2, 2, 3, 4, 4, 4, 5]) == {'mean': 3.125, 'median': 3.5, 'mode': 4, 'variance': 1.6094, 'standard_deviation': 1.2686, '25th_percentile': 2.0, '50th_percentile': 3.5, '75th_percentile': 4.0, 'interquartile_range': 2.0}\n", "\nassert descriptive_statistics([100]) == {'mean': 100.0, 'median': 100.0, 'mode': 100, 'variance': 0.0, 'standard_deviation': 0.0, '25th_percentile': 100.0, '50th_percentile': 100.0, '75th_percentile': 100.0, 'interquartile_range': 0.0}\n"], "difficulty": "easy", "category": "Statistics"}
{"id": 79, "title": "Binomial Distribution Probability", "description": "\nWrite a Python function to calculate the probability of achieving exactly k successes in n independent Bernoulli trials, each with probability p of success, using the Binomial distribution formula.\n", "examples": [{"input": "\nn = 6, k = 2, p = 0.5\n", "output": "\n0.23438\n"}], "reasoning": ["\nThe function calculates the Binomial probability, the intermediate steps include calculating the binomial coefficient, raising p and (1-p) to the appropriate powers, and multiplying the results.\n"], "entry_point": "binomial_probability", "import_code": "\nimport math\n", "output_constrains": "\nYour code should return the probability rounded to five decimal places.\n", "starter_code": "def binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"", "reference_code": "\ndef binomial_probability(n, k, p):\n    # Calculate binomial coefficient (n choose k)\n    binomial_coeff = math.comb(n, k)\n    # Calculate the probability using the binomial formula\n    probability = binomial_coeff * (p ** k) * ((1 - p) ** (n - k))\n    # Return the probability, rounded to five decimal places\n    return round(probability, 5)\n", "test_cases": ["\nassert binomial_probability(6, 2, 0.5) == 0.23438\n", "\nassert binomial_probability(6, 4, 0.7) == 0.32414\n", "\nassert binomial_probability(3, 3, 0.9) == 0.729\n", "\nassert binomial_probability(5, 0, 0.3) == 0.16807\n", "\nassert binomial_probability(7, 2, 0.1) == 0.124\n", "\nassert binomial_probability(100, 2, 0.1) == 0.00162\n", "\nassert binomial_probability(2, 2, 0.1) == 0.01\n"], "difficulty": "medium", "category": "Probability"}
{"id": 80, "title": "Normal Distribution PDF Calculator", "description": "\nWrite a Python function to calculate the probability density function (PDF) of the normal distribution for a given value, mean, and standard deviation. The function should use the mathematical formula of the normal distribution to return the PDF value rounded to 5 decimal places.\n", "examples": [{"input": "\nx = 16, mean = 15, std_dev = 2.04\n", "output": "\n0.17342\n"}], "reasoning": ["\nThe function computes the PDF using x = 16, mean = 15, and std_dev = 2.04.\n"], "entry_point": "normal_pdf", "import_code": "\nimport math\n", "output_constrains": "", "starter_code": "def normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"", "reference_code": "\ndef normal_pdf(x, mean, std_dev):\n    coefficient = 1 / (math.sqrt(2 * math.pi) * std_dev)\n    exponent = math.exp(-((x - mean) ** 2) / (2 * std_dev ** 2))\n    return round(coefficient * exponent, 5)\n", "test_cases": ["\nassert normal_pdf(0, 0, 1) == 0.39894\n", "\nassert normal_pdf(16, 15, 2.04) == 0.17342\n", "\nassert normal_pdf(1, 0, 0.5) == 0.10798\n"], "difficulty": "medium", "category": "Probability"}
{"id": 81, "title": "Poisson Distribution Probability Calculator", "description": "\nWrite a Python function to calculate the probability of observing exactly k events in a fixed interval using the Poisson distribution formula. The function should take k (number of events) and lam (mean rate of occurrences) as inputs and return the probability rounded to 5 decimal places.\n", "examples": [{"input": "\nk = 3, lam = 5\n", "output": "\n0.14037\n"}], "reasoning": ["\nThe function calculates the probability for a given number of events occurring in a fixed interval, based on the mean rate of occurrences.\n"], "entry_point": "poisson_probability", "import_code": "\nimport math\n", "output_constrains": "", "starter_code": "def poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"", "reference_code": "\ndef poisson_probability(k, lam):\n\n    # Calculate the Poisson probability using the formula\n    probability = (lam ** k) * math.exp(-lam) / math.factorial(k)\n    # Return the probability, rounded to five decimal places\n    return round(probability, 5)\n", "test_cases": ["\nassert poisson_probability(3, 5) == 0.14037\n", "\nassert poisson_probability(0, 5) == 0.00674\n", "\nassert poisson_probability(2, 10) == 0.00227\n", "\nassert poisson_probability(1, 1) == 0.36788\n", "\nassert poisson_probability(20, 20) == 0.08884\n"], "difficulty": "easy", "category": "Probability"}
{"id": 82, "title": "Grayscale Image Contrast Calculator", "description": "\nWrite a Python function to calculate the contrast of a grayscale image using the difference between the maximum and minimum pixel values.\n", "examples": [{"input": "\nimg = np.array([[0, 50], [200, 255]])\n", "output": "\n255\n"}], "reasoning": ["\nThe function calculates contrast by finding the difference between the maximum (255) and minimum (0) pixel values in the image, resulting in a contrast of 255.\n"], "entry_point": "calculate_contrast", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def calculate_contrast(img) -> int:\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"", "reference_code": "\ndef calculate_contrast(img):\n\n    # Find the maximum and minimum pixel values\n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n\n    # Calculate contrast\n    contrast = max_pixel - min_pixel\n\n    return round(float(contrast), 3)\n", "test_cases": ["\nassert calculate_contrast(np.array([[0, 50], [200, 255]])) == 255\n", "\nassert calculate_contrast(np.array([[128, 128], [128, 128]])) == 0\n", "\nassert calculate_contrast(np.zeros((10, 10), dtype=np.uint8)) == 0\n", "\nassert calculate_contrast(np.ones((10, 10), dtype=np.uint8) * 255) == 0\n", "\nassert calculate_contrast(np.array([[10, 20, 30], [40, 50, 60]])) == 50\n"], "difficulty": "easy", "category": "Computer Vision"}
{"id": 83, "title": "Dot Product Calculator", "description": "\nWrite a Python function to calculate the dot product of two vectors. The function should take two 1D NumPy arrays as input and return the dot product as a single number.\n", "examples": [{"input": "\nvec1 = np.array([1, 2, 3]), vec2 = np.array([4, 5, 6])\n", "output": "\n32\n"}], "reasoning": ["\nThe function calculates the dot product by multiplying corresponding elements of the two vectors and summing the results. For vec1 = [1, 2, 3] and vec2 = [4, 5, 6], the result is (1 * 4) + (2 * 5) + (3 * 6) = 32.\n"], "entry_point": "calculate_dot_product", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def calculate_dot_product(vec1, vec2) -> float:\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \"\"\"", "reference_code": "\ndef calculate_dot_product(vec1, vec2):\n    return np.dot(vec1, vec2)\n", "test_cases": ["\nassert calculate_dot_product(np.array([1, 2, 3]), np.array([4, 5, 6])) == 32\n", "\nassert calculate_dot_product(np.array([-1, 2, 3]), np.array([4, -5, 6])) == 4\n", "\nassert calculate_dot_product(np.array([1, 0]), np.array([0, 1])) == 0\n", "\nassert calculate_dot_product(np.array([0, 0, 0]), np.array([0, 0, 0])) == 0\n", "\nassert calculate_dot_product(np.array([7]), np.array([3])) == 21\n"], "difficulty": "easy", "category": "Linear Algebra"}
{"id": 84, "title": "Phi Transformation for Polynomial Features", "description": "\nWrite a Python function to perform a Phi Transformation that maps input features into a higher-dimensional space by generating polynomial features. The transformation allows models like linear regression to fit nonlinear data by introducing new feature dimensions that represent polynomial combinations of the original input features. The function should take a list of numerical data and a degree as inputs, and return a nested list where each inner list represents the transformed features of a data point. If the degree is less than 0, the function should return an empty list.\n", "examples": [{"input": "\ndata = [1.0, 2.0], degree = 2\n", "output": "\n[[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\n"}], "reasoning": ["\nThe Phi Transformation generates polynomial features for each data point up to the specified degree. For data = [1.0, 2.0] and degree = 2, the transformation creates a nested list where each row contains powers of the data point from 0 to 2.\n"], "entry_point": "phi_transform", "import_code": "\nimport numpy as np\n", "output_constrains": "\nThe function should output a list of lists, where each inner list contains the polynomial features of the corresponding data point. The output should be rounded to 8 decimal places.\n", "starter_code": "def phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    \"\"\"", "reference_code": "\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    if degree < 0 or not data:\n        return []\n    return np.round(np.array([[x ** i for i in range(degree + 1)] for x in data]), 8).tolist()\n", "test_cases": ["\nassert phi_transform([], 2) == []\n", "\nassert phi_transform([1.0, 2.0], -1) == []\n", "\nassert phi_transform([1.0, 2.0], 2) == [[1.0, 1.0, 1.0], [1.0, 2.0, 4.0]]\n", "\nassert phi_transform([1.0, 3.0], 3) == [[1.0, 1.0, 1.0, 1.0], [1.0, 3.0, 9.0, 27.0]]\n", "\nassert phi_transform([2.0], 4) == [[1.0, 2.0, 4.0, 8.0, 16.0]]\n"], "difficulty": "easy", "category": "Linear Algebra"}
{"id": 85, "title": "Positional Encoding Calculator", "description": "\nWrite a Python function to implement the Positional Encoding layer for Transformers. The function should calculate positional encodings for a sequence length (`position`) and model dimensionality (`d_model`) using sine and cosine functions as specified in the Transformer architecture. The function should return -1 if `position` is 0, or if `d_model` is less than or equal to 0.\n", "examples": [{"input": "\nposition = 2, d_model = 8\n", "output": "\n[[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.84130859375, 0.54052734375, 0.099853515625, 0.9951171875, 0.01000213623046875, 1.0, 0.0010004043579101562, 1.0]]\n"}], "reasoning": ["\nThe function computes the positional encoding by calculating sine values for even indices and cosine values for odd indices, ensuring that the encoding provides the required positional information.\n"], "entry_point": "pos_encoding", "import_code": "\nimport numpy as np\n", "output_constrains": "\nThe position encoding array should be of dtype np.float16 and converted to a list using tolist() when returned.\n", "starter_code": "def pos_encoding(position: int, d_model: int):", "reference_code": "\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    # Create position and dimension indices\n    pos = np.arange(position).reshape(position, 1)\n    ind = np.arange(d_model).reshape(1, d_model)\n    # Compute the angles\n    angle_rads = pos / np.power(10000, (2 * (ind // 2)) / d_model)\n    # Apply sine to even indices, cosine to odd indices\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Even indices (0, 2, 4...)\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Odd indices (1, 3, 5...)\n    # Convert to float16 as required\n    return np.round(angle_rads, 4).tolist()\n", "test_cases": ["\nassert pos_encoding(2, 8) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.0998, 0.995, 0.01, 1.0, 0.001, 1.0]]\n", "\nassert pos_encoding(5, 16) == [[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.8415, 0.5403, 0.311, 0.9504, 0.0998, 0.995, 0.0316, 0.9995, 0.01, 1.0, 0.0032, 1.0, 0.001, 1.0, 0.0003, 1.0], [0.9093, -0.4161, 0.5911, 0.8066, 0.1987, 0.9801, 0.0632, 0.998, 0.02, 0.9998, 0.0063, 1.0, 0.002, 1.0, 0.0006, 1.0], [0.1411, -0.99, 0.8126, 0.5828, 0.2955, 0.9553, 0.0947, 0.9955, 0.03, 0.9996, 0.0095, 1.0, 0.003, 1.0, 0.0009, 1.0], [-0.7568, -0.6536, 0.9536, 0.3011, 0.3894, 0.9211, 0.1262, 0.992, 0.04, 0.9992, 0.0126, 0.9999, 0.004, 1.0, 0.0013, 1.0]]\n", "\nassert pos_encoding(0, 0) == -1\n", "\nassert pos_encoding(2, -1) == -1\n"], "difficulty": "hard", "category": "Deep Learning"}
{"id": 86, "title": "Detect Overfitting or Underfitting", "description": "\nWrite a Python function to determine whether a machine learning model is overfitting, underfitting, or performing well based on training and test accuracy values. The function should take two inputs: `training_accuracy` and `test_accuracy`. It should return one of three values: 1 if Overfitting, -1 if Underfitting, or 0 if a Good fit. The rules for determination are as follows:\n\n- **Overfitting**: The training accuracy is significantly higher than the test accuracy (difference > 0.2).\n- **Underfitting**: Both training and test accuracy are below 0.7.\n- **Good fit**: Neither of the above conditions is true.\n", "examples": [{"input": "\ntraining_accuracy = 0.95, test_accuracy = 0.65\n", "output": "\n1\n"}], "reasoning": ["\nThe training accuracy is much higher than the test accuracy (difference = 0.30 > 0.2). This indicates that the model is overfitting to the training data and generalizes poorly to unseen data.\n"], "entry_point": "model_fit_quality", "import_code": "", "output_constrains": "", "starter_code": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"", "reference_code": "\ndef model_fit_quality(training_accuracy, test_accuracy):\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0\n", "test_cases": ["\nassert model_fit_quality(0.95, 0.65) == 1\n", "\nassert model_fit_quality(0.6, 0.5) == -1\n", "\nassert model_fit_quality(0.85, 0.8) == 0\n", "\nassert model_fit_quality(0.5, 0.6) == -1\n", "\nassert model_fit_quality(0.75, 0.74) == 0\n"], "difficulty": "easy", "category": "Machine Learning"}
{"id": 87, "title": "Adam Optimizer", "description": "\nImplement the Adam optimizer update step function. Your function should take the current parameter value, gradient, and moving averages as inputs, and return the updated parameter value and new moving averages. The function should also handle scalar and array inputs and include bias correction for the moving averages.\n", "examples": [{"input": "\nparameter = 1.0, grad = 0.1, m = 0.0, v = 0.0, t = 1\n", "output": "\n(0.999, 0.01, 0.00001)\n"}], "reasoning": ["\nThe Adam optimizer computes updated values for the parameter, first moment (m), and second moment (v) using bias-corrected estimates of gradients. With input values parameter=1.0, grad=0.1, m=0.0, v=0.0, and t=1, the updated parameter becomes 0.999.\n"], "entry_point": "adam_optimizer", "import_code": "\nimport numpy as np\n", "output_constrains": "\nThe updated parameter, first moment (m), and second moment (v) should be rounded to 5 decimal places and converted to a list using tolist() when returned.\n", "starter_code": "def adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"", "reference_code": "\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n\n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad**2)\n\n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1**t)\n\n    # Compute bias-corrected second raw moment estimate\n    v_hat = v / (1 - beta2**t)\n\n    # Update parameters\n    update = learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    parameter = parameter - update\n\n    return np.round(parameter,5).tolist(), np.round(m,5).tolist(), np.round(v,5).tolist()\n", "test_cases": ["\nassert adam_optimizer(1.0, 0.1, 0.0, 0.0, 1) == (0.999, 0.01, 0.00001)\n", "\nassert adam_optimizer(np.array([1.0, 2.0]), np.array([0.1, 0.2]), np.zeros(2), np.zeros(2), 1) == ([0.999, 1.999], [0.01, 0.02], [1.e-05, 4.e-05])\n"], "difficulty": "medium", "category": "Deep Learning"}
{"id": 88, "title": "GPT-2 Text Generation", "description": "\nImplement a Simplified GPT-2-like Text Generation Function\n\nYou are tasked with implementing a simplified GPT-2-like text generation function in Python. This function will incorporate the following components of a minimal GPT-2 architecture:\n\n- **Token Embeddings**: Map input tokens to dense vector representations.\n- **Positional Embeddings**: Add positional information to token embeddings.\n- **Multi-head Attention**: Attend to various parts of the sequence.\n- **Feed-Forward Network**: Process attention outputs through a dense layer.\n- **Layer Normalization**: Stabilize the training process.\n\nThe function must take in the following parameters:\n\n1. Prompt: The initial text to guide the generation process.\n2. Number of Tokens to Generate: Specify how many tokens to output.\n\nYour function should output the generated text.\n\nAdditionally, utilize the helper function `load_encoder_hparams_and_params` to retrieve:\n\n- A dummy encoder.\n- Model hyperparameters.\n- Model parameters.\n\nBuild your text generation logic around these components. This exercise is designed to help you understand the core concepts behind GPT-2's autoregressive text generation.\n", "examples": [{"input": "\nprompt=\"hello\", n_tokens_to_generate=5\n", "output": "\n\"world <UNK> <UNK> <UNK> <UNK>\"\n"}], "reasoning": ["\nThe function encodes the input \"hello\" into tokens using the dummy encoder, then runs a simplified GPT-2 forward pass to generate 5 tokens. Finally, it decodes the generated tokens back into text.\n"], "entry_point": "gen_text", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility", "reference_code": "\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\ndef layer_norm(x, g, b, eps=1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + eps) + b\n\ndef linear(x, w, b):\n    return x @ w + b\n\ndef ffn(x, c_fc, c_proj):\n    return linear(gelu(linear(x, **c_fc)), **c_proj)\n\ndef attention(q, k, v, mask):\n    return softmax(q @ k.T / np.sqrt(q.shape[-1]) + mask) @ v\n\ndef mha(x, c_attn, c_proj, n_head):\n    x = linear(x, **c_attn)\n    qkv_heads = list(map(lambda x: np.split(x, n_head, axis=-1), np.split(x, 3, axis=-1)))\n    causal_mask = (1 - np.tri(x.shape[0], dtype=x.dtype)) * -1e10\n    out_heads = [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)]\n    x = linear(np.hstack(out_heads), **c_proj)\n    return x\n\ndef transformer_block(x, mlp, attn, ln_1, ln_2, n_head):\n    x = x + mha(layer_norm(x, **ln_1), **attn, n_head=n_head)\n    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n    return x\n\ndef gpt2(inputs, wte, wpe, blocks, ln_f, n_head):\n    x = wte[inputs] + wpe[range(len(inputs))]\n    for block in blocks:\n        x = transformer_block(x, **block, n_head=n_head)\n    return layer_norm(x, **ln_f) @ wte.T\n\ndef generate(inputs, params, n_head, n_tokens_to_generate):\n    for _ in range(n_tokens_to_generate):\n        logits = gpt2(inputs, **params, n_head=n_head)\n        next_id = np.argmax(logits[-1])\n        inputs.append(int(next_id))\n    return inputs[len(inputs) - n_tokens_to_generate:]\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    np.random.seed(42)  # Set the random seed for reproducibility\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    assert len(input_ids) + n_tokens_to_generate < hparams[\"n_ctx\"]\n    output_ids = generate(input_ids, params, hparams[\"n_head\"], n_tokens_to_generate)\n    output_text = encoder.decode(output_ids)\n    return output_text\n", "test_cases": ["\nassert gen_text(\"hello\", 5) == \"hello hello hello <UNK> <UNK>\"\n", "\nassert gen_text(\"hello world\", 10) == \"world world world world world world world world world world\"\n", "\nassert gen_text(\"world\", 3) == \"world world world\"\n"], "difficulty": "hard", "category": "NLP"}
{"id": 89, "title": "The Pattern Weaver's Code", "description": "\nDeep in the Crystal Cave, the enigmatic Pattern Weaver creates stunning sequences by uncovering the intricate relationships between crystals. Each crystal is marked by a unique numeric value, and the Weaver emphasizes that the true power of any crystal depends on how it interacts with all others. You have discovered N crystals, each with a specific value, and your task is to reveal their enhanced patterns by analyzing these relationships using self-attention. Given a sequence of crystals and their values, your task is to implement a simplified self-attention mechanism. For each crystal, calculate its relationship with every other crystal, compute the attention scores using the softmax function, and derive the final weighted pattern for each crystal.\n", "examples": [{"input": "\nnumber of crystals: 5\nvalues: 4 2 7 1 9\ndimension: 1\n", "output": "\n[8.9993, 8.9638, 9.0, 8.7259, 9.0]\n"}], "reasoning": ["\nThe self-attention mechanism calculates relationships (attention scores) for each crystal using the given formula. These scores are converted to probabilities using the softmax function, and the final weighted pattern for each crystal is derived by summing the weighted values.\n"], "entry_point": "pattern_weaver", "import_code": "\nimport numpy as np\n", "output_constrains": "\nYour code should return a list of floats, each rounded to the 4th decimal place.\n", "starter_code": "def pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Your code here, implement the softmax function first", "reference_code": "\ndef softmax(values):\n    exps = np.exp(values - np.max(values))\n    return exps / np.sum(exps)\n\ndef pattern_weaver(n, crystal_values, dimension):\n    dimension_sqrt = np.sqrt(dimension)\n    final_patterns = []\n\n    for i in range(n):\n        attention_scores = []\n        for j in range(n):\n            score = crystal_values[i] * crystal_values[j] / dimension_sqrt\n            attention_scores.append(score)\n\n        softmax_scores = softmax(attention_scores)\n        weighted_sum = sum(softmax_scores[k] * crystal_values[k] for k in range(n))\n        final_patterns.append(round(weighted_sum, 4))\n\n    return final_patterns\n", "test_cases": ["\nassert pattern_weaver(5, [4, 2, 7, 1, 9], 1) == [8.9993, 8.9638, 9.0, 8.7259, 9.0]\n", "\nassert pattern_weaver(3, [1, 3, 5], 1) == [4.7019, 4.995, 4.9999]\n", "\nassert pattern_weaver(4, [2, 8, 6, 4], 1) == [7.9627, 8.0, 8.0, 7.9993]\n", "\nassert pattern_weaver(3, [9, 2, 1], 1) == [9.0, 9.0, 8.9909]\n", "\nassert pattern_weaver(3, [9, 2, 1], 2) == [9.0, 8.9996, 8.9233]\n"], "difficulty": "medium", "category": "Deep Learning"}
{"id": 90, "title": "BM25 Ranking", "description": "\nImplement the BM25 ranking function to calculate document scores for a query in an information retrieval context. BM25 is an advanced variation of TF-IDF that incorporates term frequency saturation, document length normalization, and a configurable penalty for document length effects.\n", "examples": [{"input": "\ncorpus = [['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], query = ['the', 'cat']\n", "output": "\n[0.693, 0., 0. ]\n"}], "reasoning": ["\nBM25 calculates scores for each document in the corpus by evaluating how well the query terms match each document while considering term frequency saturation and document length normalization.\n"], "entry_point": "calculate_bm25_scores", "import_code": "\nimport numpy as np\nfrom collections import Counter\n", "output_constrains": "\nThe function should output a list representing the score of each document in the corpus, with each rounded to three decimal places.\n", "starter_code": "def calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):", "reference_code": "\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    if not corpus or not query:\n        raise ValueError(\"Corpus and query cannot be empty\")\n\n    doc_lengths = [len(doc) for doc in corpus]\n    avg_doc_length = np.mean(doc_lengths)\n    doc_term_counts = [Counter(doc) for doc in corpus]\n    doc_freqs = Counter()\n    for doc in corpus:\n        doc_freqs.update(set(doc))\n\n    scores = np.zeros(len(corpus))\n    N = len(corpus)\n\n    for term in query:\n        df = doc_freqs.get(term, 0) + 1\n        idf = np.log((N + 1) / df)\n\n        for idx, term_counts in enumerate(doc_term_counts):\n            if term not in term_counts:\n                continue\n\n            tf = term_counts[term]\n            doc_len_norm = 1 - b + b * (doc_lengths[idx] / avg_doc_length)\n            term_score = (tf * (k1 + 1)) / (tf + k1 * doc_len_norm)\n            scores[idx] += idf * term_score\n\n    return np.round(scores, 3).tolist()\n", "test_cases": ["\nassert calculate_bm25_scores([['the', 'cat', 'sat'], ['the', 'dog', 'ran'], ['the', 'bird', 'flew']], ['the', 'cat']) == [0.693, 0., 0. ]\n", "\nassert calculate_bm25_scores([['the'] * 10, ['the']], ['the']) == [0,0]\n", "\nassert calculate_bm25_scores([['term'] * 10, ['the'] * 2], ['term'], k1=1.0) == [.705, 0]\n"], "difficulty": "medium", "category": "NLP"}
{"id": 91, "title": "Calculate F1 Score from Predicted and True Labels", "description": "\nImplement a function to calculate the F1 score given predicted and true labels. The F1 score is a widely used metric in machine learning, combining precision and recall into a single measure. round your solution to the 3rd decimal place\n", "examples": [{"input": "\ny_true = [1, 0, 1, 1, 0], y_pred = [1, 0, 0, 1, 1]\n", "output": "\n0.667\n"}], "reasoning": ["\nThe true positives, false positives, and false negatives are calculated from the given labels. Precision and recall are derived, and the F1 score is computed as their harmonic mean.\n"], "entry_point": "calculate_f1_score", "import_code": "", "output_constrains": "", "starter_code": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"", "reference_code": "\ndef calculate_f1_score(y_true, y_pred):\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Lengths of y_true and y_pred must be the same\")\n\n    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp == 1)\n    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n\n    if tp + fp == 0 or tp + fn == 0:\n        return 0.0\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n\n    if precision + recall == 0:\n        return 0.0\n\n    f1_score = 2 * (precision * recall) / (precision + recall)\n    return round(f1_score, 3)\n", "test_cases": ["\nassert calculate_f1_score([1, 0, 1, 1, 0], [1, 0, 0, 1, 1]) == 0.667\n", "\nassert calculate_f1_score([1, 1, 0, 0], [1, 0, 0, 1]) == 0.5\n", "\nassert calculate_f1_score([0, 0, 0, 0], [1, 1, 1, 1]) == 0.0\n", "\nassert calculate_f1_score([1, 1, 1, 1, 0], [1, 1, 0, 1, 1]) == 0.75\n", "\nassert calculate_f1_score([1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 0]) == 0.889\n"], "difficulty": "easy", "category": "Machine Learning"}
{"id": 92, "title": "Linear Regression - Power Grid Optimization", "description": "\nIt is the year 2157. Mars has its first thriving colony, and energy consumption is steadily on the rise. As the lead data scientist, you have daily power usage measurements (10 days) affected by both a growing linear trend and a daily fluctuation. The fluctuation follows the formula $f(i) = 10 \\times \\sin(2\\pi i / 10)$, where i is the day number (1 through 10). Your challenge is to remove this known fluctuation from each data point, fit a linear regression model to the detrended data, predict day 15's base consumption, add back the fluctuation for day 15, and finally include a 5% safety margin. The final answer must be an integer, ensuring you meet the colony's future needs.\n", "examples": [{"input": "\nDaily consumption data for 10 days: [150, 165, 185, 195, 210, 225, 240, 260, 275, 290]\n", "output": "\n404\n"}], "reasoning": ["\nFor each of the 10 days, we subtract the daily fluctuation given by 10*sin(2\u03c0*i/10). We then perform linear regression on the resulting values, predict day 15\u2019s base usage, and add back the day 15 fluctuation. Finally, we apply a 5% margin. Running the provided solution code yields 404 for this dataset.\n"], "entry_point": "power_grid_forecast", "import_code": "\nimport math\n", "output_constrains": "", "starter_code": "PI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    # 2) Perform linear regression on the detrended data.\n    # 3) Predict day 15's base consumption.\n    # 4) Add the day 15 fluctuation back.\n    # 5) Round, then add a 5% safety margin (rounded up).\n    # 6) Return the final integer.", "reference_code": "\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # consumption_data: list of 10 daily consumption values\n    # days: 1 through 10\n    days = list(range(1, 11))\n    n = len(days)\n\n    # 1) Remove daily fluctuation\n    detrended = []\n    for i, cons in zip(days, consumption_data):\n        fluctuation_i = 10 * math.sin((2 * PI * i) / 10)\n        detrended_value = cons - fluctuation_i\n        detrended.append(detrended_value)\n\n    # 2) Perform linear regression on the detrended data\n    sum_x = sum(days)\n    sum_y = sum(detrended)\n    sum_xy = sum(x * y for x, y in zip(days, detrended))\n    sum_x2 = sum(x**2 for x in days)\n\n    # slope (m) and intercept (b) for y = m*x + b\n    m = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)\n    b = (sum_y - m * sum_x) / n\n\n    # 3) Predict day 15's base usage\n    day_15_base = m * 15 + b\n\n    # 4) Add back daily fluctuation for day 15\n    day_15_fluctuation = 10 * math.sin((2 * PI * 15) / 10)\n    day_15_prediction = day_15_base + day_15_fluctuation\n\n    # 5) Round and add 5% safety margin\n    day_15_rounded = round(day_15_prediction)\n    final_15 = math.ceil(day_15_rounded * 1.05)\n\n    return final_15\n", "test_cases": ["\nassert power_grid_forecast([150, 165, 185, 195, 210, 225, 240, 260, 275, 290]) == 404\n", "\nassert power_grid_forecast([160, 170, 190, 200, 215, 230, 245, 265, 280, 295]) == 407\n", "\nassert power_grid_forecast([140, 158, 180, 193, 205, 220, 237, 255, 270, 288]) == 404\n", "\nassert power_grid_forecast([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) == 19\n", "\nassert power_grid_forecast([1, 19, 1, 20, 1, 18, 1, 19, 1, 20]) == 35\n"], "difficulty": "medium", "category": "Machine Learning"}
{"id": 93, "title": "Calculate Mean Absolute Error (MAE)", "description": "\nImplement a function to calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values. The MAE is a metric used to measure the average magnitude of errors in a set of predictions without considering their direction.\n", "examples": [{"input": "\ny_true = np.array([3, -0.5, 2, 7]), y_pred = np.array([2.5, 0.0, 2, 8])\n", "output": "\n0.500\n"}], "reasoning": ["\nThe MAE is calculated by taking the mean of the absolute differences between the predicted and true values. Using the formula, the result is 0.500.\n"], "entry_point": "mae", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"", "reference_code": "\ndef mae(y_true, y_pred):\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Arrays must have the same shape\")\n    if y_true.size == 0:\n        raise ValueError(\"Arrays cannot be empty\")\n\n    return round(np.mean(np.abs(y_true - y_pred)), 3)\n", "test_cases": ["\nassert mae(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.500\n", "\nassert mae(np.array([[0.5, 1], [-1, 1], [7, -6]]), np.array([[0, 2], [-1, 2], [8, -5]])) == 0.750\n", "\nassert mae(np.array([-1, -2, -3]), np.array([-1.5, -2.2, -2.8])) == 0.300\n", "\nassert mae(np.array([1, -1, 0]), np.array([-1, 1, 0])) == 1.333\n", "\nassert mae(np.array([1000, -1000, 0]), np.array([-1000, 1000, 0])) == 1333.333\n", "\nassert mae(np.array([1000, -1000, 0]), np.array([0, 0, 0])) == 666.667\n"], "difficulty": "easy", "category": "Machine Learning"}
{"id": 94, "title": "Implement Multi-Head Attention", "description": "\nImplement the multi-head attention mechanism, a key component in the Transformer model, via three key functions: `compute_qkv`, `self_attention`, and `multi_head_attention`.\n", "examples": [{"input": "\nm, n = 2, 2\nn_heads = 2\nnp.random.seed(42)\nX = np.arange(m*n).reshape(m,n)\nX = np.random.permutation(X.flatten()).reshape(m, n)\nW_q = np.random.randint(0,4,size=(n,n))\nW_k = np.random.randint(0,5,size=(n,n))\nW_v = np.random.randint(0,6,size=(n,n))\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nprint(np.round(multi_head_attention(Q, K, V, n_heads), 4).tolist())\n", "output": "\n[[9.9852, 18.0], [7.0, 14.0]]\n"}], "reasoning": ["\nMulti-head attention is computed for 2 heads using the input Q, K, and V matrices. The resulting outputs for each head are concatenated to form the final attention output.\n"], "entry_point": "entrypoint", "import_code": "\nimport numpy as np\n", "output_constrains": "", "starter_code": "def entrypoint(m, n, n_heads, random_seed):\n    np.random.seed(random_seed)\n    # Generate input data\n    X = np.arange(m*n).reshape(m,n)\n    X = np.random.permutation(X.flatten()).reshape(m, n)\n    # Generate weight matrices\n    W_q = np.random.randint(0, 4, size=(n,n))\n    W_k = np.random.randint(0, 5, size=(n,n))\n    W_v = np.random.randint(0, 6, size=(n,n))\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    return np.round(multi_head_attention(Q, K, V, n_heads), 4).tolist()\n\ndef compute_qkv(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the Query (Q), Key (K), and Value (V) matrices.\n    \n    Args:\n    X: numpy array of shape (seq_len, d_model), input sequence\n    W_q, W_k, W_v: numpy arrays of shape (d_model, d_model), weight matrices for Q, K, and V\n    \n    Returns:\n    Q, K, V: numpy arrays of shape (seq_len, d_model)\n    \"\"\"\n\ndef self_attention(Q, K, V):\n    \"\"\"\n    Compute self-attention for a single head.\n    \n    Args:\n    Q: numpy array of shape (seq_len, d_k), Query matrix\n    K: numpy array of shape (seq_len, d_k), Key matrix\n    V: numpy array of shape (seq_len, d_k), Value matrix\n    \n    Returns:\n    attention_output: numpy array of shape (seq_len, d_k), output of the self-attention mechanism\n    \"\"\"\n\ndef multi_head_attention(Q, K, V, n_heads):\n    \"\"\"\n    Compute multi-head attention.\n    \n    Args:\n    Q, K, V: numpy arrays of shape (seq_len, d_model), Query, Key, and Value matrices\n    n_heads: int, number of attention heads\n    \n    Returns:\n    attention_output: numpy array of shape (seq_len, d_model), final attention output\n    \"\"\"", "reference_code": "\ndef entrypoint(m, n, n_heads, random_seed):\n    np.random.seed(random_seed)\n    # Generate input data\n    X = np.arange(m*n).reshape(m,n)\n    X = np.random.permutation(X.flatten()).reshape(m, n)\n    # Generate weight matrices\n    W_q = np.random.randint(0, 4, size=(n,n))\n    W_k = np.random.randint(0, 5, size=(n,n))\n    W_v = np.random.randint(0, 6, size=(n,n))\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    return np.round(multi_head_attention(Q, K, V, n_heads), 4).tolist()\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n\n    Q = np.dot(X, W_q)  # Compute the Query matrix Q\n    K = np.dot(X, W_k)  # Compute the Key matrix K\n    V = np.dot(X, W_v)  # Compute the Value matrix V\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \n    d_k = Q.shape[1]  # Get the dimension of the keys\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)  # Compute scaled dot-product attention scores\n    score_max = np.max(scores, axis=1, keepdims=True)  # Find the maximum score for numerical stability\n    attention_weights = np.exp(scores - score_max) / np.sum(np.exp(scores - score_max), axis=1, keepdims=True)  # Compute softmax to get attention weights\n    attention_output = np.matmul(attention_weights, V)  # Compute the final attention output\n    return attention_output\n\ndef multi_head_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, n_heads: int) -> np.ndarray:\n    \n    d_model = Q.shape[1]  # Get the model dimension\n    assert d_model % n_heads == 0  # Ensure d_model is divisible by n_heads\n    d_k = d_model // n_heads  # Dimension for each head\n\n    # Reshape Q, K, V to separate heads\n    Q_reshaped = Q.reshape(Q.shape[0], n_heads, d_k).transpose(1, 0, 2)  # Reshape and transpose to (n_heads, seq_len, d_k)\n    K_reshaped = K.reshape(K.shape[0], n_heads, d_k).transpose(1, 0, 2)  # Reshape and transpose to (n_heads, seq_len, d_k)\n    V_reshaped = V.reshape(V.shape[0], n_heads, d_k).transpose(1, 0, 2)  # Reshape and transpose to (n_heads, seq_len, d_k)\n\n    # Compute attention scores for each head\n    attentions = []  # Store attention outputs for each head\n\n    for i in range(n_heads):\n        attn = self_attention(Q_reshaped[i], K_reshaped[i], V_reshaped[i])  # Compute attention for the i-th head\n        attentions.append(attn)  # Collect attention output\n\n    # Concatenate all head outputs\n    attention_output = np.concatenate(attentions, axis=-1)  # Concatenate along the last axis (columns)\n    return attention_output  # Return the final attention output\n", "test_cases": ["\nassert entrypoint(4, 4, 2, 42) == [[103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0], [103.0, 109.0, 46.0, 99.0]]\n", "\nassert entrypoint(6, 8, 4, 42) == [[500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [500.0, 463.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]\n", "\nassert entrypoint(6, 8, 2, 42) == [[547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0], [547.0, 490.0, 399.0, 495.0, 377.0, 450.0, 531.0, 362.0]]"], "difficulty": "hard", "category": "Deep Learning"}
{"id": 95, "title": "Calculate the Phi Coefficient", "description": "\nImplement a function to calculate the Phi coefficient, a measure of the correlation between two binary variables. The function should take two lists of integers (0s and 1s) as input and return the Phi coefficient rounded to 4 decimal places.\n", "examples": [{"input": "\nphi_corr([1, 1, 0, 0], [0, 0, 1, 1])\n", "output": "\n-1.0\n"}], "reasoning": ["\nThe Phi coefficient measures the correlation between two binary variables. In this example, the variables have a perfect negative correlation, resulting in a Phi coefficient of -1.0.\n"], "entry_point": "phi_corr", "import_code": "", "output_constrains": "", "starter_code": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"", "reference_code": "\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    x1y1 = x1y0 = x0y1 = x0y0 = 0\n\n    # Count occurrences\n    for i in range(len(x)):\n        if x[i] == 1:\n            if y[i] == 1:\n                x1y1 += 1\n            else:\n                x1y0 += 1\n        elif x[i] == 0:\n            if y[i] == 1:\n                x0y1 += 1\n            else:\n                x0y0 += 1\n\n    # Calculate numerator and denominator\n    numerator = (x0y0 * x1y1) - (x0y1 * x1y0)\n    denominator = ((x0y0 + x0y1) * (x1y0 + x1y1) * (x0y0 + x1y0) * (x0y1 + x1y1)) ** 0.5\n\n    if denominator == 0:\n        return 0.0\n\n    phi = numerator / denominator\n    return round(phi, 4)\n", "test_cases": ["\nassert phi_corr([1, 1, 0, 0], [0, 0, 1, 1]) == -1.0\n", "\nassert phi_corr([1, 1, 0, 0], [1, 0, 1, 1]) == -0.5774\n", "\nassert phi_corr([0, 0, 1, 1], [0, 1, 0, 1]) == 0.0\n", "\nassert phi_corr([1, 0, 1, 0,1,1,0], [1, 1, 0, 0,1,1,1]) == 0.0913\n"], "difficulty": "easy", "category": "Statistics"}
{"id": 96, "title": "Implement the Hard Sigmoid Activation Function", "description": "\nImplement the Hard Sigmoid activation function, a computationally efficient approximation of the standard sigmoid function. Your function should take a single input value and return the corresponding output based on the Hard Sigmoid definition.\n", "examples": [{"input": "\nhard_sigmoid(0.0)\n", "output": "\n0.5\n"}], "reasoning": ["\nThe input 0.0 falls in the linear region of the Hard Sigmoid function. Using the formula $HardSigmoid(x) = 0.2x + 0.5$, the output is $0.2 \\times 0.0 + 0.5 = 0.5$.\n"], "entry_point": "hard_sigmoid", "import_code": "", "output_constrains": "", "starter_code": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"", "reference_code": "\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n", "test_cases": ["\nassert hard_sigmoid(.56) == 0.612\n", "\nassert hard_sigmoid(3.0) == 1.0\n", "\nassert hard_sigmoid(0.0) == 0.5\n", "\nassert hard_sigmoid(1.0) == 0.7\n", "\nassert hard_sigmoid(-1.0) == 0.3\n", "\nassert hard_sigmoid(2.5) == 1.0\n", "\nassert hard_sigmoid(-2.5) == 0.0\n", "\nassert hard_sigmoid(-3.0) == 0.0\n"], "difficulty": "easy", "category": "Deep Learning"}
{"id": 97, "title": "Implement the ELU Activation Function", "description": "\nImplement the ELU (Exponential Linear Unit) activation function, which helps mitigate the limitations of ReLU by providing negative outputs for negative inputs. The function should compute the ELU activation value for a given input.\n", "examples": [{"input": "\nelu(-1)\n", "output": "\n-0.6321\n"}], "reasoning": ["\nFor x = -1 and alpha = 1.0, the ELU activation is computed as $\\alpha \\times (e^{x} - 1)$.\n"], "entry_point": "elu", "import_code": "\nimport math\n", "output_constrains": "\nYour code should return a float rounded to the 4th decimal place.\n", "starter_code": "def elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"", "reference_code": "\ndef elu(x: float, alpha: float = 1.0) -> float:\n    return round(x if x > 0 else alpha * (math.exp(x) - 1), 4)\n", "test_cases": ["\nassert elu(0) == 0.0\n", "\nassert elu(1) == 1.0\n", "\nassert elu(-1) == -0.6321\n", "\nassert elu(-1, alpha=2.0) == -1.2642\n", "\nassert elu(5) == 5.0\n", "\nassert elu(-5) == -0.9933\n"], "difficulty": "easy", "category": "Deep Learning"}
{"id": 98, "title": "Implement the PReLU Activation Function", "description": "\nImplement the PReLU (Parametric ReLU) activation function, a variant of the ReLU activation function that introduces a learnable parameter for negative inputs. Your task is to compute the PReLU activation value for a given input.\n", "examples": [{"input": "\nprelu(-2.0, alpha=0.25)\n", "output": "\n-0.5\n"}], "reasoning": ["\nFor x = -2.0 and alpha = 0.25, the PReLU activation is calculated as $ PReLU(x) = \\alpha x = 0.25 \\times -2.0 = -0.5$.\n"], "entry_point": "prelu", "import_code": "", "output_constrains": "", "starter_code": "def prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"", "reference_code": "\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    return x if x > 0 else alpha * x\n", "test_cases": ["\nassert prelu(2.0) == 2.0\n", "\nassert prelu(0.0) == 0.0\n", "\nassert prelu(-2.0) == -0.5\n", "\nassert prelu(-2.0, alpha=0.1) == -0.2\n", "\nassert prelu(-2.0, alpha=1.0) == -2.0\n"], "difficulty": "easy", "category": "Deep Learning"}
{"id": 99, "title": "Implement the Softplus Activation Function", "description": "\nImplement the Softplus activation function, a smooth approximation of the ReLU function. Your task is to compute the Softplus value for a given input, handling edge cases to prevent numerical overflow or underflow.\n", "examples": [{"input": "\nsoftplus(2)\n", "output": "\n2.1269\n"}], "reasoning": ["\nFor x = 2, the Softplus activation is calculated as $\\log(1 + e^{x})$.\n"], "entry_point": "softplus", "import_code": "\nimport math\n", "output_constrains": "\nYour code should return a float rounded to the 4th decimal place.\n", "starter_code": "def softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"", "reference_code": "\ndef softplus(x: float) -> float:\n    # To prevent overflow for large positive values\n    if x > 100:\n        return x\n    # To prevent underflow for large negative values\n    if x < -100:\n        return 0.0\n\n    return round(math.log(1.0 + math.exp(x)),4)\n", "test_cases": ["\nassert softplus(0) == 0.6931\n", "\nassert softplus(100) == 100.0\n", "\nassert softplus(-100) == 0.0\n", "\nassert softplus(2) == 2.1269\n", "\nassert softplus(-2) == 0.1269\n"], "difficulty": "easy", "category": "Deep Learning"}
{"id": 100, "title": "Implement the Softsign Activation Function", "description": "\nImplement the Softsign activation function, a smooth activation function used in neural networks. Your task is to compute the Softsign value for a given input, ensuring the output is bounded between -1 and 1.\n", "examples": [{"input": "\nsoftsign(1)\n", "output": "\n0.5\n"}], "reasoning": ["\nFor x = 1, the Softsign activation is calculated as $ \\frac{x}{1 + |x|}$.\n"], "entry_point": "softsign", "import_code": "", "output_constrains": "\nYour code should return a float rounded to the 4th decimal place.\n", "starter_code": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"", "reference_code": "\ndef softsign(x: float) -> float:\n    return round(x / (1 + abs(x)), 4)\n", "test_cases": ["\nassert softsign(0) == 0.0\n", "\nassert softsign(1) == 0.5\n", "\nassert softsign(-1) == -0.5\n", "\nassert softsign(100) == 0.9901\n", "\nassert softsign(-100) == -0.9901\n"], "difficulty": "easy", "category": "Deep Learning"}
